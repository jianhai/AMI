From 4c8140fb6ffadda22cd7c9216711027b5c1a8e26 Mon Sep 17 00:00:00 2001
From: Munehisa Kamata <kamatam@amazon.com>
Date: Tue, 12 Jan 2016 17:53:45 +0000
Subject: ena: update to Beta2 Rc2b+

Signed-off-by: Munehisa Kamata <kamatam@amazon.com>
Reviewed-by: Netanel Belgazal <netanel@annapurnalabs.com>
Reviewed-by: Rashika Kheria <rashika@amazon.de>
Reviewed-by: Matt Nierzwicki <nierzwic@amazon.com>

CR: https://cr.amazon.com/r/5132188/
---
 drivers/amazon/ena/Makefile          |    2 +-
 drivers/amazon/ena/ena_admin_defs.h  |  707 ++++++++++-----------
 drivers/amazon/ena/ena_com.c         | 1070 ++++++++++++++++++++++++++-----
 drivers/amazon/ena/ena_com.h         |  549 +++++++++++++---
 drivers/amazon/ena/ena_common_defs.h |   70 +-
 drivers/amazon/ena/ena_eth_com.c     |  494 +++++++++++++++
 drivers/amazon/ena/ena_eth_com.h     |  481 +-------------
 drivers/amazon/ena/ena_eth_io_defs.h |  361 +++++------
 drivers/amazon/ena/ena_ethtool.c     |  802 +++++++++++++++++++++++
 drivers/amazon/ena/ena_netdev.c      | 1156 ++++++++++++++++------------------
 drivers/amazon/ena/ena_netdev.h      |   81 ++-
 drivers/amazon/ena/ena_regs_defs.h   |  160 ++---
 drivers/amazon/ena/ena_sysfs.c       |    7 +-
 drivers/amazon/ena/ena_sysfs.h       |    1 -
 14 files changed, 3906 insertions(+), 2035 deletions(-)
 create mode 100644 drivers/amazon/ena/ena_eth_com.c
 create mode 100644 drivers/amazon/ena/ena_ethtool.c

diff --git a/drivers/amazon/ena/Makefile b/drivers/amazon/ena/Makefile
index 5ce8892..9df162a 100644
--- a/drivers/amazon/ena/Makefile
+++ b/drivers/amazon/ena/Makefile
@@ -1,2 +1,2 @@
 obj-$(CONFIG_AMAZON_ENA) += ena.o
-ena-objs := ena_com.o ena_netdev.o ena_sysfs.o
+ena-objs := ena_com.o ena_ethtool.o ena_eth_com.o ena_netdev.o ena_sysfs.o
diff --git a/drivers/amazon/ena/ena_admin_defs.h b/drivers/amazon/ena/ena_admin_defs.h
index 5fb67c9..a03b5db 100644
--- a/drivers/amazon/ena/ena_admin_defs.h
+++ b/drivers/amazon/ena/ena_admin_defs.h
@@ -1,304 +1,294 @@
-/******************************************************************************
-Copyright (C) 2015 Annapurna Labs Ltd.
-
-This file may be licensed under the terms of the Annapurna Labs Commercial
-License Agreement.
-
-Alternatively, this file can be distributed under the terms of the GNU General
-Public License V2 as published by the Free Software Foundation and can be
-found at http://www.gnu.org/licenses/gpl-2.0.html
-
-Alternatively, redistribution and use in source and binary forms, with or
-without modification, are permitted provided that the following conditions are
-met:
-
-    *  Redistributions of source code must retain the above copyright notice,
-this list of conditions and the following disclaimer.
-
-    *  Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer in
-the documentation and/or other materials provided with the
-distribution.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
-ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
-(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
-LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
-ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-******************************************************************************/
-
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
 #ifndef _ENA_ADMIN_H_
 #define _ENA_ADMIN_H_
 
 /* admin commands opcodes */
 enum ena_admin_aq_opcode {
 	/* create submission queue */
-	ena_admin_create_sq = 1,
+	ENA_ADMIN_CREATE_SQ = 1,
 
 	/* destroy submission queue */
-	ena_admin_destroy_sq = 2,
+	ENA_ADMIN_DESTROY_SQ = 2,
 
 	/* create completion queue */
-	ena_admin_create_cq = 3,
+	ENA_ADMIN_CREATE_CQ = 3,
 
 	/* destroy completion queue */
-	ena_admin_destroy_cq = 4,
+	ENA_ADMIN_DESTROY_CQ = 4,
 
 	/* suspend submission queue */
-	ena_admin_suspend_sq = 5,
+	ENA_ADMIN_SUSPEND_SQ = 5,
 
 	/* resume submission queue */
-	ena_admin_resume_sq = 6,
+	ENA_ADMIN_RESUME_SQ = 6,
 
 	/* flush submission queue */
-	ena_admin_flush_sq = 7,
+	ENA_ADMIN_FLUSH_SQ = 7,
 
 	/* get capabilities of particular feature */
-	ena_admin_get_feature = 8,
+	ENA_ADMIN_GET_FEATURE = 8,
 
 	/* get capabilities of particular feature */
-	ena_admin_set_feature = 9,
+	ENA_ADMIN_SET_FEATURE = 9,
 
 	/* enabling events in AENQ */
-	ena_admin_async_event_request = 10,
+	ENA_ADMIN_ASYNC_EVENT_REQUEST = 10,
 
 	/* get statistics */
-	ena_admin_get_stats = 11,
+	ENA_ADMIN_GET_STATS = 11,
 };
 
 /* privileged amdin commands opcodes */
 enum ena_admin_aq_opcode_privileged {
 	/* get device capabilities */
-	ena_admin_identify = 48,
+	ENA_ADMIN_IDENTIFY = 48,
 
 	/* configure device */
-	ena_admin_configure_pf_device = 49,
+	ENA_ADMIN_CONFIGURE_PF_DEVICE = 49,
 
 	/* setup SRIOV PCIe Virtual Function capabilities */
-	ena_admin_setup_vf = 50,
+	ENA_ADMIN_SETUP_VF = 50,
 
 	/* load firmware to the controller */
-	ena_admin_load_firmware = 52,
+	ENA_ADMIN_LOAD_FIRMWARE = 52,
 
 	/* commit previously loaded firmare */
-	ena_admin_commit_firmware = 53,
+	ENA_ADMIN_COMMIT_FIRMWARE = 53,
 
 	/* quiesce virtual function */
-	ena_admin_quiesce_vf = 54,
+	ENA_ADMIN_QUIESCE_VF = 54,
 
 	/* load virtual function from migrates context */
-	ena_admin_migrate_vf = 55,
+	ENA_ADMIN_MIGRATE_VF = 55,
 };
 
 /* admin command completion status codes */
 enum ena_admin_aq_completion_status {
 	/* Request completed successfully */
-	ena_admin_success = 0,
+	ENA_ADMIN_SUCCESS = 0,
 
 	/* no resources to satisfy request */
-	ena_admin_resource_allocation_failure = 1,
+	ENA_ADMIN_RESOURCE_ALLOCATION_FAILURE = 1,
 
 	/* Bad opcode in request descriptor */
-	ena_admin_bad_opcode = 2,
+	ENA_ADMIN_BAD_OPCODE = 2,
 
 	/* Unsupported opcode in request descriptor */
-	ena_admin_unsupported_opcode = 3,
+	ENA_ADMIN_UNSUPPORTED_OPCODE = 3,
 
 	/* Wrong request format */
-	ena_admin_malformed_request = 4,
+	ENA_ADMIN_MALFORMED_REQUEST = 4,
 
-	/*
-	 * One of parameters is not valid. Provided in ACQ entry
+	/* One of parameters is not valid. Provided in ACQ entry
 	 * extended_status
 	 */
-	ena_admin_illegal_parameter = 5,
+	ENA_ADMIN_ILLEGAL_PARAMETER = 5,
 
 	/* unexpected error */
-	ena_admin_unknown_error = 6,
+	ENA_ADMIN_UNKNOWN_ERROR = 6,
 };
 
 /* get/set feature subcommands opcodes */
 enum ena_admin_aq_feature_id {
 	/* list of all supported attributes/capabilities in the ENA */
-	ena_admin_device_attributes = 1,
+	ENA_ADMIN_DEVICE_ATTRIBUTES = 1,
 
 	/* max number of supported queues per for every queues type */
-	ena_admin_max_queues_num = 2,
+	ENA_ADMIN_MAX_QUEUES_NUM = 2,
 
 	/* low latency queues capabilities (max entry size, depth) */
-	ena_admin_llq_config = 3,
+	ENA_ADMIN_LLQ_CONFIG = 3,
 
 	/* power management capabilities */
-	ena_admin_power_management_config = 4,
+	ENA_ADMIN_POWER_MANAGEMENT_CONFIG = 4,
 
-	/* MAC address filters support, multicast, broadcast, and promiscous */
-	ena_admin_mac_filters_config = 5,
+	/* MAC address filters support, multicast, broadcast, and
+	 * promiscuous
+	 */
+	ENA_ADMIN_MAC_FILTERS_CONFIG = 5,
 
 	/* VLAN membership, frame format, etc.  */
-	ena_admin_vlan_config = 6,
+	ENA_ADMIN_VLAN_CONFIG = 6,
 
-	/*
-	 * Available size for various on-chip memory resources, accessible
+	/* Available size for various on-chip memory resources, accessible
 	 * by the driver
 	 */
-	ena_admin_on_device_memory_config = 7,
+	ENA_ADMIN_ON_DEVICE_MEMORY_CONFIG = 7,
 
 	/* L2 bridging capabilities inside ENA */
-	ena_admin_l2_bridg_config = 8,
+	ENA_ADMIN_L2_BRIDG_CONFIG = 8,
 
 	/* L3 routing capabilities inside ENA */
-	ena_admin_l3_router_config = 9,
+	ENA_ADMIN_L3_ROUTER_CONFIG = 9,
 
 	/* Receive Side Scaling (RSS) function */
-	ena_admin_rss_hash_function = 10,
+	ENA_ADMIN_RSS_HASH_FUNCTION = 10,
 
 	/* stateless TCP/UDP/IP offload capabilities. */
-	ena_admin_stateless_offload_config = 11,
+	ENA_ADMIN_STATELESS_OFFLOAD_CONFIG = 11,
 
 	/* Multiple tuples flow table configuration */
-	ena_admin_rss_redirection_table_config = 12,
+	ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG = 12,
 
 	/* Data center bridging (DCB) capabilities */
-	ena_admin_dcb_config = 13,
+	ENA_ADMIN_DCB_CONFIG = 13,
 
 	/* max MTU, current MTU */
-	ena_admin_mtu = 14,
+	ENA_ADMIN_MTU = 14,
 
-	/*
-	 * Virtual memory address translation capabilities for userland
+	/* Virtual memory address translation capabilities for userland
 	 * queues
 	 */
-	ena_admin_va_translation_config = 15,
+	ENA_ADMIN_VA_TRANSLATION_CONFIG = 15,
 
 	/* traffic class capabilities */
-	ena_admin_tc_config = 16,
+	ENA_ADMIN_TC_CONFIG = 16,
 
 	/* traffic class capabilities */
-	ena_admin_encryption_config = 17,
+	ENA_ADMIN_ENCRYPTION_CONFIG = 17,
 
 	/* Receive Side Scaling (RSS) hash input */
-	ena_admin_rss_hash_input = 18,
+	ENA_ADMIN_RSS_HASH_INPUT = 18,
 
 	/* overlay tunnels configuration */
-	ena_admin_tunnel_config = 19,
+	ENA_ADMIN_TUNNEL_CONFIG = 19,
 
 	/* interrupt moderation: count,interval,adaptive */
-	ena_admin_interrupt_moderation = 20,
+	ENA_ADMIN_INTERRUPT_MODERATION = 20,
 
 	/* 1588v2 and Timing configuration */
-	ena_admin_1588_config = 21,
+	ENA_ADMIN_1588_CONFIG = 21,
 
 	/* End-to-End invariant CRC configuration */
-	ena_admin_e2e_crc_config = 22,
+	ENA_ADMIN_E2E_CRC_CONFIG = 22,
 
-	/*
-	 * Packet Header format templates configuration for input and
+	/* Packet Header format templates configuration for input and
 	 * output parsers
 	 */
-	ena_admin_pkt_header_templates_config = 23,
+	ENA_ADMIN_PKT_HEADER_TEMPLATES_CONFIG = 23,
 
 	/* Direct Data Placement (DDP) configuration */
-	ena_admin_ddp_config = 24,
+	ENA_ADMIN_DDP_CONFIG = 24,
 
 	/* Wake on LAN configuration */
-	ena_admin_wol_config = 25,
+	ENA_ADMIN_WOL_CONFIG = 25,
 
 	/* AENQ configuration */
-	ena_admin_aenq_config = 26,
+	ENA_ADMIN_AENQ_CONFIG = 26,
 
 	/* Link configuration */
-	ena_admin_link_config = 27,
+	ENA_ADMIN_LINK_CONFIG = 27,
 
 	/* Host attributes configuration */
-	ena_admin_host_attr_config = 28,
+	ENA_ADMIN_HOST_ATTR_CONFIG = 28,
 
 	/* Number of valid opcodes */
-	ena_admin_features_opcode_num = 32,
+	ENA_ADMIN_FEATURES_OPCODE_NUM = 32,
 };
 
 /* descriptors and headers placement */
 enum ena_admin_placement_policy_type {
 	/* descriptors and headers are in OS memory */
-	ena_admin_placement_policy_host = 1,
+	ENA_ADMIN_PLACEMENT_POLICY_HOST = 1,
 
-	/*
-	 * descriptors and headers in device memory (a.k.a Low Latency
+	/* descriptors and headers in device memory (a.k.a Low Latency
 	 * Queue)
 	 */
-	ena_admin_placement_policy_dev = 3,
+	ENA_ADMIN_PLACEMENT_POLICY_DEV = 3,
 };
 
 /* link speeds */
 enum ena_admin_link_types {
-	ena_admin_link_speed_1G = 0x1,
+	ENA_ADMIN_LINK_SPEED_1G = 0X1,
 
-	ena_admin_link_speed_2_half_G = 0x2,
+	ENA_ADMIN_LINK_SPEED_2_HALF_G = 0X2,
 
-	ena_admin_link_speed_5G = 0x4,
+	ENA_ADMIN_LINK_SPEED_5G = 0X4,
 
-	ena_admin_link_speed_10G = 0x8,
+	ENA_ADMIN_LINK_SPEED_10G = 0X8,
 
-	ena_admin_link_speed_25G = 0x10,
+	ENA_ADMIN_LINK_SPEED_25G = 0X10,
 
-	ena_admin_link_speed_40G = 0x20,
+	ENA_ADMIN_LINK_SPEED_40G = 0X20,
 
-	ena_admin_link_speed_50G = 0x40,
+	ENA_ADMIN_LINK_SPEED_50G = 0X40,
 
-	ena_admin_link_speed_100G = 0x80,
+	ENA_ADMIN_LINK_SPEED_100G = 0X80,
 
-	ena_admin_link_speed_200G = 0x100,
+	ENA_ADMIN_LINK_SPEED_200G = 0X100,
 
-	ena_admin_link_speed_400G = 0x200,
+	ENA_ADMIN_LINK_SPEED_400G = 0X200,
 };
 
 /* completion queue update policy */
 enum ena_admin_completion_policy_type {
 	/* cqe for each sq descriptor */
-	ena_admin_completion_policy_desc = 0,
+	ENA_ADMIN_COMPLETION_POLICY_DESC = 0,
 
 	/* cqe upon request in sq descriptor */
-	ena_admin_completion_policy_desc_on_denamd = 1,
+	ENA_ADMIN_COMPLETION_POLICY_DESC_ON_DENAMD = 1,
 
-	/*
-	 * current queue head pointer is updated in OS memory upon sq
+	/* current queue head pointer is updated in OS memory upon sq
 	 * descriptor request
 	 */
-	ena_admin_completion_policy_head_on_deman = 2,
+	ENA_ADMIN_COMPLETION_POLICY_HEAD_ON_DEMAN = 2,
 
-	/*
-	 * current queue head pointer is updated in OS memory for each sq
+	/* current queue head pointer is updated in OS memory for each sq
 	 * descriptor
 	 */
-	ena_admin_completion_policy_head = 3,
+	ENA_ADMIN_COMPLETION_POLICY_HEAD = 3,
 };
 
 /* type of get statistics command */
 enum ena_admin_get_stats_type {
 	/* Basic statistics */
-	ena_admin_get_stats_type_basic = 0,
+	ENA_ADMIN_GET_STATS_TYPE_BASIC = 0,
 
 	/* Extended statistics */
-	ena_admin_get_stats_type_extended = 1,
+	ENA_ADMIN_GET_STATS_TYPE_EXTENDED = 1,
 };
 
 /* scope of get statistics command */
 enum ena_admin_get_stats_scope {
-	ena_admin_specific_queue = 0,
+	ENA_ADMIN_SPECIFIC_QUEUE = 0,
 
-	ena_admin_eth_traffic = 1,
+	ENA_ADMIN_ETH_TRAFFIC = 1,
 };
 
 /* ENA Admin Queue (AQ) common descriptor */
 struct ena_admin_aq_common_desc {
 	/* word 0 : */
-	/*
-	 * command identificator to associate it with the completion
+	/* command identificator to associate it with the completion
 	 * 11:0 : command_id
 	 * 15:12 : reserved12
 	 */
@@ -307,8 +297,7 @@ struct ena_admin_aq_common_desc {
 	/* as appears in ena_aq_opcode */
 	u8 opcode;
 
-	/*
-	 * 0 : phase
+	/* 0 : phase
 	 * 1 : ctrl_data - control buffer address valid
 	 * 2 : ctrl_data_indirect - control buffer address
 	 *    points to list of pages with addresses of control
@@ -318,14 +307,12 @@ struct ena_admin_aq_common_desc {
 	u8 flags;
 };
 
-/*
- * used in ena_aq_entry. Can point directly to control data, or to a page
+/* used in ena_aq_entry. Can point directly to control data, or to a page
  * list chunk. Used also at the end of indirect mode page list chunks, for
  * chaining.
  */
 struct ena_admin_ctrl_buff_info {
-	/*
-	 * word 0 : indicates length of the buffer pointed by
+	/* word 0 : indicates length of the buffer pointed by
 	 * control_buffer_address.
 	 */
 	u32 length;
@@ -340,8 +327,7 @@ struct ena_admin_sq {
 	/* queue id */
 	u16 sq_idx;
 
-	/*
-	 * 4:0 : sq_type - 0x1 - ethernet queue; 0x2 - fabric
+	/* 4:0 : sq_type - 0x1 - ethernet queue; 0x2 - fabric
 	 *    queue; 0x3 fabric queue with RDMA; 0x4 - DPDK queue
 	 * 7:5 : sq_direction - 0x1 - Tx; 0x2 - Rx; 0x3 - SRQ
 	 */
@@ -360,8 +346,7 @@ struct ena_admin_aq_entry {
 		/* command specific inline data */
 		u32 inline_data_w1[3];
 
-		/*
-		 * words 1:3 : points to control buffer (direct or
+		/* words 1:3 : points to control buffer (direct or
 		 * indirect, chained if needed)
 		 */
 		struct ena_admin_ctrl_buff_info control_buffer;
@@ -374,8 +359,7 @@ struct ena_admin_aq_entry {
 /* ENA Admin Completion Queue (ACQ) common descriptor */
 struct ena_admin_acq_common_desc {
 	/* word 0 : */
-	/*
-	 * command identifier to associate it with the aq descriptor
+	/* command identifier to associate it with the aq descriptor
 	 * 11:0 : command_id
 	 * 15:12 : reserved12
 	 */
@@ -384,8 +368,7 @@ struct ena_admin_acq_common_desc {
 	/* status of request execution */
 	u8 status;
 
-	/*
-	 * 0 : phase
+	/* 0 : phase
 	 * 7:1 : reserved1
 	 */
 	u8 flags;
@@ -394,8 +377,7 @@ struct ena_admin_acq_common_desc {
 	/* provides additional info */
 	u16 extended_status;
 
-	/*
-	 * submission queue head index, serves as a hint what AQ entries can
+	/* submission queue head index, serves as a hint what AQ entries can
 	 *    be revoked
 	 */
 	u16 sq_head_indx;
@@ -410,8 +392,7 @@ struct ena_admin_acq_entry {
 	u32 response_specific_data[14];
 };
 
-/*
- * ENA AQ Create Submission Queue command. Placed in control buffer pointed
+/* ENA AQ Create Submission Queue command. Placed in control buffer pointed
  * by AQ entry
  */
 struct ena_admin_aq_create_sq_cmd {
@@ -419,15 +400,13 @@ struct ena_admin_aq_create_sq_cmd {
 	struct ena_admin_aq_common_desc aq_common_descriptor;
 
 	/* word 1 : */
-	/*
-	 * 4:0 : sq_type - 0x1 - ethernet queue; 0x2 - fabric
+	/* 4:0 : sq_type - 0x1 - ethernet queue; 0x2 - fabric
 	 *    queue; 0x3 fabric queue with RDMA; 0x4 - DPDK queue
 	 * 7:5 : sq_direction - 0x1 - Tx; 0x2 - Rx; 0x3 - SRQ
 	 */
 	u8 sq_identity;
 
-	/*
-	 * 0 : virtual_addressing_support - whether the
+	/* 0 : virtual_addressing_support - whether the
 	 *    specific queue is requested to handle Userland
 	 *    virtual addresses, which burdens the ENA perfom VA
 	 *    to Physical address translation
@@ -446,8 +425,7 @@ struct ena_admin_aq_create_sq_cmd {
 	 */
 	u8 sq_caps_1;
 
-	/*
-	 * 3:0 : placement_policy - Describing where the SQ
+	/* 3:0 : placement_policy - Describing where the SQ
 	 *    descriptor ring and the SQ packet headers reside:
 	 *    0x1 - descriptors and headers are in OS memory,
 	 *    0x3 - descriptors and headers in device memory
@@ -464,8 +442,7 @@ struct ena_admin_aq_create_sq_cmd {
 	 */
 	u8 sq_caps_2;
 
-	/*
-	 * 0 : is_physically_contiguous - Described if the
+	/* 0 : is_physically_contiguous - Described if the
 	 *    queue ring memory is allocated in physical
 	 *    contiguous pages or split.
 	 * 7:1 : reserved1
@@ -473,8 +450,7 @@ struct ena_admin_aq_create_sq_cmd {
 	u8 sq_caps_3;
 
 	/* word 2 : */
-	/*
-	 * associated completion queue id. This CQ must be created prior to
+	/* associated completion queue id. This CQ must be created prior to
 	 *    SQ creation
 	 */
 	u16 cq_idx;
@@ -482,15 +458,13 @@ struct ena_admin_aq_create_sq_cmd {
 	/* submission queue depth in # of entries */
 	u16 sq_depth;
 
-	/*
-	 * words 3:4 : SQ physical base address in OS memory. This field
+	/* words 3:4 : SQ physical base address in OS memory. This field
 	 * should not be used for Low Latency queues. Has to be page
 	 * aligned.
 	 */
 	struct ena_common_mem_addr sq_ba;
 
-	/*
-	 * words 5:6 : specifies queue head writeback location in OS
+	/* words 5:6 : specifies queue head writeback location in OS
 	 * memory. Valid if completion_policy is set to 0x3. Has to be
 	 * cache aligned
 	 */
@@ -509,31 +483,30 @@ struct ena_admin_aq_create_sq_cmd {
 
 /* submission queue direction */
 enum ena_admin_sq_direction {
-	ena_admin_sq_direction_tx = 1,
+	ENA_ADMIN_SQ_DIRECTION_TX = 1,
 
-	ena_admin_sq_direction_rx = 2,
+	ENA_ADMIN_SQ_DIRECTION_RX = 2,
 
 	/* Shared Receive queue */
-	ena_admin_sq_direction_srq = 3,
+	ENA_ADMIN_SQ_DIRECTION_SRQ = 3,
 };
 
 /* submission queue type */
 enum ena_admin_sq_type {
 	/* ethernet queue */
-	ena_admin_eth = 1,
+	ENA_ADMIN_ETH = 1,
 
 	/* fabric queue */
-	ena_admin_fabric = 2,
+	ENA_ADMIN_FABRIC = 2,
 
 	/* fabric queue with RDMA */
-	ena_admin_fabric_rdma = 3,
+	ENA_ADMIN_FABRIC_RDMA = 3,
 
 	/* DPDK queue */
-	ena_admin_dpdk = 4,
+	ENA_ADMIN_DPDK = 4,
 };
 
-/*
- * ENA Response for Create SQ Command. Appears in ACQ entry as
+/* ENA Response for Create SQ Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_create_sq_resp_desc {
@@ -547,27 +520,23 @@ struct ena_admin_acq_create_sq_resp_desc {
 	/* sq depth in # of entries */
 	u16 sq_actual_depth;
 
-	/*
-	 * word 3 : queue doorbell address as and offset to PCIe MMIO REG
+	/* word 3 : queue doorbell address as and offset to PCIe MMIO REG
 	 * BAR
 	 */
 	u32 sq_doorbell_offset;
 
-	/*
-	 * word 4 : low latency queue ring base address as an offset to
+	/* word 4 : low latency queue ring base address as an offset to
 	 * PCIe MMIO LLQ_MEM BAR
 	 */
 	u32 llq_descriptors_offset;
 
-	/*
-	 * word 5 : low latency queue headers' memory as an offset to PCIe
+	/* word 5 : low latency queue headers' memory as an offset to PCIe
 	 * MMIO LLQ_MEM BAR
 	 */
 	u32 llq_headers_offset;
 };
 
-/*
- * ENA AQ Destroy Submission Queue command. Placed in control buffer
+/* ENA AQ Destroy Submission Queue command. Placed in control buffer
  * pointed by AQ entry
  */
 struct ena_admin_aq_destroy_sq_cmd {
@@ -578,8 +547,7 @@ struct ena_admin_aq_destroy_sq_cmd {
 	struct ena_admin_sq sq;
 };
 
-/*
- * ENA Response for Destroy SQ Command. Appears in ACQ entry as
+/* ENA Response for Destroy SQ Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_destroy_sq_resp_desc {
@@ -593,8 +561,7 @@ struct ena_admin_aq_create_cq_cmd {
 	struct ena_admin_aq_common_desc aq_common_descriptor;
 
 	/* word 1 : */
-	/*
-	 * 4:0 : cq_type - 0x1 - eth cq; 0x2 - fabric cq; 0x3
+	/* 4:0 : cq_type - 0x1 - eth cq; 0x2 - fabric cq; 0x3
 	 *    fabric cq with RDMA; 0x4 - DPDK cq
 	 * 5 : interrupt_mode_enabled - if set, cq operates
 	 *    in interrupt mode, otherwise - polling
@@ -602,8 +569,7 @@ struct ena_admin_aq_create_cq_cmd {
 	 */
 	u8 cq_caps_1;
 
-	/*
-	 * 4:0 : cq_entry_size_words - size of CQ entry in
+	/* 4:0 : cq_entry_size_words - size of CQ entry in
 	 *    32-bit words, valid values: 4, 8.
 	 * 7:5 : reserved7
 	 */
@@ -615,15 +581,13 @@ struct ena_admin_aq_create_cq_cmd {
 	/* word 2 : msix vector assigned to this cq */
 	u32 msix_vector;
 
-	/*
-	 * words 3:4 : cq physical base address in OS memory. CQ must be
+	/* words 3:4 : cq physical base address in OS memory. CQ must be
 	 * physically contiguous
 	 */
 	struct ena_common_mem_addr cq_ba;
 };
 
-/*
- * ENA Response for Create CQ Command. Appears in ACQ entry as response
+/* ENA Response for Create CQ Command. Appears in ACQ entry as response
  * specific data
  */
 struct ena_admin_acq_create_cq_resp_desc {
@@ -640,14 +604,12 @@ struct ena_admin_acq_create_cq_resp_desc {
 	/* word 3 : doorbell address as an offset to PCIe MMIO REG BAR */
 	u32 cq_doorbell_offset;
 
-	/*
-	 * word 4 : completion head doorbell address as an offset to PCIe
+	/* word 4 : completion head doorbell address as an offset to PCIe
 	 * MMIO REG BAR
 	 */
 	u32 cq_head_db_offset;
 
-	/*
-	 * word 5 : interrupt unmask register address as an offset into
+	/* word 5 : interrupt unmask register address as an offset into
 	 * PCIe MMIO REG BAR
 	 */
 	u32 cq_interrupt_unmask_register;
@@ -655,15 +617,13 @@ struct ena_admin_acq_create_cq_resp_desc {
 	/* word 6 : value to be written into interrupt unmask register */
 	u32 cq_interrupt_unmask_value;
 
-	/*
-	 * word 7 : interrupt moderation register address as an offset into
+	/* word 7 : interrupt moderation register address as an offset into
 	 * PCIe MMIO REG BAR. 1 usec granularity
 	 */
 	u32 cq_interrupt_moderation_register;
 };
 
-/*
- * ENA AQ Destroy Completion Queue command. Placed in control buffer
+/* ENA AQ Destroy Completion Queue command. Placed in control buffer
  * pointed by AQ entry
  */
 struct ena_admin_aq_destroy_cq_cmd {
@@ -677,8 +637,7 @@ struct ena_admin_aq_destroy_cq_cmd {
 	u16 reserved1;
 };
 
-/*
- * ENA Response for Destroy CQ Command. Appears in ACQ entry as
+/* ENA Response for Destroy CQ Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_destroy_cq_resp_desc {
@@ -686,8 +645,7 @@ struct ena_admin_acq_destroy_cq_resp_desc {
 	struct ena_admin_acq_common_desc acq_common_desc;
 };
 
-/*
- * ENA AQ Suspend Submission Queue command. Placed in control buffer
+/* ENA AQ Suspend Submission Queue command. Placed in control buffer
  * pointed by AQ entry
  */
 struct ena_admin_aq_suspend_sq_cmd {
@@ -698,8 +656,7 @@ struct ena_admin_aq_suspend_sq_cmd {
 	struct ena_admin_sq sq;
 };
 
-/*
- * ENA Response for Suspend SQ Command. Appears in ACQ entry as
+/* ENA Response for Suspend SQ Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_suspend_sq_resp_desc {
@@ -707,8 +664,7 @@ struct ena_admin_acq_suspend_sq_resp_desc {
 	struct ena_admin_acq_common_desc acq_common_desc;
 };
 
-/*
- * ENA AQ Resume Submission Queue command. Placed in control buffer pointed
+/* ENA AQ Resume Submission Queue command. Placed in control buffer pointed
  * by AQ entry
  */
 struct ena_admin_aq_resume_sq_cmd {
@@ -719,8 +675,7 @@ struct ena_admin_aq_resume_sq_cmd {
 	struct ena_admin_sq sq;
 };
 
-/*
- * ENA Response for Resume SQ Command. Appears in ACQ entry as
+/* ENA Response for Resume SQ Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_resume_sq_resp_desc {
@@ -728,8 +683,7 @@ struct ena_admin_acq_resume_sq_resp_desc {
 	struct ena_admin_acq_common_desc acq_common_desc;
 };
 
-/*
- * ENA AQ Flush Submission Queue command. Placed in control buffer pointed
+/* ENA AQ Flush Submission Queue command. Placed in control buffer pointed
  * by AQ entry
  */
 struct ena_admin_aq_flush_sq_cmd {
@@ -740,8 +694,7 @@ struct ena_admin_aq_flush_sq_cmd {
 	struct ena_admin_sq sq;
 };
 
-/*
- * ENA Response for Flush SQ Command. Appears in ACQ entry as
+/* ENA Response for Flush SQ Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_flush_sq_resp_desc {
@@ -749,8 +702,7 @@ struct ena_admin_acq_flush_sq_resp_desc {
 	struct ena_admin_acq_common_desc acq_common_desc;
 };
 
-/*
- * ENA AQ Get Statistics command. Extended statistics are placed in control
+/* ENA AQ Get Statistics command. Extended statistics are placed in control
  * buffer pointed by AQ entry
  */
 struct ena_admin_aq_get_stats_cmd {
@@ -762,8 +714,7 @@ struct ena_admin_aq_get_stats_cmd {
 		/* command specific inline data */
 		u32 inline_data_w1[3];
 
-		/*
-		 * words 1:3 : points to control buffer (direct or
+		/* words 1:3 : points to control buffer (direct or
 		 * indirect, chained if needed)
 		 */
 		struct ena_admin_ctrl_buff_info control_buffer;
@@ -782,8 +733,7 @@ struct ena_admin_aq_get_stats_cmd {
 	/* queue id. used when scope is specific_queue */
 	u16 queue_idx;
 
-	/*
-	 * device id, value 0xFFFF means mine. only privileged device can get
+	/* device id, value 0xFFFF means mine. only privileged device can get
 	 *    stats of other device
 	 */
 	u16 device_id;
@@ -822,8 +772,7 @@ struct ena_admin_basic_stats {
 	u32 rx_drops_high;
 };
 
-/*
- * ENA Response for Get Statistics Command. Appears in ACQ entry as
+/* ENA Response for Get Statistics Command. Appears in ACQ entry as
  * response_specific_data
  */
 struct ena_admin_acq_get_stats_resp {
@@ -834,14 +783,12 @@ struct ena_admin_acq_get_stats_resp {
 	struct ena_admin_basic_stats basic_stats;
 };
 
-/*
- * ENA Get/Set Feature common descriptor. Appears as inline word in
+/* ENA Get/Set Feature common descriptor. Appears as inline word in
  * ena_aq_entry
  */
 struct ena_admin_get_set_feature_common_desc {
 	/* word 0 : */
-	/*
-	 * 1:0 : select - 0x1 - current value; 0x3 - default
+	/* 1:0 : select - 0x1 - current value; 0x3 - default
 	 *    value
 	 * 7:3 : reserved3
 	 */
@@ -862,8 +809,7 @@ struct ena_admin_device_attr_feature_desc {
 	/* word 1 : device version */
 	u32 device_version;
 
-	/*
-	 * word 2 : bit map of which bits are supported value of 1
+	/* word 2 : bit map of which bits are supported value of 1
 	 * indicated that this feature is supported and can perform SET/GET
 	 * for it
 	 */
@@ -872,14 +818,12 @@ struct ena_admin_device_attr_feature_desc {
 	/* word 3 :  */
 	u32 reserved3;
 
-	/*
-	 * word 4 : Indicates how many bits are used physical address
+	/* word 4 : Indicates how many bits are used physical address
 	 * access. Typically 48
 	 */
 	u32 phys_addr_width;
 
-	/*
-	 * word 5 : Indicates how many bits are used virtual address
+	/* word 5 : Indicates how many bits are used virtual address
 	 * access. Typically 48
 	 */
 	u32 virt_addr_width;
@@ -927,12 +871,6 @@ struct ena_admin_set_feature_mtu_desc {
 struct ena_admin_set_feature_host_attr_desc {
 	/* word 0 : driver version */
 	u32 driver_version;
-
-	/*
-	 * words 1:2 : 4KB of dying gasp log. This buffer is filled on
-	 * fatal error.
-	 */
-	struct ena_common_mem_addr dying_gasp_log;
 };
 
 /* ENA Interrupt Moderation metrics. */
@@ -949,15 +887,13 @@ struct ena_admin_get_feature_link_desc {
 	/* word 0 : Link speed in Mb */
 	u32 speed;
 
-	/*
-	 * word 1 : supported speeds (bit field of enum ena_admin_link
+	/* word 1 : supported speeds (bit field of enum ena_admin_link
 	 * types)
 	 */
 	u32 supported;
 
 	/* word 2 : */
-	/*
-	 * 0 : autoneg - auto negotiation
+	/* 0 : autoneg - auto negotiation
 	 * 1 : duplex - Full Duplex
 	 * 31:2 : reserved2
 	 */
@@ -976,8 +912,7 @@ struct ena_admin_set_feature_intr_moder_desc {
 	u8 reserved1;
 
 	/* word 1 : */
-	/*
-	 * 0 : enable
+	/* 0 : enable
 	 * 31:1 : reserved1
 	 */
 	u32 flags;
@@ -998,8 +933,7 @@ struct ena_admin_feature_aenq_desc {
 /* ENA Stateless Offload Feature descriptor. */
 struct ena_admin_feature_offload_desc {
 	/* word 0 : */
-	/*
-	 * Trasmit side stateless offload
+	/* Trasmit side stateless offload
 	 * 0 : TX_L3_csum_ipv4 - IPv4 checksum
 	 * 1 : TX_L4_ipv4_csum_part - TCP/UDP over IPv4
 	 *    checksum, the checksum field should be initialized
@@ -1018,8 +952,7 @@ struct ena_admin_feature_offload_desc {
 	u32 tx;
 
 	/* word 1 : */
-	/*
-	 * Receive side supported stateless offload
+	/* Receive side supported stateless offload
 	 * 0 : RX_L3_csum_ipv4 - IPv4 checksum
 	 * 1 : RX_L4_ipv4_csum - TCP/UDP/IPv4 checksum
 	 * 2 : RX_L4_ipv6_csum - TCP/UDP/IPv6 checksum
@@ -1034,25 +967,35 @@ struct ena_admin_feature_offload_desc {
 /* hash functions */
 enum ena_admin_hash_functions {
 	/* Toeplitz hash */
-	ena_admin_toeplitz = 1,
+	ENA_ADMIN_TOEPLITZ = 1,
 
 	/* CRC32 hash */
-	ena_admin_crc32 = 2,
+	ENA_ADMIN_CRC32 = 2,
+};
+
+/* ENA RSS flow hash control buffer structure */
+struct ena_admin_feature_rss_flow_hash_control {
+	/* word 0 : number of valid keys */
+	u32 keys_num;
+
+	/* word 1 :  */
+	u32 reserved;
+
+	/* Toeplitz keys */
+	u32 key[10];
 };
 
 /* ENA RSS Flow Hash Function */
 struct ena_admin_feature_rss_flow_hash_function {
 	/* word 0 : */
-	/*
-	 * supported hash functions
+	/* supported hash functions
 	 * 7:0 : funcs - supported hash functions (bitmask
 	 *    accroding to ena_admin_hash_functions)
 	 */
 	u32 supported_func;
 
 	/* word 1 : */
-	/*
-	 * selected hash func
+	/* selected hash func
 	 * 7:0 : selected_func - selected hash function
 	 *    (bitmask accroding to ena_admin_hash_functions)
 	 */
@@ -1060,60 +1003,57 @@ struct ena_admin_feature_rss_flow_hash_function {
 
 	/* word 2 : initial value */
 	u32 init_val;
-
-	/* Toeplitz keys */
-	u32 key[10];
 };
 
 /* RSS flow hash protocols */
 enum ena_admin_flow_hash_proto {
 	/* tcp/ipv4 */
-	ena_admin_rss_tcp4 = 0,
+	ENA_ADMIN_RSS_TCP4 = 0,
 
 	/* udp/ipv4 */
-	ena_admin_rss_udp4 = 1,
+	ENA_ADMIN_RSS_UDP4 = 1,
 
 	/* tcp/ipv6 */
-	ena_admin_rss_tcp6 = 2,
+	ENA_ADMIN_RSS_TCP6 = 2,
 
 	/* udp/ipv6 */
-	ena_admin_rss_udp6 = 3,
+	ENA_ADMIN_RSS_UDP6 = 3,
 
 	/* ipv4 not tcp/udp */
-	ena_admin_rss_ip4 = 4,
+	ENA_ADMIN_RSS_IP4 = 4,
 
 	/* ipv6 not tcp/udp */
-	ena_admin_rss_ip6 = 5,
+	ENA_ADMIN_RSS_IP6 = 5,
 
 	/* fragmented ipv4 */
-	ena_admin_rss_ip4_frag = 6,
+	ENA_ADMIN_RSS_IP4_FRAG = 6,
 
 	/* not ipv4/6 */
-	ena_admin_rss_not_ip = 7,
+	ENA_ADMIN_RSS_NOT_IP = 7,
 
 	/* max number of protocols */
-	ena_admin_rss_proto_count = 16,
+	ENA_ADMIN_RSS_PROTO_NUM = 16,
 };
 
 /* RSS flow hash fields */
 enum ena_admin_flow_hash_fields {
 	/* Ethernet Dest Addr */
-	ena_admin_rss_l2_da = 0,
+	ENA_ADMIN_RSS_L2_DA = 0,
 
 	/* Ethernet Src Addr */
-	ena_admin_rss_l2_sa = 1,
+	ENA_ADMIN_RSS_L2_SA = 1,
 
 	/* ipv4/6 Dest Addr */
-	ena_admin_rss_l3_da = 2,
+	ENA_ADMIN_RSS_L3_DA = 2,
 
 	/* ipv4/6 Src Addr */
-	ena_admin_rss_l3_sa = 5,
+	ENA_ADMIN_RSS_L3_SA = 5,
 
 	/* tcp/udp Dest Port */
-	ena_admin_rss_l4_dp = 6,
+	ENA_ADMIN_RSS_L4_DP = 6,
 
 	/* tcp/udp Src Port */
-	ena_admin_rss_l4_sp = 7,
+	ENA_ADMIN_RSS_L4_SP = 7,
 };
 
 /* hash input fields for flow protocol */
@@ -1122,8 +1062,7 @@ struct ena_admin_proto_input {
 	/* flow hash fields (bitwise according to ena_admin_flow_hash_fields) */
 	u16 fields;
 
-	/*
-	 * 0 : inner - for tunneled packet, select the fields
+	/* 0 : inner - for tunneled packet, select the fields
 	 *    from inner header
 	 */
 	u16 flags;
@@ -1132,23 +1071,22 @@ struct ena_admin_proto_input {
 /* ENA RSS hash control buffer structure */
 struct ena_admin_feature_rss_hash_control {
 	/* supported input fields */
-	struct ena_admin_proto_input supported_input_fields[ena_admin_rss_proto_count];
+	struct ena_admin_proto_input supported_fields[ENA_ADMIN_RSS_PROTO_NUM];
 
 	/* selected input fields */
-	struct ena_admin_proto_input selected_input_fields[ena_admin_rss_proto_count];
+	struct ena_admin_proto_input selected_fields[ENA_ADMIN_RSS_PROTO_NUM];
 
 	/* supported input fields for inner header */
-	struct ena_admin_proto_input supported_inner_input_fields[ena_admin_rss_proto_count];
+	struct ena_admin_proto_input supported_inner_fields[ENA_ADMIN_RSS_PROTO_NUM];
 
 	/* selected input fields */
-	struct ena_admin_proto_input selected_inner_input_fields[ena_admin_rss_proto_count];
+	struct ena_admin_proto_input selected_inner_fields[ENA_ADMIN_RSS_PROTO_NUM];
 };
 
 /* ENA RSS flow hash input */
 struct ena_admin_feature_rss_flow_hash_input {
 	/* word 0 : */
-	/*
-	 * supported hash input sorting
+	/* supported hash input sorting
 	 * 1 : L3_sort - support swap L3 addresses if DA
 	 *    smaller than SA
 	 * 2 : L4_sort - support swap L4 ports if DP smaller
@@ -1156,8 +1094,7 @@ struct ena_admin_feature_rss_flow_hash_input {
 	 */
 	u16 supported_input_sort;
 
-	/*
-	 * enabled hash input sorting
+	/* enabled hash input sorting
 	 * 1 : enable_L3_sort - enable swap L3 addresses if
 	 *    DA smaller than SA
 	 * 2 : enable_L4_sort - enable swap L4 ports if DP
@@ -1166,8 +1103,8 @@ struct ena_admin_feature_rss_flow_hash_input {
 	u16 enabled_input_sort;
 };
 
-/* ENA RSS redirection table entry */
-struct ena_admin_rss_redirection_table_entry {
+/* ENA RSS indirection table entry */
+struct ena_admin_rss_ind_table_entry {
 	/* word 0 : */
 	/* cq identifier */
 	u16 cq_idx;
@@ -1175,8 +1112,8 @@ struct ena_admin_rss_redirection_table_entry {
 	u16 reserved;
 };
 
-/* ENA RSS redirection table */
-struct ena_admin_feature_rss_redirection_table {
+/* ENA RSS indirection table */
+struct ena_admin_feature_rss_ind_table {
 	/* word 0 : */
 	/* min supported table size (2^min_size) */
 	u16 min_size;
@@ -1189,6 +1126,14 @@ struct ena_admin_feature_rss_redirection_table {
 	u16 size;
 
 	u16 reserved;
+
+	/* word 2 : index of the inline entry. 0xFFFFFFFF means invalid */
+	u32 inline_index;
+
+	/* words 3 : used for updating single entry, ignored when setting
+	 * the entire table through the control buffer.
+	 */
+	struct ena_admin_rss_ind_table_entry inline_entry;
 };
 
 /* ENA Get Feature command */
@@ -1196,13 +1141,18 @@ struct ena_admin_get_feat_cmd {
 	/* words 0 :  */
 	struct ena_admin_aq_common_desc aq_common_descriptor;
 
-	/* words 1 :  */
+	/* words 1:3 : points to control buffer (direct or indirect,
+	 * chained if needed)
+	 */
+	struct ena_admin_ctrl_buff_info control_buffer;
+
+	/* words 4 :  */
 	struct ena_admin_get_set_feature_common_desc feat_common;
 
-	/* words 2:15 :  */
+	/* words 5:15 :  */
 	union {
 		/* raw words */
-		u32 raw[14];
+		u32 raw[11];
 	} u;
 };
 
@@ -1231,14 +1181,14 @@ struct ena_admin_get_feat_resp {
 		/* words 2:4 : offload configuration */
 		struct ena_admin_feature_offload_desc offload;
 
-		/* words 2:14 : rss flow hash function */
+		/* words 2:4 : rss flow hash function */
 		struct ena_admin_feature_rss_flow_hash_function flow_hash_func;
 
 		/* words 2 : rss flow hash input */
 		struct ena_admin_feature_rss_flow_hash_input flow_hash_input;
 
-		/* words 2:3 : rss redirection table */
-		struct ena_admin_feature_rss_redirection_table redirection_table;
+		/* words 2:3 : rss indirection table */
+		struct ena_admin_feature_rss_ind_table ind_table;
 	} u;
 };
 
@@ -1247,34 +1197,39 @@ struct ena_admin_set_feat_cmd {
 	/* words 0 :  */
 	struct ena_admin_aq_common_desc aq_common_descriptor;
 
-	/* words 1 :  */
+	/* words 1:3 : points to control buffer (direct or indirect,
+	 * chained if needed)
+	 */
+	struct ena_admin_ctrl_buff_info control_buffer;
+
+	/* words 4 :  */
 	struct ena_admin_get_set_feature_common_desc feat_common;
 
-	/* words 2:15 :  */
+	/* words 5:15 :  */
 	union {
 		/* raw words */
-		u32 raw[14];
+		u32 raw[11];
 
-		/* words 2 : mtu size */
+		/* words 5 : mtu size */
 		struct ena_admin_set_feature_mtu_desc mtu;
 
-		/* words 2:4 : host attributes */
+		/* words 5:7 : host attributes */
 		struct ena_admin_set_feature_host_attr_desc host_attr;
 
-		/* words 2:4 : interrupt moderation */
+		/* words 5:7 : interrupt moderation */
 		struct ena_admin_set_feature_intr_moder_desc intr_moder;
 
-		/* words 2:3 : AENQ configuration */
+		/* words 5:6 : AENQ configuration */
 		struct ena_admin_feature_aenq_desc aenq;
 
-		/* words 2:14 : rss flow hash function */
+		/* words 5:17 : rss flow hash function */
 		struct ena_admin_feature_rss_flow_hash_function flow_hash_func;
 
-		/* words 2 : rss flow hash input */
+		/* words 5 : rss flow hash input */
 		struct ena_admin_feature_rss_flow_hash_input flow_hash_input;
 
-		/* words 2:3 : rss redirection table */
-		struct ena_admin_feature_rss_redirection_table redirection_table;
+		/* words 5:6 : rss indirection table */
+		struct ena_admin_feature_rss_ind_table ind_table;
 	} u;
 };
 
@@ -1313,37 +1268,37 @@ struct ena_admin_aenq_common_desc {
 /* asynchronous event notification groups */
 enum ena_admin_aenq_group {
 	/* Link State Change */
-	ena_admin_link_change = 0,
+	ENA_ADMIN_LINK_CHANGE = 0,
 
-	ena_admin_fatal_error = 1,
+	ENA_ADMIN_FATAL_ERROR = 1,
 
-	ena_admin_warning = 2,
+	ENA_ADMIN_WARNING = 2,
 
-	ena_admin_notification = 3,
+	ENA_ADMIN_NOTIFICATION = 3,
 
-	ena_admin_keep_alive = 4,
+	ENA_ADMIN_KEEP_ALIVE = 4,
 
-	ena_admin_aenq_groups_num = 5,
+	ENA_ADMIN_AENQ_GROUPS_NUM = 5,
 };
 
 /* syndrom of AENQ warning group */
 enum ena_admin_aenq_warning_syndrom {
-	ena_admin_thermal = 0,
+	ENA_ADMIN_THERMAL = 0,
 
-	ena_admin_logging_fifo = 1,
+	ENA_ADMIN_LOGGING_FIFO = 1,
 
-	ena_admin_dirty_page_logging_fifo = 2,
+	ENA_ADMIN_DIRTY_PAGE_LOGGING_FIFO = 2,
 
-	ena_admin_malicious_mmio_access = 3,
+	ENA_ADMIN_MALICIOUS_MMIO_ACCESS = 3,
 
-	ena_admin_cq_full = 4,
+	ENA_ADMIN_CQ_FULL = 4,
 };
 
 /* syndorm of AENQ notification group */
 enum ena_admin_aenq_notification_syndrom {
-	ena_admin_suspend = 0,
+	ENA_ADMIN_SUSPEND = 0,
 
-	ena_admin_resume = 1,
+	ENA_ADMIN_RESUME = 1,
 };
 
 /* ENA Asynchronous Event Notification generic descriptor.  */
@@ -1366,7 +1321,7 @@ struct ena_admin_aenq_link_change_desc {
 };
 
 /* ENA MMIO Readless response interface */
-struct ena_admin_ena_mmio_read_less_resp {
+struct ena_admin_ena_mmio_req_read_less_resp {
 	/* word 0 : */
 	/* request id */
 	u16 req_id;
@@ -1379,97 +1334,97 @@ struct ena_admin_ena_mmio_read_less_resp {
 };
 
 /* aq_common_desc */
-#define ENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK		GENMASK(12, 0)
-#define ENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK		BIT(0)
-#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_SHIFT		1
-#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_MASK		BIT(1)
-#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_SHIFT		2
-#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK		BIT(2)
+#define ENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK GENMASK(11, 0)
+#define ENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK BIT(0)
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_SHIFT 1
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_MASK BIT(1)
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_SHIFT 2
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK BIT(2)
 
 /* sq */
-#define ENA_ADMIN_SQ_SQ_TYPE_MASK		GENMASK(5, 0)
-#define ENA_ADMIN_SQ_SQ_DIRECTION_SHIFT		5
-#define ENA_ADMIN_SQ_SQ_DIRECTION_MASK		GENMASK(8, 5)
+#define ENA_ADMIN_SQ_SQ_TYPE_MASK GENMASK(4, 0)
+#define ENA_ADMIN_SQ_SQ_DIRECTION_SHIFT 5
+#define ENA_ADMIN_SQ_SQ_DIRECTION_MASK GENMASK(7, 5)
 
 /* acq_common_desc */
-#define ENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK		GENMASK(12, 0)
-#define ENA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK		BIT(0)
+#define ENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK GENMASK(11, 0)
+#define ENA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK BIT(0)
 
 /* aq_create_sq_cmd */
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_TYPE_MASK		GENMASK(5, 0)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT		5
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK		GENMASK(8, 5)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_VIRTUAL_ADDRESSING_SUPPORT_MASK		BIT(0)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_TRAFFIC_CLASS_SHIFT		1
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_TRAFFIC_CLASS_MASK		GENMASK(4, 1)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_RX_FIXED_SGL_SIZE_SHIFT		4
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_RX_FIXED_SGL_SIZE_MASK		GENMASK(8, 4)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK		GENMASK(4, 0)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT		4
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK		GENMASK(7, 4)
-#define ENA_ADMIN_AQ_CREATE_SQ_CMD_IS_PHYSICALLY_CONTIGUOUS_MASK		BIT(0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_TYPE_MASK GENMASK(4, 0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT 5
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK GENMASK(7, 5)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_VIRTUAL_ADDRESSING_SUPPORT_MASK BIT(0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_TRAFFIC_CLASS_SHIFT 1
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_TRAFFIC_CLASS_MASK GENMASK(3, 1)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_RX_FIXED_SGL_SIZE_SHIFT 4
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_RX_FIXED_SGL_SIZE_MASK GENMASK(7, 4)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK GENMASK(3, 0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT 4
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK GENMASK(6, 4)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_IS_PHYSICALLY_CONTIGUOUS_MASK BIT(0)
 
 /* aq_create_cq_cmd */
-#define ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_TYPE_MASK		GENMASK(5, 0)
-#define ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_SHIFT		5
-#define ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_MASK		BIT(5)
-#define ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK		GENMASK(5, 0)
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_TYPE_MASK GENMASK(4, 0)
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_SHIFT 5
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_MASK BIT(5)
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK GENMASK(4, 0)
 
 /* get_set_feature_common_desc */
-#define ENA_ADMIN_GET_SET_FEATURE_COMMON_DESC_SELECT_MASK		GENMASK(2, 0)
+#define ENA_ADMIN_GET_SET_FEATURE_COMMON_DESC_SELECT_MASK GENMASK(1, 0)
 
 /* get_feature_link_desc */
-#define ENA_ADMIN_GET_FEATURE_LINK_DESC_AUTONEG_MASK		BIT(0)
-#define ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_SHIFT		1
-#define ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_MASK		BIT(1)
+#define ENA_ADMIN_GET_FEATURE_LINK_DESC_AUTONEG_MASK BIT(0)
+#define ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_SHIFT 1
+#define ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_MASK BIT(1)
 
 /* set_feature_intr_moder_desc */
-#define ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_SQ_DIRECTION_MASK		GENMASK(3, 0)
-#define ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_ENABLE_MASK		BIT(0)
+#define ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_SQ_DIRECTION_MASK GENMASK(2, 0)
+#define ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_ENABLE_MASK BIT(0)
 
 /* feature_offload_desc */
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L3_CSUM_IPV4_MASK		BIT(0)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_SHIFT		1
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_MASK		BIT(1)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_FULL_SHIFT		2
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_FULL_MASK		BIT(2)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_SHIFT		3
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_MASK		BIT(3)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_FULL_SHIFT		4
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_FULL_MASK		BIT(4)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_SHIFT		5
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_MASK		BIT(5)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_SHIFT		6
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_MASK		BIT(6)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_SHIFT		7
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_MASK		BIT(7)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L3_CSUM_IPV4_MASK		BIT(0)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_SHIFT		1
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_MASK		BIT(1)
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_SHIFT		2
-#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_MASK		BIT(2)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L3_CSUM_IPV4_MASK BIT(0)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_SHIFT 1
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_MASK BIT(1)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_FULL_SHIFT 2
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_FULL_MASK BIT(2)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_SHIFT 3
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_MASK BIT(3)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_FULL_SHIFT 4
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_FULL_MASK BIT(4)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_SHIFT 5
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_MASK BIT(5)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_SHIFT 6
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_MASK BIT(6)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_SHIFT 7
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_MASK BIT(7)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L3_CSUM_IPV4_MASK BIT(0)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_SHIFT 1
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_MASK BIT(1)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_SHIFT 2
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_MASK BIT(2)
 
 /* feature_rss_flow_hash_function */
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_FUNCTION_FUNCS_MASK		GENMASK(8, 0)
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_FUNCTION_SELECTED_FUNC_MASK		GENMASK(8, 0)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_FUNCTION_FUNCS_MASK GENMASK(7, 0)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_FUNCTION_SELECTED_FUNC_MASK GENMASK(7, 0)
 
 /* proto_input */
-#define ENA_ADMIN_PROTO_INPUT_INNER_MASK		BIT(0)
+#define ENA_ADMIN_PROTO_INPUT_INNER_MASK BIT(0)
 
 /* feature_rss_flow_hash_input */
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_SHIFT		1
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_MASK		BIT(1)
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_SHIFT		2
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_MASK		BIT(2)
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L3_SORT_SHIFT		1
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L3_SORT_MASK		BIT(1)
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L4_SORT_SHIFT		2
-#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L4_SORT_MASK		BIT(2)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_SHIFT 1
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_MASK BIT(1)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_SHIFT 2
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_MASK BIT(2)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L3_SORT_SHIFT 1
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L3_SORT_MASK BIT(1)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L4_SORT_SHIFT 2
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L4_SORT_MASK BIT(2)
 
 /* aenq_common_desc */
-#define ENA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK		BIT(0)
+#define ENA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK BIT(0)
 
 /* aenq_link_change_desc */
-#define ENA_ADMIN_AENQ_LINK_CHANGE_DESC_LINK_STATUS_MASK		BIT(0)
+#define ENA_ADMIN_AENQ_LINK_CHANGE_DESC_LINK_STATUS_MASK BIT(0)
 
 #endif /*_ENA_ADMIN_H_ */
diff --git a/drivers/amazon/ena/ena_com.c b/drivers/amazon/ena/ena_com.c
index 905b3fd..1d283dd 100644
--- a/drivers/amazon/ena/ena_com.c
+++ b/drivers/amazon/ena/ena_com.c
@@ -31,20 +31,19 @@
  */
 
 #include "ena_com.h"
-#include "ena_gen_info.h"
 
 /*****************************************************************************/
 /*****************************************************************************/
 
 /* Timeout in micro-sec */
-#define ADMIN_CMD_TIMEOUT_US (10 * 1000000)
+#define ADMIN_CMD_TIMEOUT_US (1000000)
 
 #define ENA_ASYNC_QUEUE_DEPTH 4
 #define ENA_ADMIN_QUEUE_DEPTH 32
 
-/* TODO get spec version from ena_defs */
-/* Minimal spec 0.9 */
-#define MIN_ENA_VER (((0) << ENA_REGS_VERSION_MAJOR_VERSION_SHIFT) | (9))
+#define MIN_ENA_VER (((ENA_COMMON_SPEC_VERSION_MAJOR) << \
+		ENA_REGS_VERSION_MAJOR_VERSION_SHIFT) \
+		| (ENA_COMMON_SPEC_VERSION_MINOR))
 
 #define ENA_CTRL_MAJOR		0
 #define ENA_CTRL_MINOR		0
@@ -65,14 +64,11 @@
 /*****************************************************************************/
 
 enum ena_cmd_status {
-	ENA_CMD_ILLIGAL,
 	ENA_CMD_SUBMITTED,
 	ENA_CMD_COMPLETED,
 	/* Abort - canceled by the driver */
 	ENA_CMD_ABORTED,
-	/* fail - failed to execute (by the HW) */
-	ENA_CMD_FAILED
-} ____cacheline_aligned;
+};
 
 struct ena_comp_ctx {
 	struct completion wait_event;
@@ -85,14 +81,20 @@ struct ena_comp_ctx {
 	bool occupied;
 };
 
-static inline void ena_com_mem_addr_set(struct ena_common_mem_addr *ena_addr,
-					dma_addr_t addr)
+static inline int ena_com_mem_addr_set(struct ena_com_dev *ena_dev,
+				       struct ena_common_mem_addr *ena_addr,
+				       dma_addr_t addr)
 {
+	if ((addr & GENMASK_ULL(ena_dev->dma_addr_bits - 1, 0)) != addr) {
+		ena_trc_err("dma address have more bits that the device supports\n");
+		return -EINVAL;
+	}
+
 	ena_addr->mem_addr_low = (u32)addr;
-	ena_addr->mem_addr_high = (addr >> 32) & 0xffff;
+	ena_addr->mem_addr_high =
+		((addr & GENMASK_ULL(ena_dev->dma_addr_bits - 1, 32)) >> 32);
 
-	ENA_ASSERT((addr >> ENA_MAX_PHYS_ADDR_SIZE_BITS) == 0,
-		   "Invalid addr (address have more than 48 bits");
+	return 0;
 }
 
 static int ena_com_admin_init_sq(struct ena_com_admin_queue *queue)
@@ -103,8 +105,10 @@ static int ena_com_admin_init_sq(struct ena_com_admin_queue *queue)
 				   &queue->sq.dma_addr,
 				   GFP_KERNEL | __GFP_ZERO);
 
-	if (!queue->sq.entries)
+	if (!queue->sq.entries) {
+		ena_trc_err("memory allocation failed");
 		return -ENOMEM;
+	}
 
 	queue->sq.head = 0;
 	queue->sq.tail = 0;
@@ -122,8 +126,11 @@ static int ena_com_admin_init_cq(struct ena_com_admin_queue *queue)
 				   ADMIN_CQ_SIZE(queue->q_depth),
 				   &queue->cq.dma_addr,
 				   GFP_KERNEL | __GFP_ZERO);
-	if (!queue->cq.entries)
+
+	if (!queue->cq.entries)  {
+		ena_trc_err("memory allocation failed");
 		return -ENOMEM;
+	}
 
 	queue->cq.head = 0;
 	queue->cq.phase = 1;
@@ -142,16 +149,19 @@ static int ena_com_admin_init_aenq(struct ena_com_dev *dev,
 				   ADMIN_AENQ_SIZE(dev->aenq.q_depth),
 				   &dev->aenq.dma_addr,
 				   GFP_KERNEL | __GFP_ZERO);
-	if (!dev->aenq.entries)
+
+	if (!dev->aenq.entries) {
+		ena_trc_err("memory allocation failed");
 		return -ENOMEM;
+	}
 
 	dev->aenq.head = dev->aenq.q_depth;
 	dev->aenq.phase = 1;
 
-	dev->reg_bar->aenq_base_lo =
-		ENA_DMA_ADDR_TO_UINT32_LOW(dev->aenq.dma_addr);
-	dev->reg_bar->aenq_base_hi =
-		ENA_DMA_ADDR_TO_UINT32_HIGH(dev->aenq.dma_addr);
+	writel(ENA_DMA_ADDR_TO_UINT32_LOW(dev->aenq.dma_addr),
+			&dev->reg_bar->aenq_base_lo);
+	writel(ENA_DMA_ADDR_TO_UINT32_HIGH(dev->aenq.dma_addr),
+			&dev->reg_bar->aenq_base_hi);
 
 	aenq_caps = 0;
 	aenq_caps |= dev->aenq.q_depth & ENA_REGS_AENQ_CAPS_AENQ_DEPTH_MASK;
@@ -160,6 +170,11 @@ static int ena_com_admin_init_aenq(struct ena_com_dev *dev,
 		ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_MASK;
 	writel(aenq_caps, &dev->reg_bar->aenq_caps);
 
+	if (unlikely(!aenq_handlers)) {
+		ena_trc_err("aenq handlers pointer is NULL\n");
+		return -EINVAL;
+	}
+
 	dev->aenq.aenq_handlers = aenq_handlers;
 
 	return 0;
@@ -175,14 +190,12 @@ static inline void comp_ctxt_release(struct ena_com_admin_queue *queue,
 static struct ena_comp_ctx *get_comp_ctxt(struct ena_com_admin_queue *queue,
 					  u16 command_id, bool capture)
 {
-	if (unlikely(command_id >= queue->q_depth)) {
-		ena_trc_err("command id is larger than the queue size. cmd_id: %u queue size %d\n",
-			    command_id, queue->q_depth);
-		return ERR_PTR(-EINVAL);
-	}
+	ENA_ASSERT(command_id < queue->q_depth,
+		   "command id is larger than the queue size. cmd_id: %u queue size %d\n",
+		   command_id, queue->q_depth);
 
-	if (unlikely(queue->comp_ctx[command_id].occupied && capture))
-		return ERR_PTR(-ENOSPC);
+	ENA_ASSERT(!(queue->comp_ctx[command_id].occupied && capture),
+		   "Completion context is occupied");
 
 	if (capture) {
 		atomic_inc(&queue->outstanding_cmds);
@@ -215,12 +228,11 @@ static struct ena_comp_ctx *__ena_com_submit_admin_cmd(
 			    admin_queue->sq.tail,
 			    admin_queue->sq.head,
 			    admin_queue->q_depth);
+		admin_queue->stats.out_of_space++;
 		return ERR_PTR(-ENOSPC);
 	}
 
 	cmd_id = admin_queue->curr_cmd_id;
-	admin_queue->curr_cmd_id = (admin_queue->curr_cmd_id + 1) &
-		queue_size_mask;
 
 	cmd->aq_common_descriptor.flags |= admin_queue->sq.phase &
 		ENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK;
@@ -229,8 +241,6 @@ static struct ena_comp_ctx *__ena_com_submit_admin_cmd(
 		ENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK;
 
 	comp_ctx = get_comp_ctxt(admin_queue, cmd_id, true);
-	if (unlikely(IS_ERR(comp_ctx)))
-		return comp_ctx;
 
 	comp_ctx->status = ENA_CMD_SUBMITTED;
 	comp_ctx->comp_size = (u32)comp_size_in_bytes;
@@ -241,10 +251,14 @@ static struct ena_comp_ctx *__ena_com_submit_admin_cmd(
 
 	memcpy(&admin_queue->sq.entries[tail_masked], cmd, cmd_size_in_bytes);
 
+	admin_queue->curr_cmd_id = (admin_queue->curr_cmd_id + 1) &
+		queue_size_mask;
+
 	admin_queue->sq.tail++;
+	admin_queue->stats.submitted_cmd++;
 
 	if (unlikely((admin_queue->sq.tail & queue_size_mask) == 0))
-		admin_queue->sq.phase = 1 - admin_queue->sq.phase;
+		admin_queue->sq.phase = !admin_queue->sq.phase;
 
 	writel(admin_queue->sq.tail, admin_queue->sq.db_addr);
 
@@ -258,8 +272,10 @@ static inline int ena_com_init_comp_ctxt(struct ena_com_admin_queue *queue)
 	u16 i;
 
 	queue->comp_ctx = devm_kzalloc(queue->q_dmadev, size, GFP_KERNEL);
-	if (unlikely(!queue->comp_ctx))
+	if (unlikely(!queue->comp_ctx)) {
+		ena_trc_err("memory allocation failed");
 		return -ENOMEM;
+	}
 
 	for (i = 0; i < queue->q_depth; i++) {
 		comp_ctx = get_comp_ctxt(queue, i, false);
@@ -300,11 +316,14 @@ static int ena_com_init_io_sq(struct ena_com_dev *ena_dev,
 
 	memset(&io_sq->desc_addr, 0x0, sizeof(struct ena_com_io_desc_addr));
 
-	size = (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
-			IO_TX_SQ_SIZE(io_sq->q_depth) :
-			IO_RX_SQ_SIZE(io_sq->q_depth);
+	io_sq->desc_entry_size =
+		(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+		sizeof(struct ena_eth_io_tx_desc) :
+		sizeof(struct ena_eth_io_rx_desc);
+
+	size = io_sq->desc_entry_size * io_sq->q_depth;
 
-	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+	if (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST)
 		io_sq->desc_addr.virt_addr =
 			dma_alloc_coherent(ena_dev->dmadev,
 					   size,
@@ -314,18 +333,15 @@ static int ena_com_init_io_sq(struct ena_com_dev *ena_dev,
 		io_sq->desc_addr.virt_addr =
 			devm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);
 
-	if (!io_sq->desc_addr.virt_addr)
+	if (!io_sq->desc_addr.virt_addr) {
+		ena_trc_err("memory allocation failed");
 		return -ENOMEM;
+	}
 
 	io_sq->tail = 0;
 	io_sq->next_to_comp = 0;
 	io_sq->phase = 1;
 
-	io_sq->desc_entry_size =
-		(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
-		sizeof(struct ena_eth_io_tx_desc) :
-		sizeof(struct ena_eth_io_rx_desc);
-
 	return 0;
 }
 
@@ -336,27 +352,28 @@ static int ena_com_init_io_cq(struct ena_com_dev *ena_dev,
 
 	memset(&io_cq->cdesc_addr, 0x0, sizeof(struct ena_com_io_desc_addr));
 
-	size = (io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
-			IO_TX_CQ_SIZE(io_cq->q_depth) :
-			IO_RX_CQ_SIZE(io_cq->q_depth);
+	/* Use the basic completion descriptor for Rx */
+	io_cq->cdesc_entry_size_in_bytes =
+		(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+		sizeof(struct ena_eth_io_tx_cdesc) :
+		sizeof(struct ena_eth_io_rx_cdesc_base);
+
+	size = io_cq->cdesc_entry_size_in_bytes * io_cq->q_depth;
 
 	io_cq->cdesc_addr.virt_addr =
 		dma_alloc_coherent(ena_dev->dmadev,
 				   size,
 				   &io_cq->cdesc_addr.phys_addr,
 				   GFP_KERNEL | __GFP_ZERO);
-	if (!io_cq->cdesc_addr.virt_addr)
+
+	if (!io_cq->cdesc_addr.virt_addr) {
+		ena_trc_err("memory allocation failed");
 		return -ENOMEM;
+	}
 
 	io_cq->phase = 1;
 	io_cq->head = 0;
 
-	/* Use the basic completion descriptor for Rx */
-	io_cq->cdesc_entry_size_in_bytes =
-		(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
-		sizeof(struct ena_eth_io_tx_cdesc) :
-		sizeof(struct ena_eth_io_rx_cdesc_base);
-
 	return 0;
 }
 
@@ -371,14 +388,15 @@ static void ena_com_handle_single_admin_completion(
 		ENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK;
 
 	comp_ctx = get_comp_ctxt(admin_queue, cmd_id, false);
-	ENA_ASSERT(comp_ctx, "null comp_ctx\n");
+
 	comp_ctx->status = ENA_CMD_COMPLETED;
 	comp_ctx->comp_status = cqe->acq_common_descriptor.status;
 
 	if (comp_ctx->user_cqe)
 		memcpy(comp_ctx->user_cqe, (void *)cqe, comp_ctx->comp_size);
 
-	complete(&comp_ctx->wait_event);
+	if (!admin_queue->polling)
+		complete(&comp_ctx->wait_event);
 }
 
 static void ena_com_handle_admin_completion(
@@ -412,6 +430,7 @@ static void ena_com_handle_admin_completion(
 	admin_queue->cq.head += comp_num;
 	admin_queue->cq.phase = phase;
 	admin_queue->sq.head += comp_num;
+	admin_queue->stats.completed_cmd += comp_num;
 }
 
 static int ena_com_comp_status_to_errno(u8 comp_status)
@@ -419,19 +438,20 @@ static int ena_com_comp_status_to_errno(u8 comp_status)
 	if (unlikely(comp_status != 0))
 		ena_trc_err("admin command failed[%u]\n", comp_status);
 
-	if (unlikely(comp_status > ena_admin_unknown_error))
+	if (unlikely(comp_status > ENA_ADMIN_UNKNOWN_ERROR))
 		return -EINVAL;
 
 	switch (comp_status) {
-	case ena_admin_success:
+	case ENA_ADMIN_SUCCESS:
 		return 0;
-	case ena_admin_resource_allocation_failure:
+	case ENA_ADMIN_RESOURCE_ALLOCATION_FAILURE:
 		return -ENOMEM;
-	case ena_admin_bad_opcode:
-	case ena_admin_unsupported_opcode:
-	case ena_admin_malformed_request:
-	case ena_admin_illegal_parameter:
-	case ena_admin_unknown_error:
+	case ENA_ADMIN_UNSUPPORTED_OPCODE:
+		return -EPERM;
+	case ENA_ADMIN_BAD_OPCODE:
+	case ENA_ADMIN_MALFORMED_REQUEST:
+	case ENA_ADMIN_ILLEGAL_PARAMETER:
+	case ENA_ADMIN_UNKNOWN_ERROR:
 		return -EINVAL;
 	}
 
@@ -450,9 +470,14 @@ static int ena_com_wait_and_process_admin_cq_polling(
 
 	while (comp_ctx->status == ENA_CMD_SUBMITTED) {
 		if (((uint32_t)jiffies_to_usecs(jiffies)) > timeout) {
+			ena_trc_err("Wait for completion (polling) timeout\n");
 			/* ENA didn't have any completion */
+			spin_lock_irqsave(&admin_queue->q_lock, flags);
+			admin_queue->stats.no_completion++;
 			admin_queue->running_state = false;
-			ret = -EPERM;
+			spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+
+			ret = -ETIME;
 			goto err;
 		}
 
@@ -465,6 +490,9 @@ static int ena_com_wait_and_process_admin_cq_polling(
 
 	if (unlikely(comp_ctx->status == ENA_CMD_ABORTED)) {
 		ena_trc_err("Command was aborted\n");
+		spin_lock_irqsave(&admin_queue->q_lock, flags);
+		admin_queue->stats.aborted_cmd++;
+		spin_unlock_irqrestore(&admin_queue->q_lock, flags);
 		ret = -ENODEV;
 		goto err;
 	}
@@ -474,7 +502,6 @@ static int ena_com_wait_and_process_admin_cq_polling(
 
 	ret = ena_com_comp_status_to_errno(comp_ctx->comp_status);
 err:
-
 	comp_ctxt_release(admin_queue, comp_ctx);
 	return ret;
 }
@@ -489,25 +516,26 @@ static int ena_com_wait_and_process_admin_cq_interrupts(
 	wait_for_completion_timeout(&comp_ctx->wait_event,
 				    usecs_to_jiffies(ADMIN_CMD_TIMEOUT_US));
 
-	/* We have here 3 scenarios.
-	 * 1) Timeout expired but nothing happened
-	 * 2) The command was completed but we didn't get the MSI-X interrupt
-	 * 3) The command completion and MSI-X were received successfully.
+	/* In case the command wasn't completed find out the root cause.
+	 * There might be 2 kinds of errors
+	 * 1) No completion (timeout reached)
+	 * 2) There is completion but the device didn't get any msi-x interrupt.
 	 */
 	if (unlikely(comp_ctx->status == ENA_CMD_SUBMITTED)) {
 		spin_lock_irqsave(&admin_queue->q_lock, flags);
 		ena_com_handle_admin_completion(admin_queue);
+		admin_queue->stats.no_completion++;
 		spin_unlock_irqrestore(&admin_queue->q_lock, flags);
 
 		if (comp_ctx->status == ENA_CMD_COMPLETED)
 			ena_trc_err("The ena device have completion but the driver didn't receive any MSI-X interrupt (cmd %d)\n",
 				    comp_ctx->cmd_opcode);
 		else
-			ena_trc_err("The ena device doensn't send any completion for the cmd (cmd %d)\n",
-				    comp_ctx->cmd_opcode);
+			ena_trc_err("The ena device doesn't send any completion for the admin cmd %d status %d\n",
+				    comp_ctx->cmd_opcode, comp_ctx->status);
 
 		admin_queue->running_state = false;
-		ret = -EPERM;
+		ret = -ETIME;
 		goto err;
 	}
 
@@ -525,7 +553,7 @@ static u32 ena_com_reg_bar_read32(struct ena_com_dev *ena_dev,
 				  u16 offset)
 {
 	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
-	volatile struct ena_admin_ena_mmio_read_less_resp *read_resp =
+	volatile struct ena_admin_ena_mmio_req_read_less_resp *read_resp =
 		mmio_read->read_resp;
 	u32 mmio_read_reg, ret;
 	unsigned long flags;
@@ -543,12 +571,17 @@ static u32 ena_com_reg_bar_read32(struct ena_com_dev *ena_dev,
 	mmio_read->seq_num++;
 
 	read_resp->req_id = mmio_read->seq_num + 0xDEAD;
-	mmio_read_reg = (offset << ENA_REGS_MMIO_READ_REG_OFF_SHIFT) &
-			ENA_REGS_MMIO_READ_REG_OFF_MASK;
+	mmio_read_reg = (offset << ENA_REGS_MMIO_REG_READ_REG_OFF_SHIFT) &
+			ENA_REGS_MMIO_REG_READ_REG_OFF_MASK;
 	mmio_read_reg |= mmio_read->seq_num &
-			ENA_REGS_MMIO_READ_REQ_ID_MASK;
+			ENA_REGS_MMIO_REG_READ_REQ_ID_MASK;
 
-	writel(mmio_read_reg, &ena_dev->reg_bar->mmio_read);
+	/* make sure read_resp->req_id get updated before the hw can write
+	 * there
+	 */
+	wmb();
+
+	writel(mmio_read_reg, &ena_dev->reg_bar->mmio_reg_read);
 
 	for (i = 0; i < ENA_REG_READ_TIMEOUT; i++) {
 		if (read_resp->req_id == mmio_read->seq_num)
@@ -590,10 +623,10 @@ static int ena_com_wait_and_process_admin_cq(
 {
 	if (admin_queue->polling)
 		return ena_com_wait_and_process_admin_cq_polling(comp_ctx,
-			admin_queue);
-	else
-		return ena_com_wait_and_process_admin_cq_interrupts(comp_ctx,
-			admin_queue);
+								 admin_queue);
+
+	return ena_com_wait_and_process_admin_cq_interrupts(comp_ctx,
+							    admin_queue);
 }
 
 static int ena_com_destroy_io_sq(struct ena_com_dev *ena_dev,
@@ -608,16 +641,16 @@ static int ena_com_destroy_io_sq(struct ena_com_dev *ena_dev,
 	memset(&destroy_cmd, 0x0, sizeof(struct ena_admin_aq_destroy_sq_cmd));
 
 	if (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)
-		direction = ena_admin_sq_direction_tx;
+		direction = ENA_ADMIN_SQ_DIRECTION_TX;
 	else
-		direction = ena_admin_sq_direction_rx;
+		direction = ENA_ADMIN_SQ_DIRECTION_RX;
 
 	destroy_cmd.sq.sq_identity |= (direction <<
 		ENA_ADMIN_SQ_SQ_DIRECTION_SHIFT) &
 		ENA_ADMIN_SQ_SQ_DIRECTION_MASK;
 
 	destroy_cmd.sq.sq_idx = io_sq->idx;
-	destroy_cmd.aq_common_descriptor.opcode = ena_admin_destroy_sq;
+	destroy_cmd.aq_common_descriptor.opcode = ENA_ADMIN_DESTROY_SQ;
 
 	ret = ena_com_execute_admin_command(admin_queue,
 					    (struct ena_admin_aq_entry *)&destroy_cmd,
@@ -638,23 +671,20 @@ static void ena_com_io_queue_free(struct ena_com_dev *ena_dev,
 	size_t size;
 
 	if (io_cq->cdesc_addr.virt_addr) {
-		size = (io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
-				IO_TX_CQ_SIZE(io_cq->q_depth) :
-				IO_RX_CQ_SIZE(io_cq->q_depth);
+		size = io_cq->cdesc_entry_size_in_bytes * io_cq->q_depth;
 
 		dma_free_coherent(ena_dev->dmadev,
 				  size,
 				  io_cq->cdesc_addr.virt_addr,
 				  io_cq->cdesc_addr.phys_addr);
+
 		io_cq->cdesc_addr.virt_addr = NULL;
 	}
 
 	if (io_sq->desc_addr.virt_addr) {
-		size = (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
-				IO_TX_SQ_SIZE(io_sq->q_depth) :
-				IO_RX_SQ_SIZE(io_sq->q_depth);
+		size = io_sq->desc_entry_size * io_sq->q_depth;
 
-		if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+		if (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST)
 			dma_free_coherent(ena_dev->dmadev,
 					  size,
 					  io_sq->desc_addr.virt_addr,
@@ -690,9 +720,24 @@ static int wait_for_reset_state(struct ena_com_dev *ena_dev,
 	return -ETIME;
 }
 
-static int ena_com_get_feature(struct ena_com_dev *ena_dev,
-			       struct ena_admin_get_feat_resp *get_resp,
-			       enum ena_admin_aq_feature_id feature_id)
+static bool ena_com_check_supported_feature_id(struct ena_com_dev *ena_dev,
+					       enum ena_admin_aq_feature_id feature_id)
+{
+	u32 feature_mask = 1 << feature_id;
+
+	/* Device attributes is always supported */
+	if ((feature_id != ENA_ADMIN_DEVICE_ATTRIBUTES) &&
+	    !(ena_dev->supported_features & feature_mask))
+		return false;
+
+	return true;
+}
+
+static int ena_com_get_feature_ex(struct ena_com_dev *ena_dev,
+				  struct ena_admin_get_feat_resp *get_resp,
+				  enum ena_admin_aq_feature_id feature_id,
+				  dma_addr_t control_buf_dma_addr,
+				  u32 control_buff_size)
 {
 	struct ena_com_admin_queue *admin_queue;
 	struct ena_admin_get_feat_cmd get_cmd;
@@ -702,11 +747,32 @@ static int ena_com_get_feature(struct ena_com_dev *ena_dev,
 		ena_trc_err("%s : ena_dev is NULL\n", __func__);
 		return -ENODEV;
 	}
+
+	if (!ena_com_check_supported_feature_id(ena_dev, feature_id)) {
+		ena_trc_err("Feature %d isn't supported\n", feature_id);
+		return -EPERM;
+	}
+
 	memset(&get_cmd, 0x0, sizeof(get_cmd));
 	admin_queue = &ena_dev->admin_queue;
 
-	get_cmd.aq_common_descriptor.opcode = ena_admin_get_feature;
-	get_cmd.aq_common_descriptor.flags = 0;
+	get_cmd.aq_common_descriptor.opcode = ENA_ADMIN_GET_FEATURE;
+
+	if (control_buff_size)
+		get_cmd.aq_common_descriptor.flags =
+			ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;
+	else
+		get_cmd.aq_common_descriptor.flags = 0;
+
+	ret = ena_com_mem_addr_set(ena_dev,
+				   &get_cmd.control_buffer.address,
+				   control_buf_dma_addr);
+	if (unlikely(ret)) {
+		ena_trc_err("memory address set failed\n");
+		return ret;
+	}
+
+	get_cmd.control_buffer.length = control_buff_size;
 
 	get_cmd.feat_common.feature_id = feature_id;
 
@@ -719,11 +785,129 @@ static int ena_com_get_feature(struct ena_com_dev *ena_dev,
 					    sizeof(*get_resp));
 
 	if (unlikely(ret))
-		ena_trc_err("Failed to get feature. error: %d\n", ret);
+		ena_trc_err("Failed to submit get_feature command %d error: %d\n",
+			    feature_id, ret);
 
 	return ret;
 }
 
+static int ena_com_get_feature(struct ena_com_dev *ena_dev,
+			       struct ena_admin_get_feat_resp *get_resp,
+			       enum ena_admin_aq_feature_id feature_id)
+{
+	return ena_com_get_feature_ex(ena_dev,
+				      get_resp,
+				      feature_id,
+				      0,
+				      0);
+}
+
+static int ena_com_hash_key_allocate(struct ena_com_dev *ena_dev)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+
+	rss->hash_key = dma_alloc_coherent(ena_dev->dmadev,
+					   sizeof(*rss->hash_key),
+					   &rss->hash_key_dma_addr,
+					   GFP_KERNEL | __GFP_ZERO);
+
+	if (unlikely(!rss->hash_key))
+		return -ENOMEM;
+
+	return 0;
+}
+
+static int ena_com_hash_key_destroy(struct ena_com_dev *ena_dev)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+
+	if (rss->hash_key)
+		dma_free_coherent(ena_dev->dmadev,
+				  sizeof(*rss->hash_key),
+				  rss->hash_key,
+				  rss->hash_key_dma_addr);
+	return 0;
+}
+
+static int ena_com_hash_ctrl_init(struct ena_com_dev *ena_dev)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+
+	rss->hash_ctrl = dma_alloc_coherent(ena_dev->dmadev,
+					    sizeof(*rss->hash_ctrl),
+					    &rss->hash_ctrl_dma_addr,
+					    GFP_KERNEL | __GFP_ZERO);
+
+	return 0;
+}
+
+static int ena_com_hash_ctrl_destroy(struct ena_com_dev *ena_dev)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+
+	if (rss->hash_ctrl)
+		dma_free_coherent(ena_dev->dmadev,
+				  sizeof(*rss->hash_ctrl),
+				  rss->hash_ctrl,
+				  rss->hash_ctrl_dma_addr);
+
+	return 0;
+}
+
+static int ena_com_indirect_table_allocate(struct ena_com_dev *ena_dev,
+					   u16 log_size)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_get_feat_resp get_resp;
+	size_t tbl_size;
+	int ret;
+
+	ret = ena_com_get_feature(ena_dev, &get_resp,
+				  ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG);
+	if (unlikely(ret))
+		return ret;
+
+	if ((get_resp.u.ind_table.min_size > log_size) ||
+	    (get_resp.u.ind_table.max_size < log_size)) {
+		ena_trc_err("indirect table size doesn't fit. requested size: %d while min is:%d and max %d\n",
+			    1 << log_size,
+			    1 << get_resp.u.ind_table.min_size,
+			    1 << get_resp.u.ind_table.max_size);
+		return -EINVAL;
+	}
+
+	tbl_size = (1 << log_size) * sizeof(struct ena_admin_rss_ind_table_entry);
+
+	rss->rss_ind_tbl =
+		dma_alloc_coherent(ena_dev->dmadev,
+				   tbl_size,
+				   &rss->rss_ind_tbl_dma_addr,
+				   GFP_KERNEL | __GFP_ZERO);
+
+	if (unlikely(!rss->rss_ind_tbl)) {
+		rss->tbl_log_size = 0;
+		return -ENOMEM;
+	}
+
+	rss->tbl_log_size = log_size;
+
+	return 0;
+}
+
+static int ena_com_indirect_table_destroy(struct ena_com_dev *ena_dev)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	size_t tbl_size = (1 << rss->tbl_log_size) *
+		sizeof(struct ena_admin_rss_ind_table_entry);
+
+	if (rss->rss_ind_tbl)
+		dma_free_coherent(ena_dev->dmadev,
+				  tbl_size,
+				  rss->rss_ind_tbl,
+				  rss->rss_ind_tbl_dma_addr);
+	return 0;
+}
+
 /*****************************************************************************/
 /*******************************      API       ******************************/
 /*****************************************************************************/
@@ -758,47 +942,31 @@ int ena_com_create_io_sq(struct ena_com_dev *ena_dev,
 	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
 	struct ena_admin_aq_create_sq_cmd create_cmd;
 	struct ena_admin_acq_create_sq_resp_desc cmd_completion;
-	u8 direction, policy;
+	u8 direction;
 	int ret;
 
 	memset(&create_cmd, 0x0, sizeof(struct ena_admin_aq_create_sq_cmd));
 
-	create_cmd.aq_common_descriptor.opcode = ena_admin_create_sq;
+	create_cmd.aq_common_descriptor.opcode = ENA_ADMIN_CREATE_SQ;
 	create_cmd.aq_common_descriptor.flags = 0;
 
 	create_cmd.sq_identity = 0;
-	create_cmd.sq_identity |= ena_admin_eth &
+	create_cmd.sq_identity |= ENA_ADMIN_ETH &
 		ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_TYPE_MASK;
 
 	if (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)
-		direction = ena_admin_sq_direction_tx;
+		direction = ENA_ADMIN_SQ_DIRECTION_TX;
 	else
-		direction = ena_admin_sq_direction_rx;
+		direction = ENA_ADMIN_SQ_DIRECTION_RX;
 
 	create_cmd.sq_identity |= (direction <<
 		ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT) &
 		ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK;
 
-	switch (io_sq->mem_queue_type) {
-	case ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY:
-		policy = ena_admin_placement_policy_dev;
-		break;
-	case ENA_MEM_QUEUE_TYPE_MIXED_MEMORY:
-		ena_trc_err("Mixed mode placement policy is currently unsupported");
-		return -EINVAL;
-	case ENA_MEM_QUEUE_TYPE_HOST_MEMORY:
-		policy = ena_admin_placement_policy_host;
-		break;
-	default:
-		ena_trc_err("Invalid placement policy %u\n",
-			    io_sq->mem_queue_type);
-		return -EINVAL;
-	}
-
-	create_cmd.sq_caps_2 |= policy &
+	create_cmd.sq_caps_2 |= io_sq->mem_queue_type &
 		ENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK;
 
-	create_cmd.sq_caps_2 |= (ena_admin_completion_policy_desc <<
+	create_cmd.sq_caps_2 |= (ENA_ADMIN_COMPLETION_POLICY_DESC <<
 		ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT) &
 		ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK;
 
@@ -808,9 +976,15 @@ int ena_com_create_io_sq(struct ena_com_dev *ena_dev,
 	create_cmd.cq_idx = cq_idx;
 	create_cmd.sq_depth = io_sq->q_depth;
 
-	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
-		ena_com_mem_addr_set(&create_cmd.sq_ba,
-				     io_sq->desc_addr.phys_addr);
+	if (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST) {
+		ret = ena_com_mem_addr_set(ena_dev,
+					   &create_cmd.sq_ba,
+					   io_sq->desc_addr.phys_addr);
+		if (unlikely(ret)) {
+			ena_trc_err("memory address set failed\n");
+			return ret;
+		}
+	}
 
 	ret = ena_com_execute_admin_command(admin_queue,
 					    (struct ena_admin_aq_entry *)&create_cmd,
@@ -829,20 +1003,21 @@ int ena_com_create_io_sq(struct ena_com_dev *ena_dev,
 		ena_trc_err("sq depth mismatch: requested[%u], result[%u]\n",
 			    io_sq->q_depth,
 			    cmd_completion.sq_actual_depth);
+		ena_com_destroy_io_sq(ena_dev, io_sq);
 		return -ENOSPC;
 	}
 
 	io_sq->db_addr = (u32 *)((u8 *)ena_dev->reg_bar +
 		cmd_completion.sq_doorbell_offset);
 
-	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY)
+	if (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {
 		io_sq->header_addr = (u8 *)((u8 *)ena_dev->mem_bar +
 				cmd_completion.llq_headers_offset);
 
-	if (io_sq->mem_queue_type != ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
 		io_sq->desc_addr.pbuf_dev_addr =
 			(u8 *)((u8 *)ena_dev->mem_bar +
 			cmd_completion.llq_descriptors_offset);
+	}
 
 	ena_trc_dbg("created sq[%u], depth[%u]\n", io_sq->idx, io_sq->q_depth);
 
@@ -859,9 +1034,9 @@ int ena_com_create_io_cq(struct ena_com_dev *ena_dev,
 
 	memset(&create_cmd, 0x0, sizeof(struct ena_admin_aq_create_cq_cmd));
 
-	create_cmd.aq_common_descriptor.opcode = ena_admin_create_cq;
+	create_cmd.aq_common_descriptor.opcode = ENA_ADMIN_CREATE_CQ;
 
-	create_cmd.cq_caps_1 |= ena_admin_eth &
+	create_cmd.cq_caps_1 |= ENA_ADMIN_ETH &
 		ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_TYPE_MASK;
 	create_cmd.cq_caps_2 |= (io_cq->cdesc_entry_size_in_bytes / 4) &
 		ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK;
@@ -871,7 +1046,13 @@ int ena_com_create_io_cq(struct ena_com_dev *ena_dev,
 	create_cmd.msix_vector = io_cq->msix_vector;
 	create_cmd.cq_depth = io_cq->q_depth;
 
-	ena_com_mem_addr_set(&create_cmd.cq_ba, io_cq->cdesc_addr.phys_addr);
+	ret = ena_com_mem_addr_set(ena_dev,
+				   &create_cmd.cq_ba,
+				   io_cq->cdesc_addr.phys_addr);
+	if (unlikely(ret)) {
+		ena_trc_err("memory address set failed\n");
+		return ret;
+	}
 
 	ret = ena_com_execute_admin_command(admin_queue,
 					    (struct ena_admin_aq_entry *)&create_cmd,
@@ -890,6 +1071,7 @@ int ena_com_create_io_cq(struct ena_com_dev *ena_dev,
 	if (io_cq->q_depth != cmd_completion.cq_actual_depth) {
 		ena_trc_err("completion actual queue size (%d) is differ from requested size (%d)\n",
 			    cmd_completion.cq_actual_depth, io_cq->q_depth);
+		ena_com_destroy_io_cq(ena_dev, io_cq);
 		return -ENOSPC;
 	}
 
@@ -924,6 +1106,9 @@ void ena_com_abort_admin_commands(struct ena_com_dev *ena_dev)
 	struct ena_comp_ctx *comp_ctx;
 	u16 i;
 
+	if (!admin_queue->comp_ctx)
+		return;
+
 	for (i = 0; i < admin_queue->q_depth; i++) {
 		comp_ctx = get_comp_ctxt(admin_queue, i, false);
 		comp_ctx->status = ENA_CMD_ABORTED;
@@ -962,7 +1147,7 @@ int ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,
 	memset(&destroy_cmd, 0x0, sizeof(struct ena_admin_aq_destroy_sq_cmd));
 
 	destroy_cmd.cq_idx = io_cq->idx;
-	destroy_cmd.aq_common_descriptor.opcode = ena_admin_destroy_cq;
+	destroy_cmd.aq_common_descriptor.opcode = ENA_ADMIN_DESTROY_CQ;
 
 	ret = ena_com_execute_admin_command(admin_queue,
 					    (struct ena_admin_aq_entry *)&destroy_cmd,
@@ -976,6 +1161,11 @@ int ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,
 	return ret;
 }
 
+bool ena_com_get_admin_running_state(struct ena_com_dev *ena_dev)
+{
+	return ena_dev->admin_queue.running_state;
+}
+
 void ena_com_set_admin_running_state(struct ena_com_dev *ena_dev, bool state)
 {
 	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
@@ -997,6 +1187,54 @@ void ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev)
 	writel(ena_dev->aenq.q_depth, &ena_dev->reg_bar->aenq_head_db);
 }
 
+int ena_com_set_aenq_config(struct ena_com_dev *ena_dev, u32 groups_flag)
+{
+	struct ena_com_admin_queue *admin_queue;
+	struct ena_admin_set_feat_cmd cmd;
+	struct ena_admin_set_feat_resp resp;
+	struct ena_admin_get_feat_resp get_resp;
+	int ret = 0;
+
+	if (unlikely(!ena_dev)) {
+		ena_trc_err("%s : ena_dev is NULL\n", __func__);
+		return -ENODEV;
+	}
+
+	ret = ena_com_get_feature(ena_dev, &get_resp, ENA_ADMIN_AENQ_CONFIG);
+	if (ret) {
+		ena_trc_info("Can't get aenq configuration\n");
+		return ret;
+	}
+
+	if ((get_resp.u.aenq.supported_groups & groups_flag) != groups_flag) {
+		ena_trc_info("Trying to set unsupported aenq events. supported flag: %x asked flag: %x\n",
+			     get_resp.u.aenq.supported_groups,
+			     groups_flag);
+		return -EPERM;
+	}
+
+	memset(&cmd, 0x0, sizeof(cmd));
+	admin_queue = &ena_dev->admin_queue;
+
+	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
+	cmd.aq_common_descriptor.flags = 0;
+	cmd.feat_common.feature_id = ENA_ADMIN_AENQ_CONFIG;
+	cmd.u.aenq.supported_groups = groups_flag;
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&cmd,
+					    sizeof(cmd),
+					    (struct ena_admin_acq_entry *)&resp,
+					    sizeof(resp));
+
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to config AENQ ret: %d\n", ret);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
 int ena_com_get_dma_width(struct ena_com_dev *ena_dev)
 {
 	u32 caps = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);
@@ -1012,7 +1250,12 @@ int ena_com_get_dma_width(struct ena_com_dev *ena_dev)
 
 	ena_trc_dbg("ENA dma width: %d\n", width);
 
-	ENA_ASSERT(width > 0, "Invalid dma width: %d\n", width);
+	if ((width < 32) || width > ENA_MAX_PHYS_ADDR_SIZE_BITS) {
+		ena_trc_err("DMA width illegal value: %d\n", width);
+		return -EINVAL;
+	}
+
+	ena_dev->dma_addr_bits = width;
 
 	return width;
 }
@@ -1136,8 +1379,10 @@ void ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev)
 {
 	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
 
-	ena_dev->reg_bar->mmio_resp_lo = ENA_DMA_ADDR_TO_UINT32_LOW(0x0);
-	ena_dev->reg_bar->mmio_resp_hi = ENA_DMA_ADDR_TO_UINT32_HIGH(0x0);
+	writel(ENA_DMA_ADDR_TO_UINT32_LOW(0x0),
+			&ena_dev->reg_bar->mmio_resp_lo);
+	writel(ENA_DMA_ADDR_TO_UINT32_HIGH(0x0),
+			&ena_dev->reg_bar->mmio_resp_hi);
 
 	dma_free_coherent(ena_dev->dmadev,
 			  sizeof(*mmio_read->read_resp),
@@ -1150,11 +1395,13 @@ void ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev)
 void ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev)
 {
 	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
+	dma_addr_t resp_dma_addr;
 
-	ena_dev->reg_bar->mmio_resp_lo =
-		ENA_DMA_ADDR_TO_UINT32_LOW(mmio_read->read_resp_dma_addr);
-	ena_dev->reg_bar->mmio_resp_hi =
-		ENA_DMA_ADDR_TO_UINT32_HIGH(mmio_read->read_resp_dma_addr);
+	resp_dma_addr = mmio_read->read_resp_dma_addr;
+	writel(ENA_DMA_ADDR_TO_UINT32_LOW(resp_dma_addr),
+			&ena_dev->reg_bar->mmio_resp_lo);
+	writel(ENA_DMA_ADDR_TO_UINT32_HIGH(resp_dma_addr),
+			&ena_dev->reg_bar->mmio_resp_hi);
 }
 
 int ena_com_admin_init(struct ena_com_dev *ena_dev,
@@ -1165,9 +1412,6 @@ int ena_com_admin_init(struct ena_com_dev *ena_dev,
 	u32 aq_caps, acq_caps, dev_sts;
 	int ret;
 
-	ena_trc_info("ena_defs : Version:[%s] Build date [%s]",
-		     ENA_GEN_COMMIT, ENA_GEN_DATE);
-
 	dev_sts = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);
 
 	if (unlikely(dev_sts == 0xFFFFFFFF)) {
@@ -1205,15 +1449,15 @@ int ena_com_admin_init(struct ena_com_dev *ena_dev,
 
 	admin_queue->sq.db_addr = (void __iomem *)&ena_dev->reg_bar->aq_db;
 
-	ena_dev->reg_bar->aq_base_lo =
-		ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->sq.dma_addr);
-	ena_dev->reg_bar->aq_base_hi =
-		ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->sq.dma_addr);
+	writel(ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->sq.dma_addr),
+			&ena_dev->reg_bar->aq_base_lo);
+	writel(ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->sq.dma_addr),
+			&ena_dev->reg_bar->aq_base_hi);
 
-	ena_dev->reg_bar->acq_base_lo =
-		ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->cq.dma_addr);
-	ena_dev->reg_bar->acq_base_hi =
-		ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->cq.dma_addr);
+	writel(ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->cq.dma_addr),
+			&ena_dev->reg_bar->acq_base_lo);
+	writel(ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->cq.dma_addr),
+			&ena_dev->reg_bar->acq_base_hi);
 
 	aq_caps = 0;
 	aq_caps |= admin_queue->q_depth & ENA_REGS_AQ_CAPS_AQ_DEPTH_MASK;
@@ -1245,7 +1489,7 @@ error:
 int ena_com_create_io_queue(struct ena_com_dev *ena_dev,
 			    u16 qid,
 			    enum queue_direction direction,
-			    enum ena_com_memory_queue_type mem_queue_type,
+			    enum ena_admin_placement_policy_type mem_queue_type,
 			    u32 msix_vector,
 			    u16 queue_size)
 {
@@ -1253,6 +1497,12 @@ int ena_com_create_io_queue(struct ena_com_dev *ena_dev,
 	struct ena_com_io_cq *io_cq = &ena_dev->io_cq_queues[qid];
 	int ret = 0;
 
+	if (qid >= ENA_TOTAL_NUM_QUEUES) {
+		ena_trc_err("Qid (%d) is bigger than max num of queues (%d)\n",
+			    qid, ENA_TOTAL_NUM_QUEUES);
+		return -EINVAL;
+	}
+
 	memset(io_sq, 0x0, sizeof(struct ena_com_io_sq));
 	memset(io_cq, 0x0, sizeof(struct ena_com_io_cq));
 
@@ -1282,9 +1532,12 @@ int ena_com_create_io_queue(struct ena_com_dev *ena_dev,
 
 	ret = ena_com_create_io_sq(ena_dev, io_sq, io_cq->idx);
 	if (ret)
-		goto error;
+		goto destroy_io_cq;
 
 	return 0;
+
+destroy_io_cq:
+	ena_com_destroy_io_cq(ena_dev, io_cq);
 error:
 	ena_com_io_queue_free(ena_dev, io_sq, io_cq);
 	return ret;
@@ -1292,8 +1545,17 @@ error:
 
 void ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid)
 {
-	struct ena_com_io_sq *io_sq = &ena_dev->io_sq_queues[qid];
-	struct ena_com_io_cq *io_cq = &ena_dev->io_cq_queues[qid];
+	struct ena_com_io_sq *io_sq;
+	struct ena_com_io_cq *io_cq;
+
+	if (qid >= ENA_TOTAL_NUM_QUEUES) {
+		ena_trc_err("Qid (%d) is bigger than max num of queues (%d)\n",
+			    qid, ENA_TOTAL_NUM_QUEUES);
+		return;
+	}
+
+	io_sq = &ena_dev->io_sq_queues[qid];
+	io_cq = &ena_dev->io_cq_queues[qid];
 
 	ena_com_destroy_io_sq(ena_dev, io_sq);
 	ena_com_destroy_io_cq(ena_dev, io_cq);
@@ -1304,7 +1566,7 @@ void ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid)
 int ena_com_get_link_params(struct ena_com_dev *ena_dev,
 			    struct ena_admin_get_feat_resp *resp)
 {
-	return ena_com_get_feature(ena_dev, resp, ena_admin_link_config);
+	return ena_com_get_feature(ena_dev, resp, ENA_ADMIN_LINK_CONFIG);
 }
 
 int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
@@ -1314,15 +1576,16 @@ int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
 	int rc;
 
 	rc = ena_com_get_feature(ena_dev, &get_resp,
-				 ena_admin_device_attributes);
+				 ENA_ADMIN_DEVICE_ATTRIBUTES);
 	if (rc)
 		return rc;
 
 	memcpy(&get_feat_ctx->dev_attr, &get_resp.u.dev_attr,
 	       sizeof(get_resp.u.dev_attr));
+	ena_dev->supported_features = get_resp.u.dev_attr.supported_features;
 
 	rc = ena_com_get_feature(ena_dev, &get_resp,
-				 ena_admin_max_queues_num);
+				 ENA_ADMIN_MAX_QUEUES_NUM);
 	if (rc)
 		return rc;
 
@@ -1330,7 +1593,7 @@ int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
 	       sizeof(get_resp.u.max_queue));
 
 	rc = ena_com_get_feature(ena_dev, &get_resp,
-				 ena_admin_aenq_config);
+				 ENA_ADMIN_AENQ_CONFIG);
 	if (rc)
 		return rc;
 
@@ -1338,7 +1601,7 @@ int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
 	       sizeof(get_resp.u.aenq));
 
 	rc = ena_com_get_feature(ena_dev, &get_resp,
-				 ena_admin_stateless_offload_config);
+				 ENA_ADMIN_STATELESS_OFFLOAD_CONFIG);
 	if (rc)
 		return rc;
 
@@ -1413,9 +1676,13 @@ void ena_com_aenq_intr_handler(struct ena_com_dev *dev, void *data)
 
 	aenq->head += processed;
 	aenq->phase = phase;
+
 	/* update ena-device for the last processed event */
-	if (processed)
+	if (processed) {
+		/* write the aenq doorbell after all AENQ descriptors were read */
+		mb();
 		writel((u32)aenq->head, &dev->reg_bar->aenq_head_db);
+	}
 }
 
 int ena_com_dev_reset(struct ena_com_dev *ena_dev)
@@ -1443,10 +1710,6 @@ int ena_com_dev_reset(struct ena_com_dev *ena_dev)
 		return -EINVAL;
 	}
 
-	/* If the register read failed */
-	if (unlikely((stat == 0xFFFFFFFF) || (cap == 0xFFFFFFFF)))
-		return -ETIME;
-
 	/* start reset */
 	writel(ENA_REGS_DEV_CTL_DEV_RESET_MASK, &ena_dev->reg_bar->dev_ctl);
 
@@ -1486,7 +1749,7 @@ static int ena_get_dev_stats(struct ena_com_dev *ena_dev,
 
 	admin_queue = &ena_dev->admin_queue;
 
-	get_cmd->aq_common_descriptor.opcode = ena_admin_get_stats;
+	get_cmd->aq_common_descriptor.opcode = ENA_ADMIN_GET_STATS;
 	get_cmd->aq_common_descriptor.flags = 0;
 	get_cmd->type = type;
 
@@ -1511,7 +1774,7 @@ int ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,
 
 	memset(&get_cmd, 0x0, sizeof(get_cmd));
 	ret = ena_get_dev_stats(ena_dev, &get_cmd, &get_resp,
-				ena_admin_get_stats_type_basic);
+				ENA_ADMIN_GET_STATS_TYPE_BASIC);
 	if (likely(ret == 0))
 		memcpy(stats, &get_resp.basic_stats,
 		       sizeof(get_resp.basic_stats));
@@ -1537,14 +1800,20 @@ int ena_com_get_dev_extended_stats(struct ena_com_dev *ena_dev, char *buff,
 		goto done;
 	}
 	memset(&get_cmd, 0x0, sizeof(get_cmd));
-	ena_com_mem_addr_set(&get_cmd.u.control_buffer.address, phys_addr);
+	ret = ena_com_mem_addr_set(ena_dev,
+				   &get_cmd.u.control_buffer.address,
+				   phys_addr);
+	if (unlikely(ret)) {
+		ena_trc_err("memory address set failed\n");
+		return ret;
+	}
 	get_cmd.u.control_buffer.length = len;
 
 	get_cmd.device_id = ena_dev->stats_func;
 	get_cmd.queue_idx = ena_dev->stats_queue;
 
 	ret = ena_get_dev_stats(ena_dev, &get_cmd, &get_resp,
-				ena_admin_get_stats_type_extended);
+				ENA_ADMIN_GET_STATS_TYPE_EXTENDED);
 	if (ret < 0)
 		goto free_ext_stats_mem;
 
@@ -1568,12 +1837,17 @@ int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu)
 		return -ENODEV;
 	}
 
+	if (!ena_com_check_supported_feature_id(ena_dev, ENA_ADMIN_MTU)) {
+		ena_trc_info("Feature %d isn't supported\n", ENA_ADMIN_MTU);
+		return -EPERM;
+	}
+
 	memset(&cmd, 0x0, sizeof(cmd));
 	admin_queue = &ena_dev->admin_queue;
 
-	cmd.aq_common_descriptor.opcode = ena_admin_set_feature;
+	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
 	cmd.aq_common_descriptor.flags = 0;
-	cmd.feat_common.feature_id = ena_admin_mtu;
+	cmd.feat_common.feature_id = ENA_ADMIN_MTU;
 	cmd.u.mtu.mtu = mtu;
 
 	ret = ena_com_execute_admin_command(admin_queue,
@@ -1605,17 +1879,24 @@ int ena_com_set_interrupt_moderation(struct ena_com_dev *ena_dev, int qid,
 		return -ENODEV;
 	}
 
+	if (!ena_com_check_supported_feature_id(ena_dev,
+						ENA_ADMIN_INTERRUPT_MODERATION)) {
+		ena_trc_err("Feature %d isn't supported\n",
+			    ENA_ADMIN_INTERRUPT_MODERATION);
+		return -EPERM;
+	}
+
 	memset(&cmd, 0x0, sizeof(cmd));
 	admin_queue = &ena_dev->admin_queue;
 
-	cmd.aq_common_descriptor.opcode = ena_admin_set_feature;
+	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
 	cmd.aq_common_descriptor.flags = 0;
-	cmd.feat_common.feature_id = ena_admin_interrupt_moderation;
+	cmd.feat_common.feature_id = ENA_ADMIN_INTERRUPT_MODERATION;
 	cmd.u.intr_moder.cq_idx = io_cq->idx;
 	if (io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)
-		direction = ena_admin_sq_direction_tx;
+		direction = ENA_ADMIN_SQ_DIRECTION_TX;
 	else
-		direction = ena_admin_sq_direction_rx;
+		direction = ENA_ADMIN_SQ_DIRECTION_RX;
 
 	cmd.u.intr_moder.queue_identity |= direction &
 		ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_SQ_DIRECTION_MASK;
@@ -1647,7 +1928,7 @@ int ena_com_get_offload_settings(struct ena_com_dev *ena_dev,
 	struct ena_admin_get_feat_resp resp;
 
 	ret = ena_com_get_feature(ena_dev, &resp,
-				  ena_admin_stateless_offload_config);
+				  ENA_ADMIN_STATELESS_OFFLOAD_CONFIG);
 	if (unlikely(ret)) {
 		ena_trc_err("Failed to get offload capabilities %d\n", ret);
 		return -EINVAL;
@@ -1657,3 +1938,446 @@ int ena_com_get_offload_settings(struct ena_com_dev *ena_dev,
 
 	return 0;
 }
+
+int ena_com_set_hash_function(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_set_feat_cmd cmd;
+	struct ena_admin_set_feat_resp resp;
+	struct ena_admin_get_feat_resp get_resp;
+	int ret;
+
+	if (!ena_com_check_supported_feature_id(ena_dev,
+						ENA_ADMIN_RSS_HASH_FUNCTION)) {
+		ena_trc_err("Feature %d isn't supported\n",
+			    ENA_ADMIN_RSS_HASH_FUNCTION);
+		return -EPERM;
+	}
+
+	/* Validate hash function is supported */
+	ret = ena_com_get_feature(ena_dev, &get_resp,
+				  ENA_ADMIN_RSS_HASH_FUNCTION);
+	if (unlikely(ret))
+		return ret;
+
+	if (get_resp.u.flow_hash_func.supported_func & (1 << rss->hash_func)) {
+		ena_trc_err("Func hash %d doesn't supported by device, abort\n",
+			    rss->hash_func);
+		return -EPERM;
+	}
+
+	memset(&cmd, 0x0, sizeof(cmd));
+
+	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
+	cmd.aq_common_descriptor.flags =
+		ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;
+	cmd.feat_common.feature_id = ENA_ADMIN_RSS_HASH_FUNCTION;
+	cmd.u.flow_hash_func.init_val = rss->hash_init_val;
+	cmd.u.flow_hash_func.selected_func = 1 << rss->hash_func;
+
+	ret = ena_com_mem_addr_set(ena_dev,
+				   &cmd.control_buffer.address,
+				   rss->hash_key_dma_addr);
+	if (unlikely(ret)) {
+		ena_trc_err("memory address set failed\n");
+		return ret;
+	}
+
+	cmd.control_buffer.length = sizeof(*rss->hash_key);
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&cmd,
+					    sizeof(cmd),
+					    (struct ena_admin_acq_entry *)&resp,
+					    sizeof(resp));
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to set hash function %d. error: %d\n",
+			    rss->hash_func, ret);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int ena_com_fill_hash_function(struct ena_com_dev *ena_dev,
+			       enum ena_admin_hash_functions func,
+			       const u8 *key, u16 key_len, u32 init_val)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_get_feat_resp get_resp;
+	struct ena_admin_feature_rss_flow_hash_control *hash_key =
+		rss->hash_key;
+	int rc;
+
+	/* Make sure size is a mult of DWs */
+	if (unlikely(key_len & 0x3))
+		return -EINVAL;
+
+	rc = ena_com_get_feature_ex(ena_dev, &get_resp,
+				    ENA_ADMIN_RSS_HASH_FUNCTION,
+				    rss->hash_key_dma_addr,
+				    sizeof(*rss->hash_key));
+	if (unlikely(rc))
+		return rc;
+
+	if (!((1 << func) & get_resp.u.flow_hash_func.supported_func)) {
+		ena_trc_err("Func doesn't supported\n");
+			return -EPERM;
+	}
+
+	switch (func) {
+	case ENA_ADMIN_TOEPLITZ:
+		if (key_len > sizeof(hash_key->key)) {
+			ena_trc_err("key len (%hu) is bigger than the max supported (%zu)\n",
+				    key_len, sizeof(hash_key->key));
+			return -EINVAL;
+		}
+
+		memcpy(hash_key->key, key, key_len);
+		rss->hash_init_val = init_val;
+		hash_key->keys_num = key_len >> 2;
+		break;
+	case ENA_ADMIN_CRC32:
+		rss->hash_init_val = init_val;
+		break;
+	default:
+		ena_trc_err("Invalid hash function (%d)\n", func);
+		return -EINVAL;
+	}
+
+	rc = ena_com_set_hash_function(ena_dev);
+
+	/* Restore the old function */
+	if (unlikely(rc))
+		ena_com_get_hash_function(ena_dev, NULL, NULL);
+
+	return rc;
+}
+
+int ena_com_get_hash_function(struct ena_com_dev *ena_dev,
+			      enum ena_admin_hash_functions *func,
+			      u8 *key)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_get_feat_resp get_resp;
+	struct ena_admin_feature_rss_flow_hash_control *hash_key =
+		rss->hash_key;
+	int rc;
+
+	rc = ena_com_get_feature_ex(ena_dev, &get_resp,
+				    ENA_ADMIN_RSS_HASH_FUNCTION,
+				    rss->hash_key_dma_addr,
+				    sizeof(*rss->hash_key));
+	if (unlikely(rc))
+		return rc;
+
+	rss->hash_func = get_resp.u.flow_hash_func.selected_func;
+	if (func)
+		*func = rss->hash_func;
+
+	if (key)
+		memcpy(key, hash_key->key, hash_key->keys_num << 2);
+
+	return 0;
+}
+
+int ena_com_get_hash_ctrl(struct ena_com_dev *ena_dev,
+			  enum ena_admin_flow_hash_proto proto,
+			  u16 *fields)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_get_feat_resp get_resp;
+	int rc;
+
+	rc = ena_com_get_feature_ex(ena_dev, &get_resp,
+				    ENA_ADMIN_RSS_HASH_INPUT,
+				    rss->hash_ctrl_dma_addr,
+				    sizeof(*rss->hash_ctrl));
+	if (unlikely(rc))
+		return rc;
+
+	if (fields)
+		*fields = rss->hash_ctrl->selected_fields[proto].fields;
+
+	return 0;
+}
+
+int ena_com_set_hash_ctrl(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_feature_rss_hash_control *hash_ctrl = rss->hash_ctrl;
+	struct ena_admin_set_feat_cmd cmd;
+	struct ena_admin_set_feat_resp resp;
+	int ret;
+
+	if (!ena_com_check_supported_feature_id(ena_dev,
+						ENA_ADMIN_RSS_HASH_INPUT)) {
+		ena_trc_err("Feature %d doesn't supported\n",
+			    ENA_ADMIN_RSS_HASH_INPUT);
+		return -EPERM;
+	}
+
+	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
+	cmd.aq_common_descriptor.flags =
+		ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;
+	cmd.feat_common.feature_id = ENA_ADMIN_RSS_HASH_INPUT;
+	cmd.u.flow_hash_input.enabled_input_sort =
+		ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_MASK |
+		ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_MASK;
+
+	ret = ena_com_mem_addr_set(ena_dev,
+				   &cmd.control_buffer.address,
+				   rss->hash_ctrl_dma_addr);
+	if (unlikely(ret)) {
+		ena_trc_err("memory address set failed\n");
+		return ret;
+	}
+	cmd.control_buffer.length = sizeof(*hash_ctrl);
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&cmd,
+					    sizeof(cmd),
+					    (struct ena_admin_acq_entry *)&resp,
+					    sizeof(resp));
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to set hash input. error: %d\n", ret);
+		ret = -EINVAL;
+	}
+
+	return 0;
+}
+
+int ena_com_set_default_hash_ctrl(struct ena_com_dev *ena_dev)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_feature_rss_hash_control *hash_ctrl =
+		rss->hash_ctrl;
+	u16 available_fields = 0;
+	int rc, i;
+
+	/* Get the supported hash input */
+	rc = ena_com_get_hash_ctrl(ena_dev, 0, NULL);
+	if (unlikely(rc))
+		return rc;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_TCP4].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |
+		ENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_UDP4].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |
+		ENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_TCP6].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |
+		ENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_UDP6].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |
+		ENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_IP6].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4_FRAG].fields =
+		ENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;
+
+	hash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4_FRAG].fields =
+		ENA_ADMIN_RSS_L2_DA | ENA_ADMIN_RSS_L2_SA;
+
+	for (i = 0; i < ENA_ADMIN_RSS_PROTO_NUM; i++) {
+		available_fields = hash_ctrl->selected_fields[i].fields &
+				hash_ctrl->supported_fields[i].fields;
+		if (available_fields != hash_ctrl->selected_fields[i].fields) {
+			ena_trc_err("hash control doesn't support all the desire configuration. proto %x supported %x selected %x\n",
+				    i, hash_ctrl->supported_fields[i].fields,
+				    hash_ctrl->selected_fields[i].fields);
+			return -EPERM;
+		}
+	}
+
+	rc = ena_com_set_hash_ctrl(ena_dev);
+
+	/* In case of failure, restore the old hash ctrl */
+	if (unlikely(rc))
+		ena_com_get_hash_ctrl(ena_dev, 0, NULL);
+
+	return rc;
+}
+
+int ena_com_fill_hash_ctrl(struct ena_com_dev *ena_dev,
+			   enum ena_admin_flow_hash_proto proto,
+			   u16 hash_fields)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_feature_rss_hash_control *hash_ctrl = rss->hash_ctrl;
+	u16 supported_fields;
+	int rc;
+
+	if (proto > ENA_ADMIN_RSS_PROTO_NUM) {
+		ena_trc_err("Invalid proto num (%u)\n", proto);
+		rc = -EINVAL;
+	}
+
+	/* Get the ctrl table */
+	rc = ena_com_get_hash_ctrl(ena_dev, proto, NULL);
+	if (unlikely(rc))
+		return rc;
+
+	/* Make sure all the fields are supported */
+	supported_fields = hash_ctrl->supported_fields[proto].fields;
+	if ((hash_fields & supported_fields) != hash_fields) {
+		ena_trc_err("proto %d doesn't support the required fields %x. supports only: %x\n",
+			    proto, hash_fields, supported_fields);
+	}
+
+	hash_ctrl->selected_fields[proto].fields = hash_fields;
+
+	rc = ena_com_set_hash_ctrl(ena_dev);
+
+	/* In case of failure, restore the old hash ctrl */
+	if (unlikely(rc))
+		ena_com_get_hash_ctrl(ena_dev, 0, NULL);
+
+	return 0;
+}
+
+int ena_com_indirect_table_fill_entry(struct ena_com_dev *ena_dev,
+				      u16 qid, u16 entry_idx)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_com_io_cq *io_cq;
+
+	if (unlikely(entry_idx >= (1 << rss->tbl_log_size)))
+		return -EINVAL;
+
+	if (unlikely((qid > ENA_MAX_NUM_IO_QUEUES)))
+		return -EINVAL;
+
+	io_cq = &ena_dev->io_cq_queues[qid];
+
+	rss->rss_ind_tbl[entry_idx].cq_idx = io_cq->idx;
+
+	return 0;
+}
+
+int ena_com_indirect_table_set(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_set_feat_cmd cmd;
+	struct ena_admin_set_feat_resp resp;
+	int ret = 0;
+
+	if (!ena_com_check_supported_feature_id(ena_dev,
+						ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG)) {
+		ena_trc_err("Feature %d isn't supported\n",
+			    ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG);
+		return -EPERM;
+	}
+
+	memset(&cmd, 0x0, sizeof(cmd));
+
+	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
+	cmd.aq_common_descriptor.flags =
+		ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;
+	cmd.feat_common.feature_id = ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG;
+	cmd.u.ind_table.size = rss->tbl_log_size;
+	cmd.u.ind_table.inline_index = 0xFFFFFFFF;
+
+	ret = ena_com_mem_addr_set(ena_dev,
+				   &cmd.control_buffer.address,
+				   rss->rss_ind_tbl_dma_addr);
+	if (unlikely(ret)) {
+		ena_trc_err("memory address set failed\n");
+		return ret;
+	}
+
+	cmd.control_buffer.length = (1 << rss->tbl_log_size) *
+		sizeof(struct ena_admin_rss_ind_table_entry);
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&cmd,
+					    sizeof(cmd),
+					    (struct ena_admin_acq_entry *)&resp,
+					    sizeof(resp));
+
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to set indirect table. error: %d\n", ret);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int ena_com_indirect_table_get(struct ena_com_dev *ena_dev, u32 *ind_tbl)
+{
+	struct ena_rss *rss = &ena_dev->rss;
+	struct ena_admin_get_feat_resp get_resp;
+	struct ena_admin_rss_ind_table_entry *rss_ind_tbl = rss->rss_ind_tbl;
+	u32 tbl_size;
+	int i, rc;
+
+	tbl_size = (1 << rss->tbl_log_size) *
+		sizeof(struct ena_admin_rss_ind_table_entry);
+
+	rc = ena_com_get_feature_ex(ena_dev, &get_resp,
+				    ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG,
+				    rss->rss_ind_tbl_dma_addr,
+				    tbl_size);
+	if (unlikely(rc))
+		return rc;
+
+	if (!ind_tbl)
+		return 0;
+
+	for (i = 0; i < (1 << rss->tbl_log_size); i++)
+		ind_tbl[i] = rss_ind_tbl[i].cq_idx;
+
+	return 0;
+}
+
+int ena_com_rss_init(struct ena_com_dev *ena_dev, u16 indr_tbl_log_size)
+{
+	int rc;
+
+	memset(&ena_dev->rss, 0x0, sizeof(ena_dev->rss));
+
+	rc = ena_com_indirect_table_allocate(ena_dev, indr_tbl_log_size);
+	if (unlikely(rc))
+		goto err_indr_tbl;
+
+	rc = ena_com_hash_key_allocate(ena_dev);
+	if (unlikely(rc))
+		goto err_hash_key;
+
+	rc = ena_com_hash_ctrl_init(ena_dev);
+	if (unlikely(rc))
+		goto err_hash_ctrl;
+
+	return 0;
+
+err_hash_ctrl:
+	ena_com_hash_key_destroy(ena_dev);
+err_hash_key:
+	ena_com_indirect_table_destroy(ena_dev);
+err_indr_tbl:
+
+	return rc;
+}
+
+int ena_com_rss_destroy(struct ena_com_dev *ena_dev)
+{
+	ena_com_indirect_table_destroy(ena_dev);
+	ena_com_hash_key_destroy(ena_dev);
+	ena_com_hash_ctrl_destroy(ena_dev);
+
+	memset(&ena_dev->rss, 0x0, sizeof(ena_dev->rss));
+
+	return 0;
+}
diff --git a/drivers/amazon/ena/ena_com.h b/drivers/amazon/ena/ena_com.h
index 07b1583..090712a 100644
--- a/drivers/amazon/ena/ena_com.h
+++ b/drivers/amazon/ena/ena_com.h
@@ -41,8 +41,6 @@
 #include <linux/sched.h>
 #include <linux/delay.h>
 
-#include "ena_includes.h"
-
 #define ena_trc_dbg(format, arg...) \
 	pr_debug("[ENA_COM: %s] " format, __func__, ##arg)
 #define ena_trc_info(format, arg...) \
@@ -61,6 +59,10 @@
 			WARN_ON(cond);					\
 		}							\
 	} while (0)
+#include "ena_common_defs.h"
+#include "ena_regs_defs.h"
+#include "ena_admin_defs.h"
+#include "ena_eth_io_defs.h"
 
 #define ENA_MAX_NUM_IO_QUEUES		128U
 /* We need to queues for each IO (on for Tx and one for Rx) */
@@ -79,12 +81,6 @@
 #define ADMIN_CQ_SIZE(depth)	((depth) * sizeof(struct ena_admin_acq_entry))
 #define ADMIN_AENQ_SIZE(depth)	((depth) * sizeof(struct ena_admin_aenq_entry))
 
-#define IO_TX_SQ_SIZE(depth)	((depth) * sizeof(struct ena_eth_io_tx_desc))
-#define IO_TX_CQ_SIZE(depth)	((depth) * sizeof(struct ena_eth_io_tx_cdesc))
-
-#define IO_RX_SQ_SIZE(depth) ((depth) * sizeof(struct ena_eth_io_rx_desc))
-#define IO_RX_CQ_SIZE(depth) ((depth) * sizeof(struct ena_eth_io_rx_cdesc_ext))
-
 /*****************************************************************************/
 /*****************************************************************************/
 
@@ -93,24 +89,16 @@ enum queue_direction {
 	ENA_COM_IO_QUEUE_DIRECTION_RX
 };
 
-enum ena_com_memory_queue_type {
-	/* descriptors and headers are located on the host OS memory
-	 */
-	ENA_MEM_QUEUE_TYPE_HOST_MEMORY = 0x1,
-	/* descriptors located on device memory
-	 * and headers located on host OS memory
-	 */
-	ENA_MEM_QUEUE_TYPE_MIXED_MEMORY = 0x2,
-	/* descriptors and headers are copied to the device memory */
-	ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY = 0x3,
-	ENA_MEM_QUEUE_TYPE_MAX_TYPES = ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY
-};
-
 struct ena_com_buf {
 	dma_addr_t paddr; /**< Buffer physical address */
 	u16 len; /**< Buffer length in bytes */
 };
 
+struct ena_com_rx_buf_info {
+	u16 len;
+	u16 req_id;
+};
+
 struct ena_com_io_desc_addr {
 	u8 __iomem *pbuf_dev_addr; /* LLQ address */
 	u8 __iomem *virt_addr;
@@ -148,8 +136,10 @@ struct ena_com_io_cq {
 	u16 cur_rx_pkt_cdesc_start_idx;
 
 	u16 q_depth;
+	/* Caller qid */
 	u16 qid;
 
+	/* Device queue index */
 	u16 idx;
 	u16 head;
 	u8 phase;
@@ -164,7 +154,7 @@ struct ena_com_io_sq {
 	u8 __iomem *header_addr;
 
 	enum queue_direction direction;
-	enum ena_com_memory_queue_type mem_queue_type;
+	enum ena_admin_placement_policy_type mem_queue_type;
 
 	u32 msix_vector;
 	struct ena_com_tx_meta cached_tx_meta;
@@ -177,6 +167,7 @@ struct ena_com_io_sq {
 	u16 next_to_comp;
 	u8 phase;
 	u8 desc_entry_size;
+	u8 dma_addr_bits;
 } ____cacheline_aligned;
 
 struct ena_com_admin_cq {
@@ -199,6 +190,14 @@ struct ena_com_admin_sq {
 
 };
 
+struct ena_com_stats_admin {
+	u32 aborted_cmd;
+	u32 submitted_cmd;
+	u32 completed_cmd;
+	u32 out_of_space;
+	u32 no_completion;
+};
+
 struct ena_com_admin_queue {
 	void *q_dmadev;
 	spinlock_t q_lock; /* spinlock for the admin queue */
@@ -219,6 +218,8 @@ struct ena_com_admin_queue {
 
 	/* Count the number of outstanding admin commands */
 	atomic_t outstanding_cmds;
+
+	struct ena_com_stats_admin stats;
 };
 
 struct ena_aenq_handlers;
@@ -233,13 +234,31 @@ struct ena_com_aenq {
 };
 
 struct ena_com_mmio_read {
-	struct ena_admin_ena_mmio_read_less_resp *read_resp;
+	struct ena_admin_ena_mmio_req_read_less_resp *read_resp;
 	dma_addr_t read_resp_dma_addr;
 	u16 seq_num;
 	/* spin lock to ensure a single outstanding read */
 	spinlock_t lock;
 };
 
+struct ena_rss {
+	/* Indirect table */
+	struct ena_admin_rss_ind_table_entry *rss_ind_tbl;
+	dma_addr_t rss_ind_tbl_dma_addr;
+	u16 tbl_log_size;
+
+	/* Hash key */
+	enum ena_admin_hash_functions hash_func;
+	struct ena_admin_feature_rss_flow_hash_control *hash_key;
+	dma_addr_t hash_key_dma_addr;
+	u32 hash_init_val;
+
+	/* Flow Control */
+	struct ena_admin_feature_rss_hash_control *hash_ctrl;
+	dma_addr_t hash_ctrl_dma_addr;
+
+};
+
 /* Each ena_dev is a PCI function. */
 struct ena_com_dev {
 	struct ena_com_admin_queue admin_queue;
@@ -250,12 +269,16 @@ struct ena_com_dev {
 	void __iomem *mem_bar;
 	void *dmadev;
 
-	enum ena_com_memory_queue_type tx_mem_queue_type;
+	enum ena_admin_placement_policy_type tx_mem_queue_type;
 
 	u16 stats_func; /* Selected function for extended statistic dump */
 	u16 stats_queue; /* Selected queue for extended statistic dump */
 
 	struct ena_com_mmio_read mmio_read;
+
+	struct ena_rss rss;
+	u32 supported_features;
+	u32 dma_addr_bits;
 };
 
 struct ena_com_dev_get_features_ctx {
@@ -265,100 +288,476 @@ struct ena_com_dev_get_features_ctx {
 	struct ena_admin_feature_offload_desc offload;
 };
 
-/*****************************************************************************/
-/*****************************************************************************/
+typedef void (*ena_aenq_handler)(void *data,
+	struct ena_admin_aenq_entry *aenq_e);
 
-int ena_com_get_link_params(struct ena_com_dev *ena_dev,
-			    struct ena_admin_get_feat_resp *resp);
+/* Holds aenq handlers. Indexed by AENQ event group */
+struct ena_aenq_handlers {
+	ena_aenq_handler handlers[ENA_MAX_HANDLERS];
+	ena_aenq_handler unimplemented_handler;
+};
 
-int ena_com_get_io_handlers(struct ena_com_dev *ena_dev, u16 qid,
-			    struct ena_com_io_sq **io_sq,
-			    struct ena_com_io_cq **io_cq);
+/*****************************************************************************/
+/*****************************************************************************/
 
+/* ena_com_mmio_reg_read_request_init - Init the mmio reg read mechanism
+ * @ena_dev: ENA communication layer struct
+ *
+ * Initialize the register read mechanism.
+ *
+ * @note: This method must be the first stage in the initialization sequence.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
 int ena_com_mmio_reg_read_request_init(struct ena_com_dev *ena_dev);
 
+/* ena_com_mmio_reg_read_request_write_dev_addr - Write the mmio reg read return
+ * value physical address.
+ * @ena_dev: ENA communication layer struct
+ */
+void ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev);
+
+/* ena_com_mmio_reg_read_request_destroy - Destroy the mmio reg read mechanism
+ * @ena_dev: ENA communication layer struct
+ */
 void ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev);
 
-void ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev);
+/* ena_com_admin_init - Init the admin and the async queues
+ * @ena_dev: ENA communication layer struct
+ * @aenq_handlers: Those handlers to be called upon event.
+ * @init_spinlock: Indicate if this method should init the admin spinlock or
+ * the spinlock was init before (for example, in a case of FLR).
+ *
+ * Initialize the admin submission and completion queues.
+ * Initialize the asynchronous events notification queues.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
+int ena_com_admin_init(struct ena_com_dev *ena_dev,
+		       struct ena_aenq_handlers *aenq_handlers,
+		       bool init_spinlock);
 
-int ena_com_get_dma_width(struct ena_com_dev *ena_dev);
+/* ena_com_admin_destroy - Destroy the admin and the async events queues.
+ * @ena_dev: ENA communication layer struct
+ *
+ * @note: Before calling this method, the caller must validate that the device
+ * won't send any additional admin completions/aenq.
+ * To achieve that, a FLR is recommended.
+ */
+void ena_com_admin_destroy(struct ena_com_dev *ena_dev);
 
-int ena_com_validate_version(struct ena_com_dev *ena_dev);
+/* ena_com_dev_reset - Perform device FLR to the device.
+ * @ena_dev: ENA communication layer struct
+ *
+ * @return - 0 on success, negative value on failure.
+ */
+int ena_com_dev_reset(struct ena_com_dev *ena_dev);
 
+/* ena_com_create_io_queue - Create io queue.
+ * @ena_dev: ENA communication layer struct
+ * @qid - the caller virtual queue id.
+ * @direction - the queue direction (Rx/Tx)
+ * @mem_queue_type - Indicate if this queue is LLQ or regular queue
+ * (relevant only for Tx queue)
+ * @msix_vector - MSI-X vector
+ * @queue_size - queue size
+ *
+ * Create the submission and the completion queues for queue id - qid.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
+int ena_com_create_io_queue(struct ena_com_dev *ena_dev, u16 qid,
+			    enum queue_direction direction,
+			    enum ena_admin_placement_policy_type mem_queue_type,
+			    u32 msix_vector,
+			    u16 queue_size);
+
+/* ena_com_admin_destroy - Destroy IO queue with the queue id - qid.
+ * @ena_dev: ENA communication layer struct
+ */
+void ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid);
+
+/* ena_com_get_io_handlers - Return the io queue handlers
+ * @ena_dev: ENA communication layer struct
+ * @qid - the caller virtual queue id.
+ * @io_sq - IO submission queue handler
+ * @io_cq - IO completion queue handler.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
+int ena_com_get_io_handlers(struct ena_com_dev *ena_dev, u16 qid,
+			    struct ena_com_io_sq **io_sq,
+			    struct ena_com_io_cq **io_cq);
+
+/* ena_com_admin_aenq_enable - ENAble asynchronous event notifications
+ * @ena_dev: ENA communication layer struct
+ *
+ * After this method, aenq event can be received via AENQ.
+ */
+void ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev);
+
+/* ena_com_set_admin_running_state - Set the state of the admin queue
+ * @ena_dev: ENA communication layer struct
+ *
+ * Change the state of the admin queue (enable/disable)
+ */
 void ena_com_set_admin_running_state(struct ena_com_dev *ena_dev, bool state);
 
+/* ena_com_get_admin_running_state - Get the admin queue state
+ * @ena_dev: ENA communication layer struct
+ *
+ * Retrieve the state of the admin queue (enable/disable)
+ *
+ * @return - current polling mode (enable/disable)
+ */
 bool ena_com_get_admin_running_state(struct ena_com_dev *ena_dev);
 
-int ena_com_set_interrupt_moderation(struct ena_com_dev *ena_dev, int qid,
-				     bool enable, u16 count, u16 interval);
-
+/* ena_com_set_admin_polling_mode - Set the admin completion queue polling mode
+ * @ena_dev: ENA communication layer struct
+ * @polling: ENAble/Disable polling mode
+ *
+ * Set the admin completion mode.
+ */
 void ena_com_set_admin_polling_mode(struct ena_com_dev *ena_dev, bool polling);
 
+/* ena_com_set_admin_polling_mode - Get the admin completion queue polling mode
+ * @ena_dev: ENA communication layer struct
+ *
+ * Get the admin completion mode.
+ * If polling mode is on, ena_com_execute_admin_command will perform a
+ * polling on the admin completion queue for the commands completion,
+ * otherwise it will wait on wait event.
+ *
+ * @return state
+ */
 bool ena_com_get_ena_admin_polling_mode(struct ena_com_dev *ena_dev);
 
-int ena_com_admin_init(struct ena_com_dev *ena_dev,
-		       struct ena_aenq_handlers *aenq_handlers,
-		       bool init_spinlock);
+/* ena_com_admin_q_comp_intr_handler - admin queue interrupt handler
+ * @ena_dev: ENA communication layer struct
+ *
+ * This method go over the admin completion queue and wake up all the pending
+ * threads that wait on the commands wait event.
+ *
+ * @note: Should be called after MSI-X interrupt.
+ */
+void ena_com_admin_q_comp_intr_handler(struct ena_com_dev *ena_dev);
 
-void ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev);
+/* ena_com_aenq_intr_handler - AENQ interrupt handler
+ * @ena_dev: ENA communication layer struct
+ *
+ * This method go over the async event notification queue and call the proper
+ * aenq handler.
+ */
+void ena_com_aenq_intr_handler(struct ena_com_dev *dev, void *data);
+
+/* ena_com_abort_admin_commands - Abort all the outstanding admin commands.
+ * @ena_dev: ENA communication layer struct
+ *
+ * This method aborts all the outstanding admin commands.
+ * The called should then call ena_com_wait_for_abort_completion to make sure
+ * all the commands were completed.
+ */
+void ena_com_abort_admin_commands(struct ena_com_dev *ena_dev);
 
+/* ena_com_wait_for_abort_completion - Wait for admin commands abort.
+ * @ena_dev: ENA communication layer struct
+ *
+ * This method wait until all the outstanding admin commands will be completed.
+ */
+void ena_com_wait_for_abort_completion(struct ena_com_dev *ena_dev);
+
+/* ena_com_validate_version - Validate the device parameters
+ * @ena_dev: ENA communication layer struct
+ *
+ * This method validate the device parameters are the same as the saved
+ * parameters in ena_dev.
+ * This method is useful after device reset, to validate the device mac address
+ * and the device offloads are the same as before the reset.
+ *
+ * @return - 0 on success negative value otherwise.
+ */
+int ena_com_validate_version(struct ena_com_dev *ena_dev);
+
+/* ena_com_get_link_params - Retrieve physical link parameters.
+ * @ena_dev: ENA communication layer struct
+ * @resp: Link parameters
+ *
+ * Retrieve the physical link parameters,
+ * like speed, auto-negotiation and full duplex support.
+ *
+ * @return - 0 on Success negative value otherwise.
+ */
+int ena_com_get_link_params(struct ena_com_dev *ena_dev,
+			    struct ena_admin_get_feat_resp *resp);
+
+/* ena_com_get_dma_width - Retrieve physical dma address width the device
+ * supports.
+ * @ena_dev: ENA communication layer struct
+ *
+ * Retrieve the maximum physical address bits the device can handle.
+ *
+ * @return: > 0 on Success and negative value otherwise.
+ */
+int ena_com_get_dma_width(struct ena_com_dev *ena_dev);
+
+/* ena_com_set_interrupt_moderation - Config interrupt moderation.
+ * @ena_dev: ENA communication layer struct
+ * @qid: the caller virtual queue id.
+ * @enable: enable/disable interrupt moderation
+ * @count:  maximum packets between interrupts
+ * @internal: maximum time between interrupts (value in us)
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_set_interrupt_moderation(struct ena_com_dev *ena_dev, int qid,
+				     bool enable, u16 count, u16 interval);
+
+/* ena_com_set_aenq_config - Set aenq groups configurations
+ * @ena_dev: ENA communication layer struct
+ * @groups flag: bit fields flags of enum ena_admin_aenq_group.
+ *
+ * Configure which aenq event group the driver would like to receive.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_set_aenq_config(struct ena_com_dev *ena_dev, u32 groups_flag);
+
+/* ena_com_get_dev_attr_feat - Get device features
+ * @ena_dev: ENA communication layer struct
+ * @get_feat_ctx: returned context that contain the get features.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
 int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
-			      struct ena_com_dev_get_features_ctx
-			      *get_feat_ctx);
+			      struct ena_com_dev_get_features_ctx *get_feat_ctx);
 
+/* ena_com_get_dev_basic_stats - Get device basic statistics
+ * @ena_dev: ENA communication layer struct
+ * @stats: stats return value
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
 int ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,
 				struct ena_admin_basic_stats *stats);
 
+/* ena_com_set_dev_mtu - Configure the device mtu.
+ * @ena_dev: ENA communication layer struct
+ * @mtu: mtu value
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
 int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu);
 
-int ena_com_create_io_queue(struct ena_com_dev *ena_dev, u16 qid,
-			    enum queue_direction direction,
-			    enum ena_com_memory_queue_type mem_queue_type,
-			    u32 msix_vector,
-			    u16 queue_size);
+/* ena_com_get_offload_settings - Retrieve the device offloads capabilities
+ * @ena_dev: ENA communication layer struct
+ * @offlad: offload return value
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_get_offload_settings(struct ena_com_dev *ena_dev,
+				 struct ena_admin_feature_offload_desc *offload);
 
-void ena_com_admin_destroy(struct ena_com_dev *ena_dev);
+/* ena_com_rss_init - Init RSS
+ * @ena_dev: ENA communication layer struct
+ * @log_size: indirection log size
+ *
+ * Allocate RSS/RFS resources.
+ * The caller then can configure rss using ena_com_set_hash_function,
+ * ena_com_set_hash_ctrl and ena_com_indirect_table_set.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_rss_init(struct ena_com_dev *ena_dev, u16 log_size);
 
-void ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid);
+/* ena_com_rss_destroy - Destroy rss
+ * @ena_dev: ENA communication layer struct
+ *
+ * Free all the RSS/RFS resources.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_rss_destroy(struct ena_com_dev *ena_dev);
+
+/* ena_com_fill_hash_function - Fill RSS hash function
+ * @ena_dev: ENA communication layer struct
+ * @func: The hash function (Toeplitz or crc)
+ * @key: Hash key (for toeplitz hash)
+ * @key_len: key length (max length 10 DW)
+ * @init_val: initial value for the hash function
+ *
+ * Fill the ena_dev resources with the desire hash function, hash key, key_len
+ * and key initial value (if needed by the hash function).
+ * To flush the key into the device the caller should call
+ * ena_com_set_hash_function.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_fill_hash_function(struct ena_com_dev *ena_dev,
+			       enum ena_admin_hash_functions func,
+			       const u8 *key, u16 key_len, u32 init_val);
 
-void ena_com_admin_q_comp_intr_handler(struct ena_com_dev *ena_dev);
+/* ena_com_set_hash_function - Flush the hash function and it dependencies to
+ * the device.
+ * @ena_dev: ENA communication layer struct
+ *
+ * Flush the hash function and it dependencies (key, key length and
+ * initial value) if needed.
+ *
+ * @note: Prior to this method the caller should call ena_com_fill_hash_function
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_set_hash_function(struct ena_com_dev *ena_dev);
 
-void ena_com_admin_queue_completion_int_handler(struct ena_com_dev *ena_dev);
+/* ena_com_get_hash_function - Retrieve the hash function and the hash key
+ * from the device.
+ * @ena_dev: ENA communication layer struct
+ * @func: hash function
+ * @key: hash key
+ *
+ * Retrieve the hash function and the hash key from the device.
+ *
+ * @note: If the caller called ena_com_fill_hash_function but didn't flash
+ * it to the device, the new configuration will be lost.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_get_hash_function(struct ena_com_dev *ena_dev,
+			      enum ena_admin_hash_functions *func,
+			      u8 *key);
+
+/* ena_com_fill_hash_ctrl - Fill RSS hash control
+ * @ena_dev: ENA communication layer struct.
+ * @proto: The protocol to configure.
+ * @hash_fields: bit mask of ena_admin_flow_hash_fields
+ *
+ * Fill the ena_dev resources with the desire hash control (the ethernet
+ * fields that take part of the hash) for a specific protocol.
+ * To flush the hash control to the device, the caller should call
+ * ena_com_set_hash_ctrl.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_fill_hash_ctrl(struct ena_com_dev *ena_dev,
+			   enum ena_admin_flow_hash_proto proto,
+			   u16 hash_fields);
+
+/* ena_com_set_hash_ctrl - Flush the hash control resources to the device.
+ * @ena_dev: ENA communication layer struct
+ *
+ * Flush the hash control (the ethernet fields that take part of the hash)
+ *
+ * @note: Prior to this method the caller should call ena_com_fill_hash_ctrl.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_set_hash_ctrl(struct ena_com_dev *ena_dev);
+
+/* ena_com_get_hash_ctrl - Retrieve the hash control from the device.
+ * @ena_dev: ENA communication layer struct
+ * @proto: The protocol to retrieve.
+ * @fields: bit mask of ena_admin_flow_hash_fields.
+ *
+ * Retrieve the hash control from the device.
+ *
+ * @note, If the caller called ena_com_fill_hash_ctrl but didn't flash
+ * it to the device, the new configuration will be lost.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_get_hash_ctrl(struct ena_com_dev *ena_dev,
+			  enum ena_admin_flow_hash_proto proto,
+			  u16 *fields);
+
+/* ena_com_set_default_hash_ctrl - Set the hash control to a default
+ * configuration.
+ * @ena_dev: ENA communication layer struct
+ *
+ * Fill the ena_dev resources with the default hash control configuration.
+ * To flush the hash control to the device, the caller should call
+ * ena_com_set_hash_ctrl.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_set_default_hash_ctrl(struct ena_com_dev *ena_dev);
+
+/* ena_com_indirect_table_fill_entry - Fill a single entry in the RSS
+ * indirection table
+ * @ena_dev: ENA communication layer struct.
+ * @qid - the caller virtual queue id.
+ * @entry_idx: RSS hash entry index.
+ *
+ * Fill a single entry of the RSS indirection table in the ena_dev resources.
+ * To flush the indirection table to the device, the called should call
+ * ena_com_indirect_table_set.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_indirect_table_fill_entry(struct ena_com_dev *ena_dev,
+				      u16 qid, u16 entry_idx);
+
+/* ena_com_indirect_table_set - Flush the indirection table to the device.
+ * @ena_dev: ENA communication layer struct
+ *
+ * Flush the indirection hash control to the device.
+ * Prior to this method the caller should call ena_com_indirect_table_fill_entry
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_indirect_table_set(struct ena_com_dev *ena_dev);
 
+/* ena_com_indirect_table_get - Retrieve the indirection table from the device.
+ * @ena_dev: ENA communication layer struct
+ * @ind_tbl: indirection table
+ *
+ * Retrieve the RSS indirection table from the device.
+ *
+ * @note: If the caller called ena_com_indirect_table_fill_entry but didn't flash
+ * it to the device, the new configuration will be lost.
+ *
+ * @return: 0 on Success and negative value otherwise.
+ */
+int ena_com_indirect_table_get(struct ena_com_dev *ena_dev, u32 *ind_tbl);
+
+/* ena_com_create_io_cq - Create io completion queue.
+ * @ena_dev: ENA communication layer struct
+ * @io_cq - io completion queue handler
+
+ * Create IO completion queue.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
 int ena_com_create_io_cq(struct ena_com_dev *ena_dev,
 			 struct ena_com_io_cq *io_cq);
 
-int ena_com_create_io_sq(struct ena_com_dev *ena_dev,
-			 struct ena_com_io_sq *io_sq, u16 cq_idx);
+/* ena_com_destroy_io_cq - Destroy io completion queue.
+ * @ena_dev: ENA communication layer struct
+ * @io_cq - io completion queue handler
 
+ * Destroy IO completion queue.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
 int ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,
 			  struct ena_com_io_cq *io_cq);
 
+/* ena_com_execute_admin_command - Execute admin command
+ * @admin_queue: admin queue.
+ * @cmd: the admin command to execute.
+ * @cmd_size: the command size.
+ * @cmd_completion: command completion return value.
+ * @cmd_comp_size: command completion size.
+
+ * Submit an admin command and then wait until the device will return a
+ * completion.
+ * The completion will be copyed into cmd_comp.
+ *
+ * @return - 0 on success, negative value on failure.
+ */
 int ena_com_execute_admin_command(struct ena_com_admin_queue *admin_queue,
 				  struct ena_admin_aq_entry *cmd,
 				  size_t cmd_size,
 				  struct ena_admin_acq_entry *cmd_comp,
 				  size_t cmd_comp_size);
 
-void ena_com_abort_admin_commands(struct ena_com_dev *ena_dev);
-
-void ena_com_wait_for_abort_completion(struct ena_com_dev *ena_dev);
-
-void ena_com_aenq_intr_handler(struct ena_com_dev *dev, void *data);
-
-typedef void (*ena_aenq_handler)(void *data,
-	struct ena_admin_aenq_entry *aenq_e);
-
-/* Holds all aenq handlers. Indexed by AENQ event group */
-struct ena_aenq_handlers {
-	ena_aenq_handler handlers[ENA_MAX_HANDLERS];
-	ena_aenq_handler unimplemented_handler;
-};
-
-int ena_com_dev_reset(struct ena_com_dev *ena_dev);
-
-int ena_com_get_offload_settings(struct ena_com_dev *ena_dev,
-				 struct ena_admin_feature_offload_desc
-				 *offload);
-
 #endif /* !(ENA_COM) */
diff --git a/drivers/amazon/ena/ena_common_defs.h b/drivers/amazon/ena/ena_common_defs.h
index 77bfc22..4a086f5 100644
--- a/drivers/amazon/ena/ena_common_defs.h
+++ b/drivers/amazon/ena/ena_common_defs.h
@@ -1,41 +1,41 @@
-/******************************************************************************
-Copyright (C) 2015 Annapurna Labs Ltd.
-
-This file may be licensed under the terms of the Annapurna Labs Commercial
-License Agreement.
-
-Alternatively, this file can be distributed under the terms of the GNU General
-Public License V2 as published by the Free Software Foundation and can be
-found at http://www.gnu.org/licenses/gpl-2.0.html
-
-Alternatively, redistribution and use in source and binary forms, with or
-without modification, are permitted provided that the following conditions are
-met:
-
-    *  Redistributions of source code must retain the above copyright notice,
-this list of conditions and the following disclaimer.
-
-    *  Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer in
-the documentation and/or other materials provided with the
-distribution.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
-ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
-(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
-LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
-ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-******************************************************************************/
-
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
 #ifndef _ENA_COMMON_H_
 #define _ENA_COMMON_H_
 
+/* spec version */
+#define ENA_COMMON_SPEC_VERSION_MAJOR	0 /* spec version major */
+#define ENA_COMMON_SPEC_VERSION_MINOR	10 /* spec version minor */
+
 /* ENA operates with 48-bit memory addresses. ena_mem_addr_t */
 struct ena_common_mem_addr {
 	/* word 0 : low 32 bit of the memory address */
diff --git a/drivers/amazon/ena/ena_eth_com.c b/drivers/amazon/ena/ena_eth_com.c
new file mode 100644
index 0000000..379cc70
--- /dev/null
+++ b/drivers/amazon/ena/ena_eth_com.c
@@ -0,0 +1,494 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "ena_eth_com.h"
+
+static inline struct ena_eth_io_rx_cdesc_base *ena_com_get_next_rx_cdesc(
+	struct ena_com_io_cq *io_cq)
+{
+	struct ena_eth_io_rx_cdesc_base *cdesc;
+	u16 expected_phase, head_masked;
+	u16 desc_phase;
+
+	head_masked = io_cq->head & (io_cq->q_depth - 1);
+	expected_phase = io_cq->phase;
+
+	cdesc = (struct ena_eth_io_rx_cdesc_base *)(io_cq->cdesc_addr.virt_addr
+			+ (head_masked * io_cq->cdesc_entry_size_in_bytes));
+
+	desc_phase = (cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_PHASE_MASK) >>
+			ENA_ETH_IO_RX_CDESC_BASE_PHASE_SHIFT;
+
+	if (desc_phase != expected_phase)
+		return NULL;
+
+	return cdesc;
+}
+
+static inline void ena_com_cq_inc_head(struct ena_com_io_cq *io_cq)
+{
+	io_cq->head++;
+
+	/* Switch phase bit in case of wrap around */
+	if (unlikely((io_cq->head & (io_cq->q_depth - 1)) == 0))
+		io_cq->phase = 1 - io_cq->phase;
+}
+
+static inline void *get_sq_desc(struct ena_com_io_sq *io_sq)
+{
+	u16 tail_masked;
+	u32 offset;
+
+	tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+
+	offset = tail_masked * io_sq->desc_entry_size;
+
+	return io_sq->desc_addr.virt_addr + offset;
+}
+
+static inline void ena_com_copy_curr_sq_desc_to_dev(struct ena_com_io_sq *io_sq)
+{
+	u16 tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+	u32 offset = tail_masked * io_sq->desc_entry_size;
+
+	/* In case this queue isn't a LLQ */
+	if (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST)
+		return;
+
+	memcpy(io_sq->desc_addr.pbuf_dev_addr + offset,
+	       io_sq->desc_addr.virt_addr + offset,
+	       io_sq->desc_entry_size);
+}
+
+static inline void ena_com_sq_update_tail(struct ena_com_io_sq *io_sq)
+{
+	io_sq->tail++;
+
+	/* Switch phase bit in case of wrap around */
+	if (unlikely((io_sq->tail & (io_sq->q_depth - 1)) == 0))
+		io_sq->phase = 1 - io_sq->phase;
+}
+
+static inline int ena_com_write_header(struct ena_com_io_sq *io_sq,
+				       u8 *head_src, u16 header_len)
+{
+	u16 tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+	u8 *dev_head_addr =
+		io_sq->header_addr + (tail_masked * ENA_MAX_PUSH_PKT_SIZE);
+
+	if (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST)
+		return 0;
+
+	ENA_ASSERT(io_sq->header_addr, "header address is NULL\n");
+
+	if (unlikely(header_len > ENA_MAX_PUSH_PKT_SIZE)) {
+		ena_trc_err("header size is too large\n");
+		return -EINVAL;
+	}
+
+	memcpy(dev_head_addr, head_src, header_len);
+
+	return 0;
+}
+
+static inline struct ena_eth_io_rx_cdesc_base *
+	ena_com_rx_cdesc_idx_to_ptr(struct ena_com_io_cq *io_cq, u16 idx)
+{
+	idx &= (io_cq->q_depth - 1);
+	return (struct ena_eth_io_rx_cdesc_base *)(io_cq->cdesc_addr.virt_addr +
+			idx * io_cq->cdesc_entry_size_in_bytes);
+}
+
+static inline int ena_com_cdesc_rx_pkt_get(struct ena_com_io_cq *io_cq,
+					   u16 *first_cdesc_idx,
+					   u16 *nb_hw_desc)
+{
+	struct ena_eth_io_rx_cdesc_base *cdesc;
+	u16 count = 0, head_masked;
+	u32 last = 0;
+
+	do {
+		cdesc = ena_com_get_next_rx_cdesc(io_cq);
+		if (!cdesc)
+			break;
+
+		ena_com_cq_inc_head(io_cq);
+		count++;
+		last = (cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_LAST_MASK) >>
+			ENA_ETH_IO_RX_CDESC_BASE_LAST_SHIFT;
+	} while (!last);
+
+	if (last) {
+		*first_cdesc_idx = io_cq->cur_rx_pkt_cdesc_start_idx;
+		count += io_cq->cur_rx_pkt_cdesc_count;
+
+		head_masked = io_cq->head & (io_cq->q_depth - 1);
+
+		io_cq->cur_rx_pkt_cdesc_count = 0;
+		io_cq->cur_rx_pkt_cdesc_start_idx = head_masked;
+
+		ena_trc_dbg("ena q_id: %d packets were completed. first desc idx %u descs# %d\n",
+			    io_cq->qid, *first_cdesc_idx, count);
+	} else {
+		io_cq->cur_rx_pkt_cdesc_count += count;
+		count = 0;
+	}
+
+	*nb_hw_desc = count;
+	return 0;
+}
+
+static inline bool ena_com_meta_desc_changed(struct ena_com_io_sq *io_sq,
+					     struct ena_com_tx_ctx *ena_tx_ctx)
+{
+	int rc;
+
+	if (ena_tx_ctx->meta_valid) {
+		rc = memcmp(&io_sq->cached_tx_meta,
+			    &ena_tx_ctx->ena_meta,
+			    sizeof(struct ena_com_tx_meta));
+
+		if (unlikely(rc != 0))
+			return true;
+	}
+
+	return false;
+}
+
+static inline void ena_com_create_and_store_tx_meta_desc(
+	struct ena_com_io_sq *io_sq,
+	struct ena_com_tx_ctx *ena_tx_ctx)
+{
+	struct ena_eth_io_tx_meta_desc *meta_desc = NULL;
+	struct ena_com_tx_meta *ena_meta = &ena_tx_ctx->ena_meta;
+
+	meta_desc = get_sq_desc(io_sq);
+	memset(meta_desc, 0x0, sizeof(struct ena_eth_io_tx_meta_desc));
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_DESC_MASK;
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_EXT_VALID_MASK;
+
+	/* bits 0-9 of the mss */
+	meta_desc->word2 |= (ena_meta->mss <<
+		ENA_ETH_IO_TX_META_DESC_MSS_LO_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_MSS_LO_MASK;
+	/* bits 10-13 of the mss */
+	meta_desc->len_ctrl |= ((ena_meta->mss >> 10) <<
+		ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_MASK;
+
+	/* Extended meta desc */
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_MASK;
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_STORE_MASK;
+	meta_desc->len_ctrl |= (io_sq->phase <<
+		ENA_ETH_IO_TX_META_DESC_PHASE_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_PHASE_MASK;
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_FIRST_MASK;
+	meta_desc->word2 |= ena_meta->l3_hdr_len &
+		ENA_ETH_IO_TX_META_DESC_L3_HDR_LEN_MASK;
+	meta_desc->word2 |= (ena_meta->l3_hdr_offset <<
+		ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_MASK;
+
+	meta_desc->word2 |= (ena_meta->l4_hdr_len <<
+		ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_MASK;
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_STORE_MASK;
+
+	/* Cached the meta desc */
+	memcpy(&io_sq->cached_tx_meta, ena_meta,
+	       sizeof(struct ena_com_tx_meta));
+
+	ena_com_copy_curr_sq_desc_to_dev(io_sq);
+	ena_com_sq_update_tail(io_sq);
+}
+
+static inline void ena_com_rx_set_flags(struct ena_com_rx_ctx *ena_rx_ctx,
+					struct ena_eth_io_rx_cdesc_base *cdesc)
+{
+	ena_rx_ctx->l3_proto = cdesc->status &
+		ENA_ETH_IO_RX_CDESC_BASE_L3_PROTO_IDX_MASK;
+	ena_rx_ctx->l4_proto =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_SHIFT;
+	ena_rx_ctx->l3_csum_err =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_SHIFT;
+	ena_rx_ctx->l4_csum_err =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_SHIFT;
+	ena_rx_ctx->hash_frag_csum =
+		(cdesc->word2 & ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_SHIFT;
+	ena_rx_ctx->frag =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_SHIFT;
+
+	ena_trc_dbg("ena_rx_ctx->l3_proto %d ena_rx_ctx->l4_proto %d\nena_rx_ctx->l3_csum_err %d ena_rx_ctx->l4_csum_err %d\nhash frag %d frag: %d cdesc_status: %x\n",
+		    ena_rx_ctx->l3_proto,
+		    ena_rx_ctx->l4_proto,
+		    ena_rx_ctx->l3_csum_err,
+		    ena_rx_ctx->l4_csum_err,
+		    ena_rx_ctx->hash_frag_csum,
+		    ena_rx_ctx->frag,
+		    cdesc->status);
+}
+
+/*****************************************************************************/
+/*****************************     API      **********************************/
+/*****************************************************************************/
+
+int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
+		       struct ena_com_tx_ctx *ena_tx_ctx,
+		       int *nb_hw_desc)
+{
+	struct ena_eth_io_tx_desc *desc = NULL;
+	struct ena_com_buf *ena_bufs = ena_tx_ctx->ena_bufs;
+	void *push_header = ena_tx_ctx->push_header;
+	u16 header_len = ena_tx_ctx->header_len;
+	u16 num_bufs = ena_tx_ctx->num_bufs;
+	int total_desc, i, rc;
+	bool have_meta;
+	u64 addr_hi;
+
+	ENA_ASSERT(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX,
+		   "wrong Q type");
+
+	/* num_bufs +1 for potential meta desc */
+	if (ena_com_sq_empty_space(io_sq) < (num_bufs + 1)) {
+		ena_trc_err("Not enough space in the tx queue\n");
+		return -ENOMEM;
+	}
+
+	have_meta = ena_tx_ctx->meta_valid && ena_com_meta_desc_changed(io_sq,
+			ena_tx_ctx);
+	if (have_meta)
+		ena_com_create_and_store_tx_meta_desc(io_sq, ena_tx_ctx);
+
+	/* start with pushing the header (if needed) */
+	rc = ena_com_write_header(io_sq, push_header, header_len);
+	if (unlikely(rc))
+		return rc;
+
+	desc = get_sq_desc(io_sq);
+	memset(desc, 0x0, sizeof(struct ena_eth_io_tx_desc));
+
+	/* Set first desc when we don't have meta descriptor */
+	if (!have_meta)
+		desc->len_ctrl |= ENA_ETH_IO_TX_DESC_FIRST_MASK;
+
+	desc->buff_addr_hi_hdr_sz |= (header_len <<
+		ENA_ETH_IO_TX_DESC_HEADER_LENGTH_SHIFT) &
+		ENA_ETH_IO_TX_DESC_HEADER_LENGTH_MASK;
+	desc->len_ctrl |= (io_sq->phase << ENA_ETH_IO_TX_DESC_PHASE_SHIFT) &
+		ENA_ETH_IO_TX_DESC_PHASE_MASK;
+
+	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_COMP_REQ_MASK;
+
+	/* Bits 0-9 */
+	desc->meta_ctrl |= (ena_tx_ctx->req_id <<
+		ENA_ETH_IO_TX_DESC_REQ_ID_LO_SHIFT) &
+		ENA_ETH_IO_TX_DESC_REQ_ID_LO_MASK;
+
+	/* Bits 10-15 */
+	desc->len_ctrl |= ((ena_tx_ctx->req_id >> 10) <<
+		ENA_ETH_IO_TX_DESC_REQ_ID_HI_SHIFT) &
+		ENA_ETH_IO_TX_DESC_REQ_ID_HI_MASK;
+
+	if (ena_tx_ctx->meta_valid) {
+		desc->meta_ctrl |= (ena_tx_ctx->tso_enable <<
+			ENA_ETH_IO_TX_DESC_TSO_EN_SHIFT) &
+			ENA_ETH_IO_TX_DESC_TSO_EN_MASK;
+		desc->meta_ctrl |= ena_tx_ctx->l3_proto &
+			ENA_ETH_IO_TX_DESC_L3_PROTO_IDX_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l4_proto <<
+			ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l3_csum_enable <<
+			ENA_ETH_IO_TX_DESC_L3_CSUM_EN_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L3_CSUM_EN_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l4_csum_enable <<
+			ENA_ETH_IO_TX_DESC_L4_CSUM_EN_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L4_CSUM_EN_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l4_csum_partial <<
+			ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_MASK;
+	}
+
+	for (i = 0; i < num_bufs; i++) {
+		/* The first desc share the same desc as the header */
+		if (likely(i != 0)) {
+			ena_com_copy_curr_sq_desc_to_dev(io_sq);
+			ena_com_sq_update_tail(io_sq);
+
+			desc = get_sq_desc(io_sq);
+			memset(desc, 0x0, sizeof(struct ena_eth_io_tx_desc));
+
+			desc->len_ctrl |= (io_sq->phase <<
+				ENA_ETH_IO_TX_DESC_PHASE_SHIFT) &
+				ENA_ETH_IO_TX_DESC_PHASE_MASK;
+		}
+
+		desc->len_ctrl |= ena_bufs->len &
+			ENA_ETH_IO_TX_DESC_LENGTH_MASK;
+
+		addr_hi = ((ena_bufs->paddr &
+			GENMASK_ULL(io_sq->dma_addr_bits - 1, 32)) >> 32);
+
+		desc->buff_addr_lo = (u32)ena_bufs->paddr;
+		desc->buff_addr_hi_hdr_sz |= addr_hi &
+			ENA_ETH_IO_TX_DESC_ADDR_HI_MASK;
+		ena_bufs++;
+	}
+
+	/* set the last desc indicator */
+	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_LAST_MASK;
+
+	ena_com_copy_curr_sq_desc_to_dev(io_sq);
+
+	ena_com_sq_update_tail(io_sq);
+
+	total_desc = max_t(u16, num_bufs, 1);
+	total_desc += have_meta ? 1 : 0;
+
+	*nb_hw_desc = total_desc;
+	return 0;
+}
+
+int ena_com_rx_pkt(struct ena_com_io_cq *io_cq,
+		   struct ena_com_io_sq *io_sq,
+		   struct ena_com_rx_ctx *ena_rx_ctx)
+{
+	struct ena_com_rx_buf_info *ena_buf = &ena_rx_ctx->ena_bufs[0];
+	struct ena_eth_io_rx_cdesc_base *cdesc = NULL;
+	u16 cdesc_idx = 0;
+	u16 nb_hw_desc;
+	u16 i;
+	int rc;
+
+	ENA_ASSERT(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_RX,
+		   "wrong Q type");
+
+	rc = ena_com_cdesc_rx_pkt_get(io_cq, &cdesc_idx, &nb_hw_desc);
+	if (rc || (nb_hw_desc == 0)) {
+		ena_rx_ctx->descs = nb_hw_desc;
+		return rc;
+	}
+
+	ena_trc_dbg("fetch rx packet: queue %d completed desc: %d\n",
+		    io_cq->qid, nb_hw_desc);
+
+	if (unlikely(ena_rx_ctx->descs >= ena_rx_ctx->max_bufs)) {
+		ena_trc_err("Too many RX cdescs (%d) > MAX(%d)\n",
+			    ena_rx_ctx->descs, nb_hw_desc);
+		return -ENOSPC;
+	}
+
+	for (i = 0; i < nb_hw_desc; i++) {
+		cdesc = ena_com_rx_cdesc_idx_to_ptr(io_cq, cdesc_idx + i);
+
+		ena_buf->len = cdesc->length;
+		ena_buf->req_id = cdesc->req_id;
+		ena_buf++;
+	}
+
+	/* Update SQ head ptr */
+	io_sq->next_to_comp += nb_hw_desc;
+
+	ena_trc_dbg("[%s][QID#%d] Updating SQ head to: %d\n", __func__,
+		    io_sq->qid, io_sq->next_to_comp);
+
+	/* Get rx flags from the last pkt */
+	ena_com_rx_set_flags(ena_rx_ctx, cdesc);
+
+	ena_rx_ctx->descs = nb_hw_desc;
+	return 0;
+}
+
+int ena_com_add_single_rx_desc(struct ena_com_io_sq *io_sq,
+			       struct ena_com_buf *ena_buf,
+			       u16 req_id)
+{
+	struct ena_eth_io_rx_desc *desc;
+
+	ENA_ASSERT(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_RX,
+		   "wrong Q type");
+
+	if (unlikely(ena_com_sq_empty_space(io_sq) == 0))
+		return -1;
+
+	desc = get_sq_desc(io_sq);
+	memset(desc, 0x0, sizeof(struct ena_eth_io_rx_desc));
+
+	desc->length = ena_buf->len;
+
+	desc->ctrl |= ENA_ETH_IO_RX_DESC_FIRST_MASK;
+	desc->ctrl |= ENA_ETH_IO_RX_DESC_LAST_MASK;
+	desc->ctrl |= io_sq->phase & ENA_ETH_IO_RX_DESC_PHASE_MASK;
+	desc->ctrl |= ENA_ETH_IO_RX_DESC_COMP_REQ_MASK;
+
+	desc->req_id = req_id;
+
+	desc->buff_addr_lo = (u32)ena_buf->paddr;
+	desc->buff_addr_hi =
+		((ena_buf->paddr & GENMASK_ULL(io_sq->dma_addr_bits - 1, 32)) >> 32);
+
+	ena_com_sq_update_tail(io_sq);
+
+	return 0;
+}
+
+int ena_com_tx_comp_req_id_get(struct ena_com_io_cq *io_cq, u16 *req_id)
+{
+	u8 expected_phase, cdesc_phase;
+	struct ena_eth_io_tx_cdesc *cdesc;
+	u16 masked_head;
+
+	masked_head = io_cq->head & (io_cq->q_depth - 1);
+	expected_phase = io_cq->phase;
+
+	cdesc = (struct ena_eth_io_tx_cdesc *)(io_cq->cdesc_addr.virt_addr
+		+ (masked_head * io_cq->cdesc_entry_size_in_bytes));
+
+	cdesc_phase = cdesc->flags & ENA_ETH_IO_TX_CDESC_PHASE_MASK;
+	if (cdesc_phase != expected_phase)
+		return -1;
+
+	ena_com_cq_inc_head(io_cq);
+
+	*req_id = cdesc->req_id;
+
+	return 0;
+}
diff --git a/drivers/amazon/ena/ena_eth_com.h b/drivers/amazon/ena/ena_eth_com.h
index bcfe122..a327b99 100644
--- a/drivers/amazon/ena/ena_eth_com.h
+++ b/drivers/amazon/ena/ena_eth_com.h
@@ -57,11 +57,10 @@ struct ena_com_tx_ctx {
 	bool l3_csum_enable;
 	bool l4_csum_enable;
 	bool l4_csum_partial;
-	bool tunnel_ctrl;
 };
 
 struct ena_com_rx_ctx {
-	struct ena_com_buf *ena_bufs;
+	struct ena_com_rx_buf_info *ena_bufs;
 	enum ena_eth_io_l3_proto_index l3_proto;
 	enum ena_eth_io_l4_proto_index l4_proto;
 	bool l3_csum_err;
@@ -69,262 +68,29 @@ struct ena_com_rx_ctx {
 	/* fragmented packet */
 	bool frag;
 	u16 hash_frag_csum;
+	u16 descs;
 	int max_bufs;
 };
 
-static inline struct ena_eth_io_rx_cdesc_base *ena_com_get_next_rx_cdesc(
-	struct ena_com_io_cq *io_cq)
-{
-	struct ena_eth_io_rx_cdesc_base *cdesc;
-	u16 expected_phase, head_masked;
-	u16 desc_phase;
-
-	head_masked = io_cq->head & (io_cq->q_depth - 1);
-	expected_phase = io_cq->phase;
-
-	cdesc = (struct ena_eth_io_rx_cdesc_base *)(io_cq->cdesc_addr.virt_addr
-			+ (head_masked * io_cq->cdesc_entry_size_in_bytes));
-
-	desc_phase = (cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_PHASE_MASK) >>
-			ENA_ETH_IO_RX_CDESC_BASE_PHASE_SHIFT;
-
-	if (desc_phase != expected_phase)
-		return NULL;
-
-	return cdesc;
-}
-
-static inline void ena_com_cq_inc_head(struct ena_com_io_cq *io_cq)
-{
-	io_cq->head++;
-
-	/* Switch phase bit in case of wrap around */
-	if (unlikely((io_cq->head & (io_cq->q_depth - 1)) == 0))
-		io_cq->phase = 1 - io_cq->phase;
-}
-
-static inline void *get_sq_desc(struct ena_com_io_sq *io_sq)
-{
-	u16 tail_masked;
-	u32 offset;
+int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
+		       struct ena_com_tx_ctx *ena_tx_ctx,
+		       int *nb_hw_desc);
 
-	tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+int ena_com_rx_pkt(struct ena_com_io_cq *io_cq,
+		   struct ena_com_io_sq *io_sq,
+		   struct ena_com_rx_ctx *ena_rx_ctx);
 
-	offset = tail_masked * io_sq->desc_entry_size;
-
-	return io_sq->desc_addr.virt_addr + offset;
-}
-
-static inline void ena_com_copy_curr_sq_desc_to_dev(struct ena_com_io_sq *io_sq)
-{
-	u16 tail_masked = io_sq->tail & (io_sq->q_depth - 1);
-	u32 offset = tail_masked * io_sq->desc_entry_size;
+int ena_com_add_single_rx_desc(struct ena_com_io_sq *io_sq,
+			       struct ena_com_buf *ena_buf,
+			       u16 req_id);
 
-	/* In case this queue isn't a LLQ */
-	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
-		return;
+int ena_com_tx_comp_req_id_get(struct ena_com_io_cq *io_cq, u16 *req_id);
 
-	memcpy(io_sq->desc_addr.pbuf_dev_addr + offset,
-	       io_sq->desc_addr.virt_addr + offset,
-	       io_sq->desc_entry_size);
-}
-
-static inline void ena_com_sq_update_tail(struct ena_com_io_sq *io_sq)
-{
-	io_sq->tail++;
-
-	/* Switch phase bit in case of wrap around */
-	if (unlikely((io_sq->tail & (io_sq->q_depth - 1)) == 0))
-		io_sq->phase = 1 - io_sq->phase;
-}
-
-static inline int ena_com_write_header(struct ena_com_io_sq *io_sq,
-				       u8 *head_src, u16 header_len)
-{
-	u16 tail_masked = io_sq->tail & (io_sq->q_depth - 1);
-	u8 *dev_head_addr =
-		io_sq->header_addr + (tail_masked * ENA_MAX_PUSH_PKT_SIZE);
-
-	if (io_sq->mem_queue_type != ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY)
-		return 0;
-
-	ENA_ASSERT(io_sq->header_addr, "header address is NULL\n");
-
-	if (unlikely(header_len > ENA_MAX_PUSH_PKT_SIZE)) {
-		ena_trc_err("header size is too large\n");
-		return -EINVAL;
-	}
-
-	memcpy(dev_head_addr, head_src, header_len);
-
-	return 0;
-}
-
-static inline struct ena_eth_io_rx_cdesc_base *
-	ena_com_rx_cdesc_idx_to_ptr(struct ena_com_io_cq *io_cq, u16 idx)
-{
-	idx &= (io_cq->q_depth - 1);
-	return (struct ena_eth_io_rx_cdesc_base *)(io_cq->cdesc_addr.virt_addr +
-			idx * io_cq->cdesc_entry_size_in_bytes);
-}
-
-static inline int ena_com_cdesc_rx_pkt_get(struct ena_com_io_cq *io_cq,
-					   u16 *first_cdesc_idx,
-					   u16 *nb_hw_desc)
-{
-	struct ena_eth_io_rx_cdesc_base *cdesc;
-	u16 count = 0, head_masked;
-	u32 last = 0;
-
-	do {
-		cdesc = ena_com_get_next_rx_cdesc(io_cq);
-		if (!cdesc)
-			break;
-
-		ena_com_cq_inc_head(io_cq);
-		count++;
-		last = (cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_LAST_MASK) >>
-			ENA_ETH_IO_RX_CDESC_BASE_LAST_SHIFT;
-	} while (!last);
-
-	if (last) {
-		*first_cdesc_idx = io_cq->cur_rx_pkt_cdesc_start_idx;
-		count += io_cq->cur_rx_pkt_cdesc_count;
-
-		head_masked = io_cq->head & (io_cq->q_depth - 1);
-
-		io_cq->cur_rx_pkt_cdesc_count = 0;
-		io_cq->cur_rx_pkt_cdesc_start_idx = head_masked;
-
-		ena_trc_dbg("ena q_id: %d packets were completed. first desc idx %u descs# %d\n",
-			    io_cq->qid, *first_cdesc_idx, count);
-	} else {
-		io_cq->cur_rx_pkt_cdesc_count += count;
-		count = 0;
-	}
-
-	*nb_hw_desc = count;
-	return 0;
-}
-
-static inline bool ena_com_meta_desc_changed(struct ena_com_io_sq *io_sq,
-					     struct ena_com_tx_ctx *ena_tx_ctx)
-{
-	int rc;
-
-	if (ena_tx_ctx->meta_valid) {
-		rc = memcmp(&io_sq->cached_tx_meta,
-			    &ena_tx_ctx->ena_meta,
-			    sizeof(struct ena_com_tx_meta));
-
-		if (unlikely(rc != 0))
-			return true;
-	}
-
-	return false;
-}
-
-static inline void ena_com_create_and_store_tx_meta_desc(
-	struct ena_com_io_sq *io_sq,
-	struct ena_com_tx_ctx *ena_tx_ctx)
-{
-	struct ena_eth_io_tx_meta_desc *meta_desc = NULL;
-	struct ena_com_tx_meta *ena_meta = &ena_tx_ctx->ena_meta;
-
-	meta_desc = get_sq_desc(io_sq);
-	memset(meta_desc, 0x0, sizeof(struct ena_eth_io_tx_meta_desc));
-
-	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_DESC_MASK;
-
-	if (ena_tx_ctx->tunnel_ctrl) {
-		/* Bits 0-2 */
-		meta_desc->word3 |= (ena_meta->l3_outer_hdr_offset <<
-			ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_SHIFT) &
-			ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_MASK;
-		/* Bits 3-4 */
-		meta_desc->len_ctrl |= ((ena_meta->l3_outer_hdr_offset >> 0x3)
-			<< ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_SHIFT) &
-			ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_MASK;
-
-		meta_desc->word3 |= (ena_meta->l3_outer_hdr_len <<
-			ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_SHIFT) &
-			ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_MASK;
-	}
-
-	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_EXT_VALID_MASK;
-
-	/* bits 0-9 of the mss */
-	meta_desc->word2 |= (ena_meta->mss <<
-		ENA_ETH_IO_TX_META_DESC_MSS_LO_SHIFT) &
-		ENA_ETH_IO_TX_META_DESC_MSS_LO_MASK;
-	/* bits 10-13 of the mss */
-	meta_desc->len_ctrl |= ((ena_meta->mss >> 10) <<
-		ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_SHIFT) &
-		ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_MASK;
-
-	/* Extended meta desc */
-	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_MASK;
-	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_STORE_MASK;
-	meta_desc->len_ctrl |= (io_sq->phase <<
-		ENA_ETH_IO_TX_META_DESC_PHASE_SHIFT) &
-		ENA_ETH_IO_TX_META_DESC_PHASE_MASK;
-
-	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_FIRST_MASK;
-	meta_desc->word2 |= ena_meta->l3_hdr_len &
-		ENA_ETH_IO_TX_META_DESC_L3_HDR_LEN_MASK;
-	meta_desc->word2 |= (ena_meta->l3_hdr_offset <<
-		ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_SHIFT) &
-		ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_MASK;
-
-	meta_desc->word2 |= (ena_meta->l4_hdr_len <<
-		ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_SHIFT) &
-		ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_MASK;
-
-	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_STORE_MASK;
-
-	/* Cached the meta desc */
-	memcpy(&io_sq->cached_tx_meta, ena_meta,
-	       sizeof(struct ena_com_tx_meta));
-
-	ena_com_copy_curr_sq_desc_to_dev(io_sq);
-	ena_com_sq_update_tail(io_sq);
-}
-
-static inline void ena_com_rx_set_flags(struct ena_com_rx_ctx *ena_rx_ctx,
-					struct ena_eth_io_rx_cdesc_base *cdesc)
+static inline void ena_com_unmask_intr(struct ena_com_io_cq *io_cq)
 {
-	ena_rx_ctx->l3_proto = cdesc->status &
-		ENA_ETH_IO_RX_CDESC_BASE_L3_PROTO_IDX_MASK;
-	ena_rx_ctx->l4_proto =
-		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_MASK) >>
-		ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_SHIFT;
-	ena_rx_ctx->l3_csum_err =
-		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_MASK) >>
-		ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_SHIFT;
-	ena_rx_ctx->l4_csum_err =
-		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_MASK) >>
-		ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_SHIFT;
-	ena_rx_ctx->hash_frag_csum =
-		(cdesc->word2 & ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_MASK) >>
-		ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_SHIFT;
-	ena_rx_ctx->frag =
-		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_MASK) >>
-		ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_SHIFT;
-
-	ena_trc_dbg("ena_rx_ctx->l3_proto %d ena_rx_ctx->l4_proto %d\nena_rx_ctx->l3_csum_err %d ena_rx_ctx->l4_csum_err %d\nhash frag %d frag: %d cdesc_status: %x\n",
-		    ena_rx_ctx->l3_proto,
-		    ena_rx_ctx->l4_proto,
-		    ena_rx_ctx->l3_csum_err,
-		    ena_rx_ctx->l4_csum_err,
-		    ena_rx_ctx->hash_frag_csum,
-		    ena_rx_ctx->frag,
-		    cdesc->status);
+	writel(io_cq->unmask_val, io_cq->unmask_reg);
 }
 
-/*****************************************************************************/
-/*****************************     API      **********************************/
-/*****************************************************************************/
-
 static inline int ena_com_sq_empty_space(struct ena_com_io_sq *io_sq)
 {
 	u16 tail, next_to_comp, cnt;
@@ -349,225 +115,6 @@ static inline int ena_com_write_sq_doorbell(struct ena_com_io_sq *io_sq)
 	return 0;
 }
 
-static inline int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
-				     struct ena_com_tx_ctx *ena_tx_ctx,
-				     int *nb_hw_desc)
-{
-	struct ena_eth_io_tx_desc *desc = NULL;
-	struct ena_com_buf *ena_bufs = ena_tx_ctx->ena_bufs;
-	void *push_header = ena_tx_ctx->push_header;
-	u16 header_len = ena_tx_ctx->header_len;
-	u16 num_bufs = ena_tx_ctx->num_bufs;
-	int total_desc, i, rc;
-	bool have_meta;
-
-	ENA_ASSERT(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX,
-		   "wrong Q type");
-
-	/* num_bufs +1 for potential meta desc */
-	if (ena_com_sq_empty_space(io_sq) < (num_bufs + 1))
-		return -ENOMEM;
-
-	have_meta = ena_tx_ctx->meta_valid && ena_com_meta_desc_changed(io_sq,
-			ena_tx_ctx);
-	if (have_meta)
-		ena_com_create_and_store_tx_meta_desc(io_sq, ena_tx_ctx);
-
-	/* start with pushing the header (if needed) */
-	rc = ena_com_write_header(io_sq, push_header, header_len);
-	if (unlikely(rc))
-		return rc;
-
-	desc = get_sq_desc(io_sq);
-	memset(desc, 0x0, sizeof(struct ena_eth_io_tx_desc));
-
-	/* Set first desc when we don't have meta descriptor */
-	if (!have_meta)
-		desc->len_ctrl |= ENA_ETH_IO_TX_DESC_FIRST_MASK;
-
-	desc->buff_addr_hi_hdr_sz |= (header_len <<
-		ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_SHIFT) &
-		ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_MASK;
-	desc->len_ctrl |= (io_sq->phase << ENA_ETH_IO_TX_DESC_PHASE_SHIFT) &
-		ENA_ETH_IO_TX_DESC_PHASE_MASK;
-
-	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_COMP_REQ_MASK;
-
-	/* Bits 0-9 */
-	desc->meta_ctrl |= (ena_tx_ctx->req_id <<
-		ENA_ETH_IO_TX_DESC_REQ_ID_LO_SHIFT) &
-		ENA_ETH_IO_TX_DESC_REQ_ID_LO_MASK;
-
-	/* Bits 10-15 */
-	desc->len_ctrl |= ((ena_tx_ctx->req_id >> 10) <<
-		ENA_ETH_IO_TX_DESC_REQ_ID_HI_SHIFT) &
-		ENA_ETH_IO_TX_DESC_REQ_ID_HI_MASK;
-
-	if (ena_tx_ctx->meta_valid) {
-		desc->meta_ctrl |= (ena_tx_ctx->tso_enable <<
-			ENA_ETH_IO_TX_DESC_TSO_EN_SHIFT) &
-			ENA_ETH_IO_TX_DESC_TSO_EN_MASK;
-		desc->meta_ctrl |= ena_tx_ctx->l3_proto &
-			ENA_ETH_IO_TX_DESC_L3_PROTO_IDX_MASK;
-		desc->meta_ctrl |= (ena_tx_ctx->l4_proto <<
-			ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_SHIFT) &
-			ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_MASK;
-		desc->meta_ctrl |= (ena_tx_ctx->l3_csum_enable <<
-			ENA_ETH_IO_TX_DESC_L3_CSUM_EN_SHIFT) &
-			ENA_ETH_IO_TX_DESC_L3_CSUM_EN_MASK;
-		desc->meta_ctrl |= (ena_tx_ctx->l4_csum_enable <<
-			ENA_ETH_IO_TX_DESC_L4_CSUM_EN_SHIFT) &
-			ENA_ETH_IO_TX_DESC_L4_CSUM_EN_MASK;
-		desc->meta_ctrl |= (ena_tx_ctx->l4_csum_partial <<
-			ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_SHIFT) &
-			ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_MASK;
-		desc->meta_ctrl |= (ena_tx_ctx->tunnel_ctrl <<
-			ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_SHIFT) &
-			ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_MASK;
-	}
-
-	for (i = 0; i < num_bufs; i++) {
-		/* The first desc share the same desc as the header */
-		if (likely(i != 0)) {
-			ena_com_copy_curr_sq_desc_to_dev(io_sq);
-			ena_com_sq_update_tail(io_sq);
-
-			desc = get_sq_desc(io_sq);
-			memset(desc, 0x0, sizeof(struct ena_eth_io_tx_desc));
-
-			desc->len_ctrl |= (io_sq->phase <<
-				ENA_ETH_IO_TX_DESC_PHASE_SHIFT) &
-				ENA_ETH_IO_TX_DESC_PHASE_MASK;
-		}
-
-		desc->len_ctrl |= ena_bufs->len &
-			ENA_ETH_IO_TX_DESC_LENGTH_MASK;
-
-		desc->buff_addr_hi_hdr_sz |= (ena_bufs->paddr >> 32) &
-			ENA_ETH_IO_TX_DESC_ADDR_HI_MASK;
-		desc->buff_addr_lo = ena_bufs->paddr & 0xFFFFFFFF;
-		ena_bufs++;
-	}
-
-	/* set the last desc indicator */
-	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_LAST_MASK;
-
-	ena_com_copy_curr_sq_desc_to_dev(io_sq);
-
-	ena_com_sq_update_tail(io_sq);
-
-	total_desc = max_t(u16, num_bufs, 1);
-	total_desc += have_meta ? 1 : 0;
-
-	*nb_hw_desc = total_desc;
-	return 0;
-}
-
-static inline int ena_com_rx_pkt(struct ena_com_io_cq *io_cq,
-				 struct ena_com_io_sq *io_sq,
-				 struct ena_com_rx_ctx *ena_rx_ctx)
-{
-	struct ena_com_buf *ena_buf = &ena_rx_ctx->ena_bufs[0];
-	struct ena_eth_io_rx_cdesc_base *cdesc = NULL;
-	u16 cdesc_idx = 0;
-	u16 nb_hw_desc;
-	u16 i;
-	int rc;
-
-	ENA_ASSERT(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_RX,
-		   "wrong Q type");
-
-	rc = ena_com_cdesc_rx_pkt_get(io_cq, &cdesc_idx, &nb_hw_desc);
-	if (rc || (nb_hw_desc == 0))
-		return 0;
-
-	ena_trc_dbg(
-		"fetch rx packet: queue %d completed desc: %d\n",
-		io_cq->qid, nb_hw_desc);
-
-	ENA_ASSERT(nb_hw_desc <= ena_rx_ctx->max_bufs,
-		   "Too many RX cdescs (%d) > MAX(%d)\n",
-		   nb_hw_desc, ena_rx_ctx->max_bufs);
-
-	for (i = 0; i < nb_hw_desc; i++) {
-		cdesc = ena_com_rx_cdesc_idx_to_ptr(io_cq, cdesc_idx + i);
-
-		ena_buf->len = cdesc->length;
-		ena_buf++;
-	}
-
-	/* Update SQ head ptr */
-	io_sq->next_to_comp += nb_hw_desc;
-
-	ena_trc_dbg("[%s][QID#%d] Updating SQ head to: %d\n", __func__,
-		    io_sq->qid, io_sq->next_to_comp);
-
-	/* Get rx flags from the last pkt */
-	ena_com_rx_set_flags(ena_rx_ctx, cdesc);
-
-	return nb_hw_desc;
-}
-
-static inline int ena_com_add_single_rx_desc(struct ena_com_io_sq *io_sq,
-					     struct ena_com_buf *ena_buf)
-{
-	struct ena_eth_io_rx_desc *desc;
-
-	ENA_ASSERT(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_RX,
-		   "wrong Q type");
-
-	if (unlikely(ena_com_sq_empty_space(io_sq) == 0))
-		return -1;
-
-	desc = get_sq_desc(io_sq);
-	memset(desc, 0x0, sizeof(struct ena_eth_io_rx_desc));
-
-	desc->length = ena_buf->len;
-
-	desc->ctrl |= ENA_ETH_IO_RX_DESC_FIRST_MASK;
-	desc->ctrl |= ENA_ETH_IO_RX_DESC_LAST_MASK;
-	desc->ctrl |= io_sq->phase & ENA_ETH_IO_RX_DESC_PHASE_MASK;
-	desc->ctrl |= ENA_ETH_IO_RX_DESC_COMP_REQ_MASK;
-
-	desc->req_id = io_sq->tail;
-
-	desc->buff_addr_lo = ena_buf->paddr & 0xFFFFFFFF;
-	desc->buff_addr_hi = (ena_buf->paddr >> 32) & 0xFFFF;
-
-	ena_com_sq_update_tail(io_sq);
-
-	return 0;
-}
-
-static inline void ena_com_unmask_intr(struct ena_com_io_cq *io_cq)
-{
-	writel(io_cq->unmask_val, io_cq->unmask_reg);
-}
-
-static inline int ena_com_tx_comp_req_id_get(struct ena_com_io_cq *io_cq,
-					     u16 *req_id)
-{
-	u8 expected_phase, cdesc_phase;
-	struct ena_eth_io_tx_cdesc *cdesc;
-	u16 masked_head;
-
-	masked_head = io_cq->head & (io_cq->q_depth - 1);
-	expected_phase = io_cq->phase;
-
-	cdesc = (struct ena_eth_io_tx_cdesc *)(io_cq->cdesc_addr.virt_addr
-		+ (masked_head * io_cq->cdesc_entry_size_in_bytes));
-
-	cdesc_phase = cdesc->flags & ENA_ETH_IO_TX_CDESC_PHASE_MASK;
-	if (cdesc_phase != expected_phase)
-		return -1;
-
-	ena_com_cq_inc_head(io_cq);
-
-	*req_id = cdesc->req_id;
-
-	return 0;
-}
-
 static inline void ena_com_comp_ack(struct ena_com_io_sq *io_sq, u16 elem)
 {
 	io_sq->next_to_comp += elem;
diff --git a/drivers/amazon/ena/ena_eth_io_defs.h b/drivers/amazon/ena/ena_eth_io_defs.h
index 823b1ec..5c6fcaf 100644
--- a/drivers/amazon/ena/ena_eth_io_defs.h
+++ b/drivers/amazon/ena/ena_eth_io_defs.h
@@ -1,70 +1,65 @@
-/******************************************************************************
-Copyright (C) 2015 Annapurna Labs Ltd.
-
-This file may be licensed under the terms of the Annapurna Labs Commercial
-License Agreement.
-
-Alternatively, this file can be distributed under the terms of the GNU General
-Public License V2 as published by the Free Software Foundation and can be
-found at http://www.gnu.org/licenses/gpl-2.0.html
-
-Alternatively, redistribution and use in source and binary forms, with or
-without modification, are permitted provided that the following conditions are
-met:
-
-    *  Redistributions of source code must retain the above copyright notice,
-this list of conditions and the following disclaimer.
-
-    *  Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer in
-the documentation and/or other materials provided with the
-distribution.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
-ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
-(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
-LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
-ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-******************************************************************************/
-
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
 #ifndef _ENA_ETH_IO_H_
 #define _ENA_ETH_IO_H_
 
 /* Layer 3 protocol index */
 enum ena_eth_io_l3_proto_index {
-	ena_eth_io_l3_proto_unknown = 0,
+	ENA_ETH_IO_L3_PROTO_UNKNOWN = 0,
 
-	ena_eth_io_l3_proto_ipv4 = 8,
+	ENA_ETH_IO_L3_PROTO_IPV4 = 8,
 
-	ena_eth_io_l3_proto_ipv6 = 11,
+	ENA_ETH_IO_L3_PROTO_IPV6 = 11,
 
-	ena_eth_io_l3_proto_fcoe = 21,
+	ENA_ETH_IO_L3_PROTO_FCOE = 21,
 
-	ena_eth_io_l3_proto_roce = 22,
+	ENA_ETH_IO_L3_PROTO_ROCE = 22,
 };
 
 /* Layer 4 protocol index */
 enum ena_eth_io_l4_proto_index {
-	ena_eth_io_l4_proto_unknown = 0,
+	ENA_ETH_IO_L4_PROTO_UNKNOWN = 0,
 
-	ena_eth_io_l4_proto_tcp = 12,
+	ENA_ETH_IO_L4_PROTO_TCP = 12,
 
-	ena_eth_io_l4_proto_udp = 13,
+	ENA_ETH_IO_L4_PROTO_UDP = 13,
 
-	ena_eth_io_l4_proto_routeable_roce = 23,
+	ENA_ETH_IO_L4_PROTO_ROUTEABLE_ROCE = 23,
 };
 
 /* ENA IO Queue Tx descriptor */
 struct ena_eth_io_tx_desc {
 	/* word 0 : */
-	/*
-	 * length, request id and control flags
+	/* length, request id and control flags
 	 * 15:0 : length - Buffer length in bytes, must
 	 *    include any packet trailers that the ENA supposed
 	 *    to update like End-to-End CRC, Authentication GMAC
@@ -89,8 +84,7 @@ struct ena_eth_io_tx_desc {
 	u32 len_ctrl;
 
 	/* word 1 : */
-	/*
-	 * ethernet control
+	/* ethernet control
 	 * 3:0 : l3_proto_idx - L3 protocol, if
 	 *    tunnel_ctrl[0] is set, then this is the inner
 	 *    packet L3. This field required when
@@ -137,17 +131,14 @@ struct ena_eth_io_tx_desc {
 	u32 buff_addr_lo;
 
 	/* word 3 : */
-	/*
-	 * address high and header size
+	/* address high and header size
 	 * 15:0 : addr_hi - Buffer Pointer[47:32]
 	 * 23:16 : reserved16_w2
-	 * 31:24 : push_buffer_length - Push Buffer length,
-	 *    number of bytes pushed to the ENA memory. used
-	 *    only for low latency queues. Maximum allowed value
-	 *    is negotiated through Admin Aueue, Minimum allowed
-	 *    value: for IPv4/6, the pushed buffer must include
-	 *    the Layer 2 and layer 3 headers, for ARP packets,
-	 *    in must include the Layer 2 and ARP header
+	 * 31:24 : header_length - Header length, number of
+	 *    bytes written to the ENA memory. used only for low
+	 *    latency queues. Maximum allowed value is
+	 *    negotiated through Admin Aueue, Minimum allowed
+	 *    value should include the packet headers
 	 */
 	u32 buff_addr_hi_hdr_sz;
 };
@@ -155,8 +146,7 @@ struct ena_eth_io_tx_desc {
 /* ENA IO Queue Tx Meta descriptor */
 struct ena_eth_io_tx_meta_desc {
 	/* word 0 : */
-	/*
-	 * length, request id and control flags
+	/* length, request id and control flags
 	 * 9:0 : req_id_lo - Request ID[9:0]
 	 * 11:10 : outr_l3_off_hi - valid if
 	 *    tunnel_ctrl[0]=1. bits[4:3] of outer packet L3
@@ -190,16 +180,14 @@ struct ena_eth_io_tx_meta_desc {
 	u32 len_ctrl;
 
 	/* word 1 : */
-	/*
-	 * word 1
+	/* word 1
 	 * 5:0 : req_id_hi
 	 * 31:6 : reserved6 - MBZ
 	 */
 	u32 word1;
 
 	/* word 2 : */
-	/*
-	 * word 2
+	/* word 2
 	 * 7:0 : l3_hdr_len - the header length L3 IP header.
 	 *    if tunnel_ctrl[0]=1, this is the IP header length
 	 *    of the inner packet.  FIXME - check if includes IP
@@ -218,8 +206,7 @@ struct ena_eth_io_tx_meta_desc {
 	u32 word2;
 
 	/* word 3 : */
-	/*
-	 * word 3
+	/* word 3
 	 * 23:0 : crypto_info
 	 * 28:24 : outr_l3_hdr_len_words - valid if
 	 *    tunnel_ctrl[0]=1.  Counts in words
@@ -241,8 +228,7 @@ struct ena_eth_io_tx_cdesc {
 
 	u8 status;
 
-	/*
-	 * flags
+	/* flags
 	 * 0 : phase
 	 * 7:1 : reserved1
 	 */
@@ -264,8 +250,7 @@ struct ena_eth_io_rx_desc {
 	/* MBZ */
 	u8 reserved2;
 
-	/*
-	 * control flags
+	/* control flags
 	 * 0 : phase
 	 * 1 : reserved1 - MBZ
 	 * 2 : first - Indicates first descriptor in
@@ -294,14 +279,12 @@ struct ena_eth_io_rx_desc {
 	u16 reserved16_w3;
 };
 
-/*
- * ENA IO Queue Rx Completion Base Descriptor (4-word format). Note: all
+/* ENA IO Queue Rx Completion Base Descriptor (4-word format). Note: all
  * ethernet parsing information are valid only when last=1
  */
 struct ena_eth_io_rx_cdesc_base {
 	/* word 0 : */
-	/*
-	 * 4:0 : l3_proto_idx - L3 protocol index
+	/* 4:0 : l3_proto_idx - L3 protocol index
 	 * 6:5 : src_vlan_cnt - Source VLAN count
 	 * 7 : tunnel - Tunnel exists
 	 * 12:8 : l4_proto_idx - L4 protocol index
@@ -340,8 +323,7 @@ struct ena_eth_io_rx_cdesc_base {
 	u16 req_id;
 
 	/* word 2 : */
-	/*
-	 * 8:0 : tunnel_off - inner packet offset
+	/* 8:0 : tunnel_off - inner packet offset
 	 * 15:9 : l3_off - Offset of first byte in the L3
 	 *    header from the beginning of the packet. if
 	 *    tunnel=1, this is of the inner packet
@@ -358,8 +340,7 @@ struct ena_eth_io_rx_cdesc_base {
 
 	u8 reserved;
 
-	/*
-	 * Offset of first byte in the L4 header from the beginning of the
+	/* Offset of first byte in the L4 header from the beginning of the
 	 *    packet. if tunnel=1, this is of the inner packet
 	 */
 	u8 l4_off;
@@ -387,126 +368,126 @@ struct ena_eth_io_rx_cdesc_ext {
 };
 
 /* tx_desc */
-#define ENA_ETH_IO_TX_DESC_LENGTH_MASK		GENMASK(16, 0)
-#define ENA_ETH_IO_TX_DESC_REQ_ID_HI_SHIFT		16
-#define ENA_ETH_IO_TX_DESC_REQ_ID_HI_MASK		GENMASK(22, 16)
-#define ENA_ETH_IO_TX_DESC_META_DESC_SHIFT		23
-#define ENA_ETH_IO_TX_DESC_META_DESC_MASK		BIT(23)
-#define ENA_ETH_IO_TX_DESC_PHASE_SHIFT		24
-#define ENA_ETH_IO_TX_DESC_PHASE_MASK		BIT(24)
-#define ENA_ETH_IO_TX_DESC_FIRST_SHIFT		26
-#define ENA_ETH_IO_TX_DESC_FIRST_MASK		BIT(26)
-#define ENA_ETH_IO_TX_DESC_LAST_SHIFT		27
-#define ENA_ETH_IO_TX_DESC_LAST_MASK		BIT(27)
-#define ENA_ETH_IO_TX_DESC_COMP_REQ_SHIFT		28
-#define ENA_ETH_IO_TX_DESC_COMP_REQ_MASK		BIT(28)
-#define ENA_ETH_IO_TX_DESC_L3_PROTO_IDX_MASK		GENMASK(4, 0)
-#define ENA_ETH_IO_TX_DESC_TSO_EN_SHIFT		7
-#define ENA_ETH_IO_TX_DESC_TSO_EN_MASK		BIT(7)
-#define ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_SHIFT		8
-#define ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_MASK		GENMASK(13, 8)
-#define ENA_ETH_IO_TX_DESC_L3_CSUM_EN_SHIFT		13
-#define ENA_ETH_IO_TX_DESC_L3_CSUM_EN_MASK		BIT(13)
-#define ENA_ETH_IO_TX_DESC_L4_CSUM_EN_SHIFT		14
-#define ENA_ETH_IO_TX_DESC_L4_CSUM_EN_MASK		BIT(14)
-#define ENA_ETH_IO_TX_DESC_ETHERNET_FCS_DIS_SHIFT		15
-#define ENA_ETH_IO_TX_DESC_ETHERNET_FCS_DIS_MASK		BIT(15)
-#define ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_SHIFT		17
-#define ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_MASK		BIT(17)
-#define ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_SHIFT		18
-#define ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_MASK		GENMASK(21, 18)
-#define ENA_ETH_IO_TX_DESC_TS_REQ_SHIFT		21
-#define ENA_ETH_IO_TX_DESC_TS_REQ_MASK		BIT(21)
-#define ENA_ETH_IO_TX_DESC_REQ_ID_LO_SHIFT		22
-#define ENA_ETH_IO_TX_DESC_REQ_ID_LO_MASK		GENMASK(32, 22)
-#define ENA_ETH_IO_TX_DESC_ADDR_HI_MASK		GENMASK(16, 0)
-#define ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_SHIFT		24
-#define ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_MASK		GENMASK(32, 24)
+#define ENA_ETH_IO_TX_DESC_LENGTH_MASK GENMASK(15, 0)
+#define ENA_ETH_IO_TX_DESC_REQ_ID_HI_SHIFT 16
+#define ENA_ETH_IO_TX_DESC_REQ_ID_HI_MASK GENMASK(21, 16)
+#define ENA_ETH_IO_TX_DESC_META_DESC_SHIFT 23
+#define ENA_ETH_IO_TX_DESC_META_DESC_MASK BIT(23)
+#define ENA_ETH_IO_TX_DESC_PHASE_SHIFT 24
+#define ENA_ETH_IO_TX_DESC_PHASE_MASK BIT(24)
+#define ENA_ETH_IO_TX_DESC_FIRST_SHIFT 26
+#define ENA_ETH_IO_TX_DESC_FIRST_MASK BIT(26)
+#define ENA_ETH_IO_TX_DESC_LAST_SHIFT 27
+#define ENA_ETH_IO_TX_DESC_LAST_MASK BIT(27)
+#define ENA_ETH_IO_TX_DESC_COMP_REQ_SHIFT 28
+#define ENA_ETH_IO_TX_DESC_COMP_REQ_MASK BIT(28)
+#define ENA_ETH_IO_TX_DESC_L3_PROTO_IDX_MASK GENMASK(3, 0)
+#define ENA_ETH_IO_TX_DESC_TSO_EN_SHIFT 7
+#define ENA_ETH_IO_TX_DESC_TSO_EN_MASK BIT(7)
+#define ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_SHIFT 8
+#define ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_MASK GENMASK(12, 8)
+#define ENA_ETH_IO_TX_DESC_L3_CSUM_EN_SHIFT 13
+#define ENA_ETH_IO_TX_DESC_L3_CSUM_EN_MASK BIT(13)
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_EN_SHIFT 14
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_EN_MASK BIT(14)
+#define ENA_ETH_IO_TX_DESC_ETHERNET_FCS_DIS_SHIFT 15
+#define ENA_ETH_IO_TX_DESC_ETHERNET_FCS_DIS_MASK BIT(15)
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_SHIFT 17
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_MASK BIT(17)
+#define ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_SHIFT 18
+#define ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_MASK GENMASK(20, 18)
+#define ENA_ETH_IO_TX_DESC_TS_REQ_SHIFT 21
+#define ENA_ETH_IO_TX_DESC_TS_REQ_MASK BIT(21)
+#define ENA_ETH_IO_TX_DESC_REQ_ID_LO_SHIFT 22
+#define ENA_ETH_IO_TX_DESC_REQ_ID_LO_MASK GENMASK(31, 22)
+#define ENA_ETH_IO_TX_DESC_ADDR_HI_MASK GENMASK(15, 0)
+#define ENA_ETH_IO_TX_DESC_HEADER_LENGTH_SHIFT 24
+#define ENA_ETH_IO_TX_DESC_HEADER_LENGTH_MASK GENMASK(31, 24)
 
 /* tx_meta_desc */
-#define ENA_ETH_IO_TX_META_DESC_REQ_ID_LO_MASK		GENMASK(10, 0)
-#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_SHIFT		10
-#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_MASK		GENMASK(12, 10)
-#define ENA_ETH_IO_TX_META_DESC_EXT_VALID_SHIFT		14
-#define ENA_ETH_IO_TX_META_DESC_EXT_VALID_MASK		BIT(14)
-#define ENA_ETH_IO_TX_META_DESC_WORD3_VALID_SHIFT		15
-#define ENA_ETH_IO_TX_META_DESC_WORD3_VALID_MASK		BIT(15)
-#define ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_SHIFT		16
-#define ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_MASK		GENMASK(20, 16)
-#define ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_SHIFT		20
-#define ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_MASK		BIT(20)
-#define ENA_ETH_IO_TX_META_DESC_META_STORE_SHIFT		21
-#define ENA_ETH_IO_TX_META_DESC_META_STORE_MASK		BIT(21)
-#define ENA_ETH_IO_TX_META_DESC_META_DESC_SHIFT		23
-#define ENA_ETH_IO_TX_META_DESC_META_DESC_MASK		BIT(23)
-#define ENA_ETH_IO_TX_META_DESC_PHASE_SHIFT		24
-#define ENA_ETH_IO_TX_META_DESC_PHASE_MASK		BIT(24)
-#define ENA_ETH_IO_TX_META_DESC_FIRST_SHIFT		26
-#define ENA_ETH_IO_TX_META_DESC_FIRST_MASK		BIT(26)
-#define ENA_ETH_IO_TX_META_DESC_LAST_SHIFT		27
-#define ENA_ETH_IO_TX_META_DESC_LAST_MASK		BIT(27)
-#define ENA_ETH_IO_TX_META_DESC_COMP_REQ_SHIFT		28
-#define ENA_ETH_IO_TX_META_DESC_COMP_REQ_MASK		BIT(28)
-#define ENA_ETH_IO_TX_META_DESC_REQ_ID_HI_MASK		GENMASK(6, 0)
-#define ENA_ETH_IO_TX_META_DESC_L3_HDR_LEN_MASK		GENMASK(8, 0)
-#define ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_SHIFT		8
-#define ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_MASK		GENMASK(16, 8)
-#define ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_SHIFT		16
-#define ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_MASK		GENMASK(22, 16)
-#define ENA_ETH_IO_TX_META_DESC_MSS_LO_SHIFT		22
-#define ENA_ETH_IO_TX_META_DESC_MSS_LO_MASK		GENMASK(32, 22)
-#define ENA_ETH_IO_TX_META_DESC_CRYPTO_INFO_MASK		GENMASK(24, 0)
-#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_SHIFT		24
-#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_MASK		GENMASK(29, 24)
-#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_SHIFT		29
-#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_MASK		GENMASK(32, 29)
+#define ENA_ETH_IO_TX_META_DESC_REQ_ID_LO_MASK GENMASK(9, 0)
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_SHIFT 10
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_MASK GENMASK(11, 10)
+#define ENA_ETH_IO_TX_META_DESC_EXT_VALID_SHIFT 14
+#define ENA_ETH_IO_TX_META_DESC_EXT_VALID_MASK BIT(14)
+#define ENA_ETH_IO_TX_META_DESC_WORD3_VALID_SHIFT 15
+#define ENA_ETH_IO_TX_META_DESC_WORD3_VALID_MASK BIT(15)
+#define ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_SHIFT 16
+#define ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_MASK GENMASK(19, 16)
+#define ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_SHIFT 20
+#define ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_MASK BIT(20)
+#define ENA_ETH_IO_TX_META_DESC_META_STORE_SHIFT 21
+#define ENA_ETH_IO_TX_META_DESC_META_STORE_MASK BIT(21)
+#define ENA_ETH_IO_TX_META_DESC_META_DESC_SHIFT 23
+#define ENA_ETH_IO_TX_META_DESC_META_DESC_MASK BIT(23)
+#define ENA_ETH_IO_TX_META_DESC_PHASE_SHIFT 24
+#define ENA_ETH_IO_TX_META_DESC_PHASE_MASK BIT(24)
+#define ENA_ETH_IO_TX_META_DESC_FIRST_SHIFT 26
+#define ENA_ETH_IO_TX_META_DESC_FIRST_MASK BIT(26)
+#define ENA_ETH_IO_TX_META_DESC_LAST_SHIFT 27
+#define ENA_ETH_IO_TX_META_DESC_LAST_MASK BIT(27)
+#define ENA_ETH_IO_TX_META_DESC_COMP_REQ_SHIFT 28
+#define ENA_ETH_IO_TX_META_DESC_COMP_REQ_MASK BIT(28)
+#define ENA_ETH_IO_TX_META_DESC_REQ_ID_HI_MASK GENMASK(5, 0)
+#define ENA_ETH_IO_TX_META_DESC_L3_HDR_LEN_MASK GENMASK(7, 0)
+#define ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_SHIFT 8
+#define ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_MASK GENMASK(15, 8)
+#define ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_SHIFT 16
+#define ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_MASK GENMASK(21, 16)
+#define ENA_ETH_IO_TX_META_DESC_MSS_LO_SHIFT 22
+#define ENA_ETH_IO_TX_META_DESC_MSS_LO_MASK GENMASK(31, 22)
+#define ENA_ETH_IO_TX_META_DESC_CRYPTO_INFO_MASK GENMASK(23, 0)
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_SHIFT 24
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_MASK GENMASK(28, 24)
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_SHIFT 29
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_MASK GENMASK(31, 29)
 
 /* tx_cdesc */
-#define ENA_ETH_IO_TX_CDESC_PHASE_MASK		BIT(0)
+#define ENA_ETH_IO_TX_CDESC_PHASE_MASK BIT(0)
 
 /* rx_desc */
-#define ENA_ETH_IO_RX_DESC_PHASE_MASK		BIT(0)
-#define ENA_ETH_IO_RX_DESC_FIRST_SHIFT		2
-#define ENA_ETH_IO_RX_DESC_FIRST_MASK		BIT(2)
-#define ENA_ETH_IO_RX_DESC_LAST_SHIFT		3
-#define ENA_ETH_IO_RX_DESC_LAST_MASK		BIT(3)
-#define ENA_ETH_IO_RX_DESC_COMP_REQ_SHIFT		4
-#define ENA_ETH_IO_RX_DESC_COMP_REQ_MASK		BIT(4)
+#define ENA_ETH_IO_RX_DESC_PHASE_MASK BIT(0)
+#define ENA_ETH_IO_RX_DESC_FIRST_SHIFT 2
+#define ENA_ETH_IO_RX_DESC_FIRST_MASK BIT(2)
+#define ENA_ETH_IO_RX_DESC_LAST_SHIFT 3
+#define ENA_ETH_IO_RX_DESC_LAST_MASK BIT(3)
+#define ENA_ETH_IO_RX_DESC_COMP_REQ_SHIFT 4
+#define ENA_ETH_IO_RX_DESC_COMP_REQ_MASK BIT(4)
 
 /* rx_cdesc_base */
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_PROTO_IDX_MASK		GENMASK(5, 0)
-#define ENA_ETH_IO_RX_CDESC_BASE_SRC_VLAN_CNT_SHIFT		5
-#define ENA_ETH_IO_RX_CDESC_BASE_SRC_VLAN_CNT_MASK		GENMASK(7, 5)
-#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_SHIFT		7
-#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_MASK		BIT(7)
-#define ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_SHIFT		8
-#define ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_MASK		GENMASK(13, 8)
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_SHIFT		13
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_MASK		BIT(13)
-#define ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_SHIFT		14
-#define ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_MASK		BIT(14)
-#define ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_SHIFT		15
-#define ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_MASK		BIT(15)
-#define ENA_ETH_IO_RX_CDESC_BASE_SECURED_PKT_SHIFT		20
-#define ENA_ETH_IO_RX_CDESC_BASE_SECURED_PKT_MASK		BIT(20)
-#define ENA_ETH_IO_RX_CDESC_BASE_CRYPTO_STATUS_SHIFT		21
-#define ENA_ETH_IO_RX_CDESC_BASE_CRYPTO_STATUS_MASK		GENMASK(23, 21)
-#define ENA_ETH_IO_RX_CDESC_BASE_PHASE_SHIFT		24
-#define ENA_ETH_IO_RX_CDESC_BASE_PHASE_MASK		BIT(24)
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM2_SHIFT		25
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM2_MASK		BIT(25)
-#define ENA_ETH_IO_RX_CDESC_BASE_FIRST_SHIFT		26
-#define ENA_ETH_IO_RX_CDESC_BASE_FIRST_MASK		BIT(26)
-#define ENA_ETH_IO_RX_CDESC_BASE_LAST_SHIFT		27
-#define ENA_ETH_IO_RX_CDESC_BASE_LAST_MASK		BIT(27)
-#define ENA_ETH_IO_RX_CDESC_BASE_INR_L4_CSUM_SHIFT		28
-#define ENA_ETH_IO_RX_CDESC_BASE_INR_L4_CSUM_MASK		BIT(28)
-#define ENA_ETH_IO_RX_CDESC_BASE_BUFFER_SHIFT		30
-#define ENA_ETH_IO_RX_CDESC_BASE_BUFFER_MASK		BIT(30)
-#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_OFF_MASK		GENMASK(9, 0)
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_OFF_SHIFT		9
-#define ENA_ETH_IO_RX_CDESC_BASE_L3_OFF_MASK		GENMASK(16, 9)
-#define ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_SHIFT		16
-#define ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_MASK		GENMASK(32, 16)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_PROTO_IDX_MASK GENMASK(4, 0)
+#define ENA_ETH_IO_RX_CDESC_BASE_SRC_VLAN_CNT_SHIFT 5
+#define ENA_ETH_IO_RX_CDESC_BASE_SRC_VLAN_CNT_MASK GENMASK(6, 5)
+#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_SHIFT 7
+#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_MASK BIT(7)
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_SHIFT 8
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_MASK GENMASK(12, 8)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_SHIFT 13
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_MASK BIT(13)
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_SHIFT 14
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_MASK BIT(14)
+#define ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_SHIFT 15
+#define ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_MASK BIT(15)
+#define ENA_ETH_IO_RX_CDESC_BASE_SECURED_PKT_SHIFT 20
+#define ENA_ETH_IO_RX_CDESC_BASE_SECURED_PKT_MASK BIT(20)
+#define ENA_ETH_IO_RX_CDESC_BASE_CRYPTO_STATUS_SHIFT 21
+#define ENA_ETH_IO_RX_CDESC_BASE_CRYPTO_STATUS_MASK GENMASK(22, 21)
+#define ENA_ETH_IO_RX_CDESC_BASE_PHASE_SHIFT 24
+#define ENA_ETH_IO_RX_CDESC_BASE_PHASE_MASK BIT(24)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM2_SHIFT 25
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM2_MASK BIT(25)
+#define ENA_ETH_IO_RX_CDESC_BASE_FIRST_SHIFT 26
+#define ENA_ETH_IO_RX_CDESC_BASE_FIRST_MASK BIT(26)
+#define ENA_ETH_IO_RX_CDESC_BASE_LAST_SHIFT 27
+#define ENA_ETH_IO_RX_CDESC_BASE_LAST_MASK BIT(27)
+#define ENA_ETH_IO_RX_CDESC_BASE_INR_L4_CSUM_SHIFT 28
+#define ENA_ETH_IO_RX_CDESC_BASE_INR_L4_CSUM_MASK BIT(28)
+#define ENA_ETH_IO_RX_CDESC_BASE_BUFFER_SHIFT 30
+#define ENA_ETH_IO_RX_CDESC_BASE_BUFFER_MASK BIT(30)
+#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_OFF_MASK GENMASK(8, 0)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_OFF_SHIFT 9
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_OFF_MASK GENMASK(15, 9)
+#define ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_SHIFT 16
+#define ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_MASK GENMASK(31, 16)
 
 #endif /*_ENA_ETH_IO_H_ */
diff --git a/drivers/amazon/ena/ena_ethtool.c b/drivers/amazon/ena/ena_ethtool.c
new file mode 100644
index 0000000..29e775b
--- /dev/null
+++ b/drivers/amazon/ena/ena_ethtool.c
@@ -0,0 +1,802 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "ena_netdev.h"
+#include <linux/pci.h>
+
+struct ena_stats {
+	char name[ETH_GSTRING_LEN];
+	int stat_offset;
+};
+
+#define ENA_STAT_ENA_COM_ENTRY(stat) { \
+	.name = #stat, \
+	.stat_offset = offsetof(struct ena_com_stats_admin, stat) \
+}
+
+#define ENA_STAT_ENTRY(stat, stat_type) { \
+	.name = #stat, \
+	.stat_offset = offsetof(struct ena_stats_##stat_type, stat) \
+}
+
+#define ENA_STAT_RX_ENTRY(stat) \
+	ENA_STAT_ENTRY(stat, rx)
+
+#define ENA_STAT_TX_ENTRY(stat) \
+	ENA_STAT_ENTRY(stat, tx)
+
+#define ENA_STAT_GLOBAL_ENTRY(stat) \
+	ENA_STAT_ENTRY(stat, dev)
+
+static const struct ena_stats ena_stats_global_strings[] = {
+	ENA_STAT_GLOBAL_ENTRY(tx_timeout),
+	ENA_STAT_GLOBAL_ENTRY(io_suspend),
+	ENA_STAT_GLOBAL_ENTRY(io_resume),
+	ENA_STAT_GLOBAL_ENTRY(wd_expired),
+	ENA_STAT_GLOBAL_ENTRY(interface_up),
+	ENA_STAT_GLOBAL_ENTRY(interface_down),
+	ENA_STAT_GLOBAL_ENTRY(admin_q_pause),
+};
+
+static const struct ena_stats ena_stats_tx_strings[] = {
+	ENA_STAT_TX_ENTRY(cnt),
+	ENA_STAT_TX_ENTRY(bytes),
+	ENA_STAT_TX_ENTRY(queue_stop),
+	ENA_STAT_TX_ENTRY(queue_wakeup),
+	ENA_STAT_TX_ENTRY(dma_mapping_err),
+	ENA_STAT_TX_ENTRY(unsupported_desc_num),
+	ENA_STAT_TX_ENTRY(napi_comp),
+	ENA_STAT_TX_ENTRY(tx_poll),
+	ENA_STAT_TX_ENTRY(doorbells),
+	ENA_STAT_TX_ENTRY(prepare_ctx_err),
+};
+
+static const struct ena_stats ena_stats_rx_strings[] = {
+	ENA_STAT_RX_ENTRY(cnt),
+	ENA_STAT_RX_ENTRY(bytes),
+	ENA_STAT_RX_ENTRY(refil_partial),
+	ENA_STAT_RX_ENTRY(bad_csum),
+	ENA_STAT_RX_ENTRY(page_alloc_fail),
+	ENA_STAT_RX_ENTRY(skb_alloc_fail),
+	ENA_STAT_RX_ENTRY(dma_mapping_err),
+	ENA_STAT_RX_ENTRY(bad_desc_num),
+	ENA_STAT_RX_ENTRY(small_copy_len_pkt),
+};
+
+static const struct ena_stats ena_stats_ena_com_strings[] = {
+	ENA_STAT_ENA_COM_ENTRY(aborted_cmd),
+	ENA_STAT_ENA_COM_ENTRY(submitted_cmd),
+	ENA_STAT_ENA_COM_ENTRY(completed_cmd),
+	ENA_STAT_ENA_COM_ENTRY(out_of_space),
+	ENA_STAT_ENA_COM_ENTRY(no_completion),
+};
+
+#define ENA_STATS_ARRAY_GLOBAL	ARRAY_SIZE(ena_stats_global_strings)
+#define ENA_STATS_ARRAY_TX	ARRAY_SIZE(ena_stats_tx_strings)
+#define ENA_STATS_ARRAY_RX	ARRAY_SIZE(ena_stats_rx_strings)
+#define ENA_STATS_ARRAY_ENA_COM	ARRAY_SIZE(ena_stats_ena_com_strings)
+
+static void ena_safe_update_stat(u64 *src, u64 *dst,
+				 struct u64_stats_sync *syncp)
+{
+	unsigned int start;
+
+	do {
+		start = u64_stats_fetch_begin_irq(syncp);
+		*(dst) = *src;
+	} while (u64_stats_fetch_retry_irq(syncp, start));
+}
+
+static void ena_queue_stats(struct ena_adapter *adapter, u64 **data)
+{
+	const struct ena_stats *ena_stats;
+	struct ena_ring *ring;
+
+	u64 *ptr;
+	int i, j;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		/* Tx stats */
+		ring = &adapter->tx_ring[i];
+
+		for (j = 0; j < ENA_STATS_ARRAY_TX; j++) {
+			ena_stats = &ena_stats_tx_strings[j];
+
+			ptr = (u64 *)((uintptr_t)&ring->tx_stats +
+				(uintptr_t)ena_stats->stat_offset);
+
+			ena_safe_update_stat(ptr, (*data)++, &ring->syncp);
+		}
+
+		/* Rx stats */
+		ring = &adapter->rx_ring[i];
+
+		for (j = 0; j < ENA_STATS_ARRAY_RX; j++) {
+			ena_stats = &ena_stats_rx_strings[j];
+
+			ptr = (u64 *)((uintptr_t)&ring->rx_stats +
+				(uintptr_t)ena_stats->stat_offset);
+
+			ena_safe_update_stat(ptr, (*data)++, &ring->syncp);
+		}
+	}
+}
+
+static void ena_dev_admin_queue_stats(struct ena_adapter *adapter, u64 **data)
+{
+	const struct ena_stats *ena_stats;
+	u32 *ptr;
+	int i;
+
+	for (i = 0; i < ENA_STATS_ARRAY_ENA_COM; i++) {
+		ena_stats = &ena_stats_ena_com_strings[i];
+
+		ptr = (u32 *)((uintptr_t)&adapter->ena_dev->admin_queue.stats +
+			(uintptr_t)ena_stats->stat_offset);
+
+		*(*data)++ = *ptr;
+	}
+}
+
+static void ena_get_ethtool_stats(struct net_device *netdev,
+				  struct ethtool_stats *stats,
+				  u64 *data)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	const struct ena_stats *ena_stats;
+	u64 *ptr;
+	int i;
+
+	for (i = 0; i < ENA_STATS_ARRAY_GLOBAL; i++) {
+		ena_stats = &ena_stats_global_strings[i];
+
+		ptr = (u64 *)((uintptr_t)&adapter->dev_stats +
+			(uintptr_t)ena_stats->stat_offset);
+
+		ena_safe_update_stat(ptr, data++, &adapter->syncp);
+	}
+
+	ena_queue_stats(adapter, &data);
+	ena_dev_admin_queue_stats(adapter, &data);
+}
+
+static int ena_get_sset_count(struct net_device *netdev, int sset)
+{
+	if (sset != ETH_SS_STATS)
+		return -EOPNOTSUPP;
+
+	return  netdev->num_tx_queues *
+		(ENA_STATS_ARRAY_TX + ENA_STATS_ARRAY_RX) +
+		ENA_STATS_ARRAY_GLOBAL + ENA_STATS_ARRAY_ENA_COM;
+}
+
+static void ena_queue_strings(struct ena_adapter *adapter, u8 **data)
+{
+	const struct ena_stats *ena_stats;
+	int i, j;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		/* Tx stats */
+		for (j = 0; j < ENA_STATS_ARRAY_TX; j++) {
+			ena_stats = &ena_stats_tx_strings[j];
+
+			snprintf(*data, ETH_GSTRING_LEN,
+				 "queue_%u_tx_%s", i, ena_stats->name);
+			 (*data) += ETH_GSTRING_LEN;
+		}
+		/* Rx stats */
+		for (j = 0; j < ENA_STATS_ARRAY_RX; j++) {
+			ena_stats = &ena_stats_rx_strings[j];
+
+			snprintf(*data, ETH_GSTRING_LEN,
+				 "queue_%u_rx_%s", i, ena_stats->name);
+			(*data) += ETH_GSTRING_LEN;
+		}
+	}
+}
+
+static void ena_com_dev_strings(u8 **data)
+{
+	const struct ena_stats *ena_stats;
+	int i;
+
+	for (i = 0; i < ENA_STATS_ARRAY_ENA_COM; i++) {
+		ena_stats = &ena_stats_ena_com_strings[i];
+
+		snprintf(*data, ETH_GSTRING_LEN,
+			 "ena_admin_q_%s", ena_stats->name);
+		(*data) += ETH_GSTRING_LEN;
+	}
+}
+
+static void ena_get_strings(struct net_device *netdev, u32 sset, u8 *data)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	const struct ena_stats *ena_stats;
+	int i;
+
+	if (sset != ETH_SS_STATS)
+		return;
+
+	for (i = 0; i < ENA_STATS_ARRAY_GLOBAL; i++) {
+		ena_stats = &ena_stats_global_strings[i];
+
+		memcpy(data, ena_stats->name, ETH_GSTRING_LEN);
+		data += ETH_GSTRING_LEN;
+	}
+
+	ena_queue_strings(adapter, &data);
+	ena_com_dev_strings(&data);
+}
+
+static int ena_get_settings(struct net_device *netdev,
+			    struct ethtool_cmd *ecmd)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	struct ena_admin_get_feature_link_desc *link;
+	struct ena_admin_get_feat_resp feat_resp;
+	int rc;
+
+	rc = ena_com_get_link_params(ena_dev, &feat_resp);
+	if (rc)
+		return rc;
+
+	link = &feat_resp.u.link;
+
+	ethtool_cmd_speed_set(ecmd, link->speed);
+
+	if (link->flags & ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_MASK)
+		ecmd->duplex = DUPLEX_FULL;
+	else
+		ecmd->duplex = DUPLEX_HALF;
+
+	if (link->flags & ENA_ADMIN_GET_FEATURE_LINK_DESC_AUTONEG_MASK)
+		ecmd->autoneg = AUTONEG_ENABLE;
+	else
+		ecmd->autoneg = AUTONEG_DISABLE;
+
+	return 0;
+}
+
+static int ena_get_coalesce(struct net_device *net_dev,
+			    struct ethtool_coalesce *coalesce)
+{
+	struct ena_adapter *adapter = netdev_priv(net_dev);
+
+	coalesce->tx_coalesce_usecs = adapter->tx_usecs;
+	coalesce->rx_coalesce_usecs = adapter->rx_usecs;
+	coalesce->rx_max_coalesced_frames = adapter->rx_frames;
+	coalesce->tx_max_coalesced_frames = adapter->tx_frames;
+	coalesce->use_adaptive_rx_coalesce = false;
+
+	return 0;
+}
+
+static int ena_ethtool_set_coalesce(struct net_device *net_dev,
+				    struct ethtool_coalesce *coalesce)
+{
+	struct ena_adapter *adapter = netdev_priv(net_dev);
+	bool tx_enable = true;
+	bool rx_enable = true;
+	int i, rc;
+
+	/* The above params are unsupported by our driver */
+	if (coalesce->rx_coalesce_usecs_irq ||
+	    coalesce->rx_max_coalesced_frames_irq ||
+	    coalesce->tx_coalesce_usecs_irq ||
+	    coalesce->tx_max_coalesced_frames_irq ||
+	    coalesce->stats_block_coalesce_usecs ||
+	    coalesce->use_adaptive_rx_coalesce ||
+	    coalesce->use_adaptive_tx_coalesce ||
+	    coalesce->pkt_rate_low ||
+	    coalesce->rx_coalesce_usecs_low ||
+	    coalesce->rx_max_coalesced_frames_low ||
+	    coalesce->tx_coalesce_usecs_low ||
+	    coalesce->tx_max_coalesced_frames_low ||
+	    coalesce->pkt_rate_high ||
+	    coalesce->rx_coalesce_usecs_high ||
+	    coalesce->rx_max_coalesced_frames_high ||
+	    coalesce->tx_coalesce_usecs_high ||
+	    coalesce->tx_max_coalesced_frames_high ||
+	    coalesce->rate_sample_interval)
+		return -EINVAL;
+
+	if ((coalesce->rx_coalesce_usecs == 0) &&
+	    (coalesce->rx_max_coalesced_frames == 0))
+		return -EINVAL;
+
+	if ((coalesce->tx_coalesce_usecs == 0) &&
+	    (coalesce->tx_max_coalesced_frames == 0))
+		return -EINVAL;
+
+	if ((coalesce->rx_coalesce_usecs == 0) &&
+	    (coalesce->rx_max_coalesced_frames == 1))
+		rx_enable = false;
+
+	if ((coalesce->tx_coalesce_usecs == 0) &&
+	    (coalesce->tx_max_coalesced_frames == 1))
+		tx_enable = false;
+
+	/* Set tx interrupt moderation */
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_com_set_interrupt_moderation(adapter->ena_dev,
+						      ENA_IO_TXQ_IDX(i),
+						      tx_enable,
+						      coalesce->tx_max_coalesced_frames,
+						      coalesce->tx_coalesce_usecs);
+		if (rc) {
+			netdev_info(adapter->netdev,
+				    "setting interrupt moderation for TX queue %d failed. err: %d\n",
+				    i, rc);
+			goto err;
+		}
+	}
+
+	/* Set rx interrupt moderation */
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_com_set_interrupt_moderation(adapter->ena_dev,
+						      ENA_IO_RXQ_IDX(i),
+						      rx_enable,
+						      coalesce->tx_max_coalesced_frames,
+						      coalesce->rx_coalesce_usecs);
+
+		if (rc) {
+			netdev_info(adapter->netdev,
+				    "setting interrupt moderation for RX queue %d failed. err: %d\n",
+				    i, rc);
+			goto err;
+		}
+	}
+
+	adapter->tx_usecs = coalesce->tx_coalesce_usecs;
+	adapter->rx_usecs = coalesce->rx_coalesce_usecs;
+
+	adapter->rx_frames = coalesce->rx_max_coalesced_frames_high;
+	adapter->tx_frames = coalesce->tx_max_coalesced_frames_high;
+
+	return 0;
+
+err:
+	return rc;
+}
+
+static int ena_nway_reset(struct net_device *netdev)
+{
+	return -ENODEV;
+}
+
+static u32 ena_get_msglevel(struct net_device *netdev)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	return adapter->msg_enable;
+}
+
+static void ena_set_msglevel(struct net_device *netdev, u32 value)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	adapter->msg_enable = value;
+}
+
+static void ena_get_drvinfo(struct net_device *dev,
+			    struct ethtool_drvinfo *info)
+{
+	struct ena_adapter *adapter = netdev_priv(dev);
+
+	strlcpy(info->driver, DRV_MODULE_NAME, sizeof(info->driver));
+	strlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));
+	strlcpy(info->bus_info, pci_name(adapter->pdev),
+		sizeof(info->bus_info));
+}
+
+static void ena_get_ringparam(struct net_device *netdev,
+			      struct ethtool_ringparam *ring)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_ring *tx_ring = &adapter->tx_ring[0];
+	struct ena_ring *rx_ring = &adapter->rx_ring[0];
+
+	ring->rx_max_pending = ENA_DEFAULT_RX_DESCS;
+	ring->tx_max_pending = ENA_DEFAULT_TX_DESCS;
+	ring->rx_pending = rx_ring->ring_size;
+	ring->tx_pending = tx_ring->ring_size;
+}
+
+static u32 ena_flow_hash_to_flow_type(u16 hash_fields)
+{
+	u32 data = 0;
+
+	if (hash_fields & ENA_ADMIN_RSS_L2_DA)
+		data |= RXH_L2DA;
+
+	if (hash_fields & ENA_ADMIN_RSS_L3_DA)
+		data |= RXH_IP_DST;
+
+	if (hash_fields & ENA_ADMIN_RSS_L3_SA)
+		data |= RXH_IP_SRC;
+
+	if (hash_fields & ENA_ADMIN_RSS_L4_DP)
+		data |= RXH_L4_B_2_3;
+
+	if (hash_fields & ENA_ADMIN_RSS_L4_SP)
+		data |= RXH_L4_B_0_1;
+
+	return data;
+}
+
+static u16 ena_flow_data_to_flow_hash(u32 hash_fields)
+{
+	u16 data = 0;
+
+	if (hash_fields & RXH_L2DA)
+		data |= ENA_ADMIN_RSS_L2_DA;
+
+	if (hash_fields & RXH_IP_DST)
+		data |= ENA_ADMIN_RSS_L3_DA;
+
+	if (hash_fields & RXH_IP_SRC)
+		data |= ENA_ADMIN_RSS_L3_SA;
+
+	if (hash_fields & RXH_L4_B_2_3)
+		data |= ENA_ADMIN_RSS_L4_DP;
+
+	if (hash_fields & RXH_L4_B_0_1)
+		data |= ENA_ADMIN_RSS_L4_SP;
+
+	return data;
+}
+
+static int ena_get_rss_hash(struct ena_com_dev *ena_dev,
+			    struct ethtool_rxnfc *cmd)
+{
+	enum ena_admin_flow_hash_proto proto;
+	u16 hash_fields;
+	int rc;
+
+	cmd->data = 0;
+
+	switch (cmd->flow_type) {
+	case TCP_V4_FLOW:
+		proto = ENA_ADMIN_RSS_TCP4;
+		break;
+	case UDP_V4_FLOW:
+		proto = ENA_ADMIN_RSS_UDP4;
+		break;
+	case TCP_V6_FLOW:
+		proto = ENA_ADMIN_RSS_TCP6;
+		break;
+	case UDP_V6_FLOW:
+		proto = ENA_ADMIN_RSS_UDP6;
+		break;
+	case IPV4_FLOW:
+		proto = ENA_ADMIN_RSS_IP4;
+		break;
+	case IPV6_FLOW:
+		proto = ENA_ADMIN_RSS_IP6;
+		break;
+	case ETHER_FLOW:
+		proto = ENA_ADMIN_RSS_NOT_IP;
+		break;
+	case AH_V4_FLOW:
+	case ESP_V4_FLOW:
+	case AH_V6_FLOW:
+	case ESP_V6_FLOW:
+	case SCTP_V4_FLOW:
+	case AH_ESP_V4_FLOW:
+		/* Unsupported */
+		return -EOPNOTSUPP;
+	default:
+		return -EINVAL;
+	}
+
+	rc = ena_com_get_hash_ctrl(ena_dev, proto, &hash_fields);
+	if (rc) {
+		/* If device don't have permission, return unsupported */
+		if (rc == -EPERM)
+			rc = -EOPNOTSUPP;
+		return rc;
+	}
+
+	cmd->data = ena_flow_hash_to_flow_type(hash_fields);
+
+	return 0;
+}
+
+static int ena_set_rss_hash(struct ena_com_dev *ena_dev,
+			    struct ethtool_rxnfc *cmd)
+{
+	enum ena_admin_flow_hash_proto proto;
+	u16 hash_fields;
+
+	switch (cmd->flow_type) {
+	case TCP_V4_FLOW:
+		proto = ENA_ADMIN_RSS_TCP4;
+		break;
+	case UDP_V4_FLOW:
+		proto = ENA_ADMIN_RSS_UDP4;
+		break;
+	case TCP_V6_FLOW:
+		proto = ENA_ADMIN_RSS_TCP6;
+		break;
+	case UDP_V6_FLOW:
+		proto = ENA_ADMIN_RSS_UDP6;
+		break;
+	case IPV4_FLOW:
+		proto = ENA_ADMIN_RSS_IP4;
+		break;
+	case IPV6_FLOW:
+		proto = ENA_ADMIN_RSS_IP6;
+		break;
+	case ETHER_FLOW:
+		proto = ENA_ADMIN_RSS_NOT_IP;
+		break;
+	case AH_V4_FLOW:
+	case ESP_V4_FLOW:
+	case AH_V6_FLOW:
+	case ESP_V6_FLOW:
+	case SCTP_V4_FLOW:
+	case AH_ESP_V4_FLOW:
+		/* Unsupported */
+		return -EOPNOTSUPP;
+	default:
+		return -EINVAL;
+	}
+
+	hash_fields = ena_flow_data_to_flow_hash(cmd->data);
+
+	return ena_com_fill_hash_ctrl(ena_dev, proto, hash_fields);
+}
+
+static int ena_set_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *info)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	int rc = 0;
+
+	switch (info->cmd) {
+	case ETHTOOL_SRXFH:
+		rc = ena_set_rss_hash(adapter->ena_dev, info);
+		break;
+	case ETHTOOL_SRXCLSRLDEL:
+	case ETHTOOL_SRXCLSRLINS:
+	default:
+		netdev_err(netdev, "Command parameters %d doesn't supported\n",
+			   info->cmd);
+		rc = -EOPNOTSUPP;
+	}
+
+	return (rc == -EPERM) ? -EOPNOTSUPP : rc;
+}
+
+static int ena_get_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *info,
+			 u32 *rules)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	int rc = 0;
+
+	switch (info->cmd) {
+	case ETHTOOL_GRXRINGS:
+		info->data = adapter->num_queues;
+		rc = 0;
+		break;
+	case ETHTOOL_GRXFH:
+		rc = ena_get_rss_hash(adapter->ena_dev, info);
+		break;
+	case ETHTOOL_GRXCLSRLCNT:
+	case ETHTOOL_GRXCLSRULE:
+	case ETHTOOL_GRXCLSRLALL:
+	default:
+		netdev_err(netdev, "Command parameters %x doesn't supported\n",
+			   info->cmd);
+		rc = -EOPNOTSUPP;
+	}
+
+	return (rc == -EPERM) ? -EOPNOTSUPP : rc;
+}
+
+static u32 ena_get_rxfh_indir_size(struct net_device *netdev)
+{
+	return ENA_RX_RSS_TABLE_SIZE;
+}
+
+static u32 ena_get_rxfh_key_size(struct net_device *netdev)
+{
+	return ENA_HASH_KEY_SIZE;
+}
+
+static int ena_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key,
+			u8 *hfunc)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	enum ena_admin_hash_functions ena_func;
+	u8 func;
+	int rc;
+
+	rc = ena_com_indirect_table_get(adapter->ena_dev, indir);
+	if (rc)
+		return rc;
+
+	rc = ena_com_get_hash_function(adapter->ena_dev, &ena_func, key);
+	if (rc)
+		return rc;
+
+	switch (ena_func) {
+	case ENA_ADMIN_TOEPLITZ:
+		func = ETH_RSS_HASH_TOP;
+	case ENA_ADMIN_CRC32:
+		func = ETH_RSS_HASH_XOR;
+	default:
+		netdev_err(netdev, "Command parameters doesn't supported\n");
+		return -EOPNOTSUPP;
+	}
+
+	if (hfunc)
+		*hfunc = func;
+
+	return rc;
+}
+
+static int ena_set_rxfh(struct net_device *netdev, const u32 *indir,
+			const u8 *key, const u8 hfunc)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	enum ena_admin_hash_functions func;
+	int rc, i;
+
+	if (indir) {
+		for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++) {
+			rc = ena_com_indirect_table_fill_entry(ena_dev,
+							       ENA_IO_RXQ_IDX(indir[i]),
+							       i);
+			if (unlikely(rc)) {
+				netdev_err(adapter->netdev,
+					   "Cannot fill indirect table (index is too large)\n");
+				return rc;
+			}
+		}
+
+		rc = ena_com_indirect_table_set(ena_dev);
+		if (rc) {
+			netdev_err(adapter->netdev, "Cannot set indirect table\n");
+			return rc == -EPERM ? -EOPNOTSUPP : rc;
+		}
+	}
+
+	switch (hfunc) {
+	case ETH_RSS_HASH_TOP:
+		func = ENA_ADMIN_TOEPLITZ;
+		break;
+	case ETH_RSS_HASH_XOR:
+		func = ENA_ADMIN_CRC32;
+		break;
+	default:
+		netdev_err(adapter->netdev, "Unsupported hfunc %d\n", hfunc);
+		return -EOPNOTSUPP;
+	}
+
+	if (key) {
+		rc = ena_com_fill_hash_function(ena_dev, func, key,
+						ENA_HASH_KEY_SIZE, 0);
+		if (unlikely(rc)) {
+			netdev_err(adapter->netdev, "Cannot fill key\n");
+			return rc == -EPERM ? -EOPNOTSUPP : rc;
+		}
+	}
+
+	return 0;
+}
+
+static void ena_get_channels(struct net_device *netdev,
+			     struct ethtool_channels *channels)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	channels->max_rx = ENA_MAX_NUM_IO_QUEUES;
+	channels->max_tx = ENA_MAX_NUM_IO_QUEUES;
+	channels->max_other = 0;
+	channels->max_combined = 0;
+	channels->rx_count = adapter->num_queues;
+	channels->tx_count = adapter->num_queues;
+	channels->other_count = 0;
+	channels->combined_count = 0;
+}
+
+static const struct ethtool_ops ena_ethtool_ops = {
+	.get_settings		= ena_get_settings,
+	.get_drvinfo		= ena_get_drvinfo,
+	.get_msglevel		= ena_get_msglevel,
+	.set_msglevel		= ena_set_msglevel,
+	.nway_reset		= ena_nway_reset,
+	.get_link		= ethtool_op_get_link,
+	.get_coalesce		= ena_get_coalesce,
+	.set_coalesce		= ena_ethtool_set_coalesce,
+	.get_ringparam		= ena_get_ringparam,
+	.get_sset_count         = ena_get_sset_count,
+	.get_strings		= ena_get_strings,
+	.get_ethtool_stats      = ena_get_ethtool_stats,
+	.get_rxnfc		= ena_get_rxnfc,
+	.set_rxnfc		= ena_set_rxnfc,
+	.get_rxfh_indir_size    = ena_get_rxfh_indir_size,
+	.get_rxfh_key_size	= ena_get_rxfh_key_size,
+	.get_rxfh		= ena_get_rxfh,
+	.set_rxfh		= ena_set_rxfh,
+	.get_channels		= ena_get_channels,
+};
+
+void ena_set_ethtool_ops(struct net_device *netdev)
+{
+	netdev->ethtool_ops = &ena_ethtool_ops;
+}
+
+void ena_dmup_stats_to_dmesg(struct ena_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	u8* strings_buf;
+	u64* data_buf;
+	int strings_num;
+	int i;
+
+	strings_num = ena_get_sset_count(netdev, ETH_SS_STATS);
+	if (strings_num < 0) {
+		ena_trc_err("Can't get stats num\n");
+		return;
+	}
+
+	strings_buf = devm_kzalloc(&adapter->pdev->dev,
+				   strings_num * ETH_GSTRING_LEN,
+				   GFP_KERNEL);
+	if (!strings_buf) {
+		ena_trc_err("failed to alloc strings_buf\n");
+		return;
+	}
+
+	data_buf = devm_kzalloc(&adapter->pdev->dev,
+				strings_num * sizeof(u64),
+				GFP_KERNEL);
+	if (!data_buf) {
+		ena_trc_err("failed to allocate data buf\n");
+		devm_kfree(&adapter->pdev->dev, strings_buf);
+		return;
+	}
+
+	ena_get_strings(netdev, ETH_SS_STATS, strings_buf);
+	ena_get_ethtool_stats(netdev, NULL, data_buf);
+
+	for (i = 0; i < strings_num; i++)
+		ena_trc_err("%s: %llu\n", strings_buf + i * ETH_GSTRING_LEN,
+			    data_buf[i]);
+
+	devm_kfree(&adapter->pdev->dev, strings_buf);
+	devm_kfree(&adapter->pdev->dev, data_buf);
+}
diff --git a/drivers/amazon/ena/ena_netdev.c b/drivers/amazon/ena/ena_netdev.c
index 07f4a0e..5c1e86d 100644
--- a/drivers/amazon/ena/ena_netdev.c
+++ b/drivers/amazon/ena/ena_netdev.c
@@ -48,14 +48,6 @@
 
 #include "ena_pci_id_tbl.h"
 
-#define DRV_MODULE_NAME		"ena"
-#ifndef DRV_MODULE_VERSION
-#define DRV_MODULE_VERSION      "0.2"
-#endif
-#define DRV_MODULE_RELDATE      "OCT 14, 2015"
-
-#define DEVICE_NAME	"Elastic Network Adapter (ENA)"
-
 static char version[] =
 		DEVICE_NAME DRV_MODULE_NAME " v"
 		DRV_MODULE_VERSION " (" DRV_MODULE_RELDATE ")\n";
@@ -77,7 +69,7 @@ MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
 
 static int push_mode = 3;
 module_param(push_mode, int, 0);
-MODULE_PARM_DESC(push_mode, "1 - Don't push anything to the device memory.\n2 - Push the header buffer to the dev memory.\n3 - Push descriptors and header buffer to the dev memory. (default)\n");
+MODULE_PARM_DESC(push_mode, "1 - Don't push anything to the device memory.\n3 - Push descriptors and header buffer to the dev memory. (default)\n");
 
 static struct ena_aenq_handlers aenq_handlers;
 
@@ -87,8 +79,14 @@ static void ena_tx_timeout(struct net_device *dev)
 {
 	struct ena_adapter *adapter = netdev_priv(dev);
 
-	if (netif_msg_tx_err(adapter))
-		netdev_err(dev, "transmit timed out\n");
+	u64_stats_update_begin(&adapter->syncp);
+	adapter->dev_stats.tx_timeout++;
+	u64_stats_update_end(&adapter->syncp);
+
+	netif_err(adapter, tx_err, dev, "transmit timed out\n");
+
+	/* Change the state of the device to trigger reset */
+	ena_com_set_admin_running_state(adapter->ena_dev, false);
 }
 
 static void update_rx_ring_mtu(struct ena_adapter *adapter, int mtu)
@@ -104,20 +102,21 @@ static int ena_change_mtu(struct net_device *dev, int new_mtu)
 	struct ena_adapter *adapter = netdev_priv(dev);
 	int ret;
 
-	if (new_mtu > adapter->max_mtu) {
-		netdev_err(dev,
-			   "Invalid MTU setting. new_mtu: %d\n", new_mtu);
+	if ((new_mtu > adapter->max_mtu) || (new_mtu < ENA_MIN_MTU)) {
+		netif_err(adapter, drv, dev,
+			  "Invalid MTU setting. new_mtu: %d\n", new_mtu);
+
 		return -EINVAL;
 	}
 
 	ret = ena_com_set_dev_mtu(adapter->ena_dev, new_mtu);
 	if (!ret) {
-		netdev_dbg(adapter->netdev, "set MTU to %d\n", new_mtu);
+		netif_dbg(adapter, drv, dev, "set MTU to %d\n", new_mtu);
 		update_rx_ring_mtu(adapter, new_mtu);
 		dev->mtu = new_mtu;
 	} else {
-		netdev_err(adapter->netdev, "Failed to set MTU to %d\n",
-			   new_mtu);
+		netif_err(adapter, drv, dev, "Failed to set MTU to %d\n",
+			  new_mtu);
 	}
 
 	return ret;
@@ -165,6 +164,10 @@ static void ena_init_io_rings(struct ena_adapter *adapter)
 		ring->qid = i;
 
 		ring->tx_mem_queue_type = ena_dev->tx_mem_queue_type;
+
+		ring->adapter = adapter;
+
+		u64_stats_init(&ring->syncp);
 	}
 
 	/* RX */
@@ -178,16 +181,19 @@ static void ena_init_io_rings(struct ena_adapter *adapter)
 		ring->ring_size = adapter->rx_ring_size;
 		ring->rx_small_copy_len = adapter->small_copy_len;
 		ring->qid = i;
+
+		ring->adapter = adapter;
+
+		u64_stats_init(&ring->syncp);
 	}
 }
 
-/**
- * ena_setup_tx_resources - allocate I/O Tx resources (Descriptors)
+/* ena_setup_tx_resources - allocate I/O Tx resources (Descriptors)
  * @adapter: network interface device structure
  * @qid: queue index
  *
  * Return 0 on success, negative on failure
- **/
+ */
 static int ena_setup_tx_resources(struct ena_adapter *adapter, int qid)
 {
 	struct ena_ring *tx_ring = &adapter->tx_ring[qid];
@@ -196,8 +202,11 @@ static int ena_setup_tx_resources(struct ena_adapter *adapter, int qid)
 
 	dev = &adapter->pdev->dev;
 
-	if ((tx_ring->tx_buffer_info) || (tx_ring->rx_buffer_info))
+	if ((tx_ring->tx_buffer_info) || (tx_ring->rx_buffer_info)) {
+		netif_err(adapter, ifup,
+			  adapter->netdev, "buffer info is NULL");
 		return -EEXIST;
+	}
 
 	size = sizeof(struct ena_tx_buffer) * tx_ring->ring_size;
 
@@ -221,20 +230,17 @@ static int ena_setup_tx_resources(struct ena_adapter *adapter, int qid)
 	return 0;
 }
 
-/**
- * ena_free_tx_resources - Free I/O Tx Resources per Queue
+/* ena_free_tx_resources - Free I/O Tx Resources per Queue
  * @adapter: network interface device structure
  * @qid: queue index
  *
  * Free all transmit software resources
- **/
+ */
 static void ena_free_tx_resources(struct ena_adapter *adapter, int qid)
 {
 	struct ena_ring *tx_ring = &adapter->tx_ring[qid];
 	struct device *dev = &adapter->pdev->dev;
 
-	netdev_dbg(adapter->netdev, "%s qid %d\n", __func__, qid);
-
 	devm_kfree(dev, tx_ring->tx_buffer_info);
 	tx_ring->tx_buffer_info = NULL;
 
@@ -242,12 +248,11 @@ static void ena_free_tx_resources(struct ena_adapter *adapter, int qid)
 	tx_ring->free_tx_ids = NULL;
 }
 
-/**
- * ena_setup_all_tx_resources - allocate I/O Tx queues resources for All queues
+/* ena_setup_all_tx_resources - allocate I/O Tx queues resources for All queues
  * @adapter: private structure
  *
  * Return 0 on success, negative on failure
- **/
+ */
 static int ena_setup_all_tx_resources(struct ena_adapter *adapter)
 {
 	int i, rc = 0;
@@ -262,7 +267,8 @@ static int ena_setup_all_tx_resources(struct ena_adapter *adapter)
 
 err_setup_tx:
 
-	netdev_err(adapter->netdev, "Allocation for Tx Queue %u failed\n", i);
+	netif_err(adapter, ifup, adapter->netdev,
+		  "Allocation for Tx Queue %u failed\n", i);
 
 	/* rewind the index freeing the rings as we go */
 	while (i--)
@@ -270,12 +276,11 @@ err_setup_tx:
 	return rc;
 }
 
-/**
- * ena_free_all_io_tx_resources - Free I/O Tx Resources for All Queues
+/* ena_free_all_io_tx_resources - Free I/O Tx Resources for All Queues
  * @adapter: board private structure
  *
  * Free all transmit software resources
- **/
+ */
 static void ena_free_all_io_tx_resources(struct ena_adapter *adapter)
 {
 	int i;
@@ -284,21 +289,23 @@ static void ena_free_all_io_tx_resources(struct ena_adapter *adapter)
 		ena_free_tx_resources(adapter, i);
 }
 
-/**
- * ena_setup_rx_resources - allocate I/O Rx resources (Descriptors)
+/* ena_setup_rx_resources - allocate I/O Rx resources (Descriptors)
  * @adapter: network interface device structure
  * @qid: queue index
  *
  * Returns 0 on success, negative on failure
- **/
+ */
 static int ena_setup_rx_resources(struct ena_adapter *adapter,
 				  u32 qid)
 {
 	struct ena_ring *rx_ring = &adapter->rx_ring[qid];
 	int size;
 
-	if (rx_ring->rx_buffer_info)
+	if (rx_ring->rx_buffer_info) {
+		netif_err(adapter, ifup,
+			  adapter->netdev, "buffer info is NULL");
 		return -EEXIST;
+	}
 
 	size = sizeof(struct ena_rx_buffer) * rx_ring->ring_size;
 
@@ -318,8 +325,7 @@ static int ena_setup_rx_resources(struct ena_adapter *adapter,
 	return 0;
 }
 
-/**
- * ena_free_rx_resources - Free I/O Rx Resources
+/* ena_free_rx_resources - Free I/O Rx Resources
  * @adapter: network interface device structure
  * @qid: queue index
  *
@@ -334,12 +340,11 @@ static void ena_free_rx_resources(struct ena_adapter *adapter,
 	rx_ring->rx_buffer_info = NULL;
 }
 
-/**
- * ena_setup_all_rx_resources - allocate I/O Rx queues resources for all queues
+/* ena_setup_all_rx_resources - allocate I/O Rx queues resources for all queues
  * @adapter: board private structure
  *
  * Return 0 on success, negative on failure
- **/
+ */
 static int ena_setup_all_rx_resources(struct ena_adapter *adapter)
 {
 	int i, rc = 0;
@@ -354,7 +359,8 @@ static int ena_setup_all_rx_resources(struct ena_adapter *adapter)
 
 err_setup_rx:
 
-	netdev_err(adapter->netdev, "Allocation for Rx Queue %u failed\n", i);
+	netif_err(adapter, ifup, adapter->netdev,
+		  "Allocation for Rx Queue %u failed\n", i);
 
 	/* rewind the index freeing the rings as we go */
 	while (i--)
@@ -362,12 +368,11 @@ err_setup_rx:
 	return rc;
 }
 
-/**
- * ena_free_all_io_rx_resources - Free I/O Rx Resources for All Queues
+/* ena_free_all_io_rx_resources - Free I/O Rx Resources for All Queues
  * @adapter: board private structure
  *
  * Free all receive software resources
- **/
+ */
 static void ena_free_all_rx_resources(struct ena_adapter *adapter)
 {
 	int i;
@@ -402,19 +407,25 @@ static inline int ena_alloc_rx_frag(struct ena_ring *rx_ring,
 	data = netdev_alloc_frag(rx_info->frag_size);
 
 	if (unlikely(!data)) {
-		rx_ring->alloc_fail_cnt++;
+		u64_stats_update_begin(&rx_ring->syncp);
+		rx_ring->rx_stats.skb_alloc_fail++;
+		u64_stats_update_end(&rx_ring->syncp);
 		return -ENOMEM;
 	}
 
 	dma = dma_map_single(rx_ring->dev, data + NET_IP_ALIGN,
 			     rx_info->data_size, DMA_FROM_DEVICE);
 	if (unlikely(dma_mapping_error(rx_ring->dev, dma))) {
+		u64_stats_update_begin(&rx_ring->syncp);
+		rx_ring->rx_stats.dma_mapping_err++;
+		u64_stats_update_end(&rx_ring->syncp);
 		put_page(virt_to_head_page(data));
 		return -EIO;
 	}
-	netdev_dbg(rx_ring->netdev,
-		   "alloc frag %p, rx_info %p len %x skb size %x\n", data,
-		   rx_info, rx_info->data_size, rx_info->frag_size);
+
+	netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+		  "alloc frag %p, rx_info %p len %x skb size %x\n", data,
+		  rx_info, rx_info->data_size, rx_info->frag_size);
 
 	rx_info->data = data;
 
@@ -457,17 +468,18 @@ static int ena_refill_rx_bufs(struct ena_ring *rx_ring, u32 num)
 
 		if (unlikely(
 			ena_alloc_rx_frag(rx_ring, rx_info) < 0)) {
-			netdev_warn(rx_ring->netdev,
-				    "failed to alloc buffer for rx queue %d\n",
-				    rx_ring->qid);
+			netif_warn(rx_ring->adapter, rx_err, rx_ring->netdev,
+				   "failed to alloc buffer for rx queue %d\n",
+				   rx_ring->qid);
 			break;
 		}
 		rc = ena_com_add_single_rx_desc(rx_ring->ena_com_io_sq,
-						&rx_info->ena_buf);
+						&rx_info->ena_buf,
+						next_to_use);
 		if (unlikely(rc)) {
-			netdev_warn(rx_ring->netdev,
-				    "failed to add buffer for rx queue %d\n",
-				    rx_ring->qid);
+			netif_warn(rx_ring->adapter, rx_status, rx_ring->netdev,
+				   "failed to add buffer for rx queue %d\n",
+				   rx_ring->qid);
 			break;
 		}
 		next_to_use = ENA_RX_RING_IDX_NEXT(next_to_use,
@@ -475,6 +487,9 @@ static int ena_refill_rx_bufs(struct ena_ring *rx_ring, u32 num)
 	}
 
 	if (unlikely(i < num)) {
+		u64_stats_update_begin(&rx_ring->syncp);
+		rx_ring->rx_stats.refil_partial++;
+		u64_stats_update_end(&rx_ring->syncp);
 		netdev_warn(rx_ring->netdev,
 			    "refilled rx qid %d with only %d buffers (from %d)\n",
 			    rx_ring->qid, i, num);
@@ -507,11 +522,10 @@ static void ena_free_rx_bufs(struct ena_adapter *adapter,
 	}
 }
 
-/**
- * ena_refill_all_rx_bufs - allocate all queues Rx buffers
+/* ena_refill_all_rx_bufs - allocate all queues Rx buffers
  * @adapter: board private structure
  *
- **/
+ */
 static void ena_refill_all_rx_bufs(struct ena_adapter *adapter)
 {
 	struct ena_ring *rx_ring;
@@ -523,9 +537,9 @@ static void ena_refill_all_rx_bufs(struct ena_adapter *adapter)
 		rc = ena_refill_rx_bufs(rx_ring, bufs_num);
 
 		if (unlikely(rc != bufs_num))
-			netdev_warn(adapter->netdev,
-				    "refilling Queue %d failed. allocated %d buffers from: %d\n",
-				    i, rc, bufs_num);
+			netif_warn(rx_ring->adapter, rx_status, rx_ring->netdev,
+				   "refilling Queue %d failed. allocated %d buffers from: %d\n",
+				   i, rc, bufs_num);
 	}
 }
 
@@ -537,11 +551,10 @@ static void ena_free_all_rx_bufs(struct ena_adapter *adapter)
 		ena_free_rx_bufs(adapter, i);
 }
 
-/**
- * ena_free_tx_bufs - Free Tx Buffers per Queue
+/* ena_free_tx_bufs - Free Tx Buffers per Queue
  * @adapter: network interface device structure
  * @qid: queue index
- **/
+ */
 static void ena_free_tx_bufs(struct ena_ring *tx_ring)
 {
 	u32 i;
@@ -675,8 +688,9 @@ static int ena_clean_tx_irq(struct ena_ring *tx_ring,
 			}
 		}
 
-		dev_dbg(tx_ring->dev, "tx_poll: q %d skb %p completed\n",
-			tx_ring->qid, skb);
+		netif_dbg(tx_ring->adapter, tx_done, tx_ring->netdev,
+			  "tx_poll: q %d skb %p completed\n", tx_ring->qid,
+			  skb);
 
 		tx_bytes += skb->len;
 		dev_kfree_skb(skb);
@@ -693,8 +707,9 @@ static int ena_clean_tx_irq(struct ena_ring *tx_ring,
 
 	netdev_tx_completed_queue(txq, tx_pkts, tx_bytes);
 
-	dev_dbg(tx_ring->dev, "tx_poll: q %d done. total pkts: %d\n",
-		tx_ring->qid, tx_pkts);
+	netif_dbg(tx_ring->adapter, tx_done, tx_ring->netdev,
+		  "tx_poll: q %d done. total pkts: %d\n",
+		  tx_ring->qid, tx_pkts);
 
 	/* need to make the rings circular update visible to
 	 * ena_start_xmit() before checking for netif_queue_stopped().
@@ -704,17 +719,15 @@ static int ena_clean_tx_irq(struct ena_ring *tx_ring,
 	above_thresh = ena_com_sq_empty_space(tx_ring->ena_com_io_sq) >
 		ENA_TX_WAKEUP_THRESH;
 	if (unlikely(netif_tx_queue_stopped(txq) && above_thresh)) {
-		/* TODO I am not sure if I need the tx queue lock.
-		 * Some drivers have this lock (like tg3) but others
-		 * (like mlx4) don't have it.
-		 * It will best if I can ask someone from the network
-		 * community. For now leave the lock
-		 */
 		__netif_tx_lock(txq, smp_processor_id());
 		above_thresh = ena_com_sq_empty_space(tx_ring->ena_com_io_sq) >
 			ENA_TX_WAKEUP_THRESH;
-		if (netif_tx_queue_stopped(txq) && above_thresh)
+		if (netif_tx_queue_stopped(txq) && above_thresh) {
 			netif_tx_wake_queue(txq);
+			u64_stats_update_begin(&tx_ring->syncp);
+			tx_ring->tx_stats.queue_wakeup++;
+			u64_stats_update_end(&tx_ring->syncp);
+		}
 		__netif_tx_unlock(txq);
 	}
 
@@ -722,7 +735,7 @@ static int ena_clean_tx_irq(struct ena_ring *tx_ring,
 }
 
 static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
-				  struct ena_com_buf *ena_bufs,
+				  struct ena_com_rx_buf_info *ena_bufs,
 				  u32 descs,
 				  u16 *next_to_clean)
 {
@@ -735,20 +748,25 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 	ENA_ASSERT(rx_info->data, "Invalid alloc frag buffer\n");
 
 	len = ena_bufs[0].len;
-	netdev_dbg(rx_ring->netdev, "rx_info %p data %p\n", rx_info,
-		   rx_info->data);
+	netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+		  "rx_info %p data %p\n", rx_info, rx_info->data);
 
 	ENA_ASSERT(len > 0, "pkt length is 0\n");
 
 	prefetch(rx_info->data + NET_IP_ALIGN);
 
 	if (len <= rx_ring->rx_small_copy_len) {
-		netdev_dbg(rx_ring->netdev, "rx small packet. len %d\n", len);
+		netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+			  "rx small packet. len %d\n", len);
 
 		skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
 						rx_ring->rx_small_copy_len);
-		if (unlikely(!skb))
+		if (unlikely(!skb)) {
+			u64_stats_update_begin(&rx_ring->syncp);
+			rx_ring->rx_stats.skb_alloc_fail++;
+			u64_stats_update_end(&rx_ring->syncp);
 			return NULL;
+		}
 
 		pci_dma_sync_single_for_cpu(rx_ring->pdev,
 					    dma_unmap_addr(&rx_info->ena_buf, paddr),
@@ -770,8 +788,12 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 			 rx_info->data_size, DMA_FROM_DEVICE);
 
 	skb = napi_get_frags(rx_ring->napi);
-	if (unlikely(!skb))
+	if (unlikely(!skb)) {
+		u64_stats_update_begin(&rx_ring->syncp);
+		rx_ring->rx_stats.skb_alloc_fail++;
+		u64_stats_update_end(&rx_ring->syncp);
 		return NULL;
+	}
 
 	skb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags, rx_info->page,
 			   rx_info->page_offset + NET_IP_ALIGN, len);
@@ -780,8 +802,9 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 	skb->data_len += len;
 	skb->truesize += len;
 
-	netdev_dbg(rx_ring->netdev, "rx skb updated. len %d. data_len %d\n",
-		   skb->len, skb->data_len);
+	netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+		  "rx skb updated. len %d. data_len %d\n",
+		  skb->len, skb->data_len);
 
 	rx_info->data = NULL;
 	*next_to_clean = ENA_RX_RING_IDX_NEXT(*next_to_clean,
@@ -799,8 +822,9 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 				rx_info->page_offset + NET_IP_ALIGN, len,
 				rx_info->data_size);
 
-		netdev_dbg(rx_ring->netdev, "rx skb updated. len %d. data_len %d\n",
-			   skb->len, skb->data_len);
+		netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+			  "rx skb updated. len %d. data_len %d\n",
+			  skb->len, skb->data_len);
 
 		rx_info->data = NULL;
 
@@ -811,12 +835,11 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 	return skb;
 }
 
-/**
- * ena_rx_checksum - indicate in skb if hw indicated a good cksum
+/* ena_rx_checksum - indicate in skb if hw indicated a good cksum
  * @adapter: structure containing adapter specific data
  * @hal_pkt: HAL structure for the packet
  * @skb: skb currently being received and modified
- **/
+ */
 static inline void ena_rx_checksum(struct ena_ring *rx_ring,
 				   struct ena_com_rx_ctx *ena_rx_ctx,
 				   struct sk_buff *skb)
@@ -834,21 +857,26 @@ static inline void ena_rx_checksum(struct ena_ring *rx_ring,
 	}
 
 	/* if IP and error */
-	if (unlikely((ena_rx_ctx->l3_proto == ena_eth_io_l3_proto_ipv4) &&
+	if (unlikely((ena_rx_ctx->l3_proto == ENA_ETH_IO_L3_PROTO_IPV4) &&
 		     (ena_rx_ctx->l3_csum_err))) {
 		/* ipv4 checksum error */
 		skb->ip_summed = CHECKSUM_NONE;
-		rx_ring->bad_checksum++;
-		netdev_err(rx_ring->netdev, "rx ipv4 header checksum error\n");
+		u64_stats_update_begin(&rx_ring->syncp);
+		rx_ring->rx_stats.bad_csum++;
+		u64_stats_update_end(&rx_ring->syncp);
+		netif_err(rx_ring->adapter, rx_err, rx_ring->netdev,
+			  "rx ipv4 header checksum error\n");
 		return;
 	}
 
 	/* if TCP/UDP */
-	if (likely((ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_tcp) ||
-		   (ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_udp))) {
+	if (likely((ena_rx_ctx->l4_proto == ENA_ETH_IO_L4_PROTO_TCP) ||
+		   (ena_rx_ctx->l4_proto == ENA_ETH_IO_L4_PROTO_UDP))) {
 		if (unlikely(ena_rx_ctx->l4_csum_err)) {
 			/* TCP/UDP checksum error */
-			rx_ring->bad_checksum++;
+			u64_stats_update_begin(&rx_ring->syncp);
+			rx_ring->rx_stats.bad_csum++;
+			u64_stats_update_end(&rx_ring->syncp);
 			netdev_err(rx_ring->netdev, "rx L4 checksum error\n");
 			skb->ip_summed = CHECKSUM_NONE;
 			return;
@@ -868,8 +896,8 @@ static void ena_set_rx_hash(struct ena_ring *rx_ring,
 	enum pkt_hash_types hash_type;
 
 	if (likely(rx_ring->netdev->features & NETIF_F_RXHASH)) {
-		if (likely((ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_tcp) ||
-			   (ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_udp)))
+		if (likely((ena_rx_ctx->l4_proto == ENA_ETH_IO_L4_PROTO_TCP) ||
+			   (ena_rx_ctx->l4_proto == ENA_ETH_IO_L4_PROTO_UDP)))
 
 			hash_type = PKT_HASH_TYPE_L4;
 		else
@@ -883,54 +911,60 @@ static void ena_set_rx_hash(struct ena_ring *rx_ring,
 	}
 }
 
-/**
- * ena_clean_rx_irq - Cleanup RX irq
+/* ena_clean_rx_irq - Cleanup RX irq
  * @napi: napi handler
  * @rx_refill_needed: return if refill is required.
  * @budget: how many packets driver is allowed to clean
  *
  * This function the number of cleaned buffers.
- **/
+ */
 static int ena_clean_rx_irq(struct ena_ring *rx_ring,
 			    struct napi_struct *napi,
 			    u32 budget)
 {
 	u16 next_to_clean = rx_ring->next_to_clean;
-	u32 descs, res_budget, work_done;
+	u32 res_budget, work_done;
 
 	struct ena_com_rx_ctx ena_rx_ctx;
+	struct ena_adapter *adapter;
 	struct sk_buff *skb;
 	int refill_required;
 	int refill_threshold;
+	int rc = 0;
+	int total_len = 0;
+	int small_copy_pkt = 0;
 
-	netdev_dbg(rx_ring->netdev, "%s qid %d\n", __func__, rx_ring->qid);
+	netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+		  "%s qid %d\n", __func__, rx_ring->qid);
 	res_budget = budget;
 
 	do {
-		descs = 0;
 		ena_rx_ctx.ena_bufs = rx_ring->ena_bufs;
 		ena_rx_ctx.max_bufs = ENA_PKT_MAX_BUFS;
-		descs = ena_com_rx_pkt(rx_ring->ena_com_io_cq,
-				       rx_ring->ena_com_io_sq,
-				       &ena_rx_ctx);
-		if (unlikely(descs == 0))
+		ena_rx_ctx.descs = 0;
+		rc = ena_com_rx_pkt(rx_ring->ena_com_io_cq,
+				    rx_ring->ena_com_io_sq,
+				    &ena_rx_ctx);
+		if (unlikely(rc))
+			goto error;
+
+		if (unlikely(ena_rx_ctx.descs == 0))
 			break;
 
-		netdev_dbg(rx_ring->netdev,
-			   "rx_poll: q %d got packet from ena. descs #: %d l3 proto %d l4 proto %d frag_pkt: %d\n",
-			   rx_ring->qid, descs, ena_rx_ctx.l3_proto,
-			   ena_rx_ctx.l4_proto, ena_rx_ctx.hash_frag_csum);
+		netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
+			  "rx_poll: q %d got packet from ena. descs #: %d l3 proto %d l4 proto %d frag_pkt: %d\n",
+			  rx_ring->qid, ena_rx_ctx.descs, ena_rx_ctx.l3_proto,
+			  ena_rx_ctx.l4_proto, ena_rx_ctx.hash_frag_csum);
 
 		/* allocate skb and fill it */
-		skb = ena_rx_skb(rx_ring, rx_ring->ena_bufs, descs,
+		skb = ena_rx_skb(rx_ring, rx_ring->ena_bufs, ena_rx_ctx.descs,
 				 &next_to_clean);
 
 		/* exit if we failed to retrieve a buffer */
 		if (unlikely(!skb)) {
 			next_to_clean = ENA_RX_RING_IDX_ADD(next_to_clean,
-							    descs,
+							    ena_rx_ctx.descs,
 							    rx_ring->ring_size);
-			rx_ring->alloc_fail_cnt++;
 			break;
 		}
 
@@ -940,16 +974,26 @@ static int ena_clean_rx_irq(struct ena_ring *rx_ring,
 
 		skb_record_rx_queue(skb, rx_ring->qid);
 
-		if (rx_ring->ena_bufs[0].len <= rx_ring->rx_small_copy_len)
+		if (rx_ring->ena_bufs[0].len <= rx_ring->rx_small_copy_len) {
+			total_len += rx_ring->ena_bufs[0].len;
+			small_copy_pkt = 1;
 			napi_gro_receive(napi, skb);
-		else
+		} else {
+			total_len += skb->len;
 			napi_gro_frags(napi);
+		}
 
 		res_budget--;
 	} while (likely(res_budget));
 
 	work_done = budget - res_budget;
 
+	u64_stats_update_begin(&rx_ring->syncp);
+	rx_ring->rx_stats.bytes += total_len;
+	rx_ring->rx_stats.cnt += work_done;
+	rx_ring->rx_stats.small_copy_len_pkt += small_copy_pkt;
+	u64_stats_update_end(&rx_ring->syncp);
+
 	rx_ring->next_to_clean = next_to_clean;
 
 	refill_required = ena_com_sq_empty_space(rx_ring->ena_com_io_sq);
@@ -960,6 +1004,20 @@ static int ena_clean_rx_irq(struct ena_ring *rx_ring,
 		ena_refill_rx_bufs(rx_ring, refill_required);
 
 	return work_done;
+
+error:
+	adapter = netdev_priv(rx_ring->netdev);
+
+	u64_stats_update_begin(&rx_ring->syncp);
+	rx_ring->rx_stats.bad_desc_num++;
+	u64_stats_update_end(&rx_ring->syncp);
+
+	/* Too many desc from the device.
+	 * Change the state of the device to trigger reset
+	 */
+	ena_com_set_admin_running_state(adapter->ena_dev, false);
+
+	return 0;
 }
 
 static int ena_io_poll(struct napi_struct *napi, int budget)
@@ -969,6 +1027,8 @@ static int ena_io_poll(struct napi_struct *napi, int budget)
 	u32 tx_work_done;
 	u32 rx_work_done;
 	int tx_budget;
+	int napi_comp_call = 0;
+	int ret;
 
 	tx_ring = ena_napi->tx_ring;
 	rx_ring = ena_napi->rx_ring;
@@ -981,15 +1041,23 @@ static int ena_io_poll(struct napi_struct *napi, int budget)
 	if ((budget > rx_work_done) && (tx_budget > tx_work_done)) {
 		napi_complete(napi);
 
+		napi_comp_call = 1;
 		/* Tx and Rx share the same interrupt vector so the driver
 		 *  can unmask either of the interrupts
 		 */
 		ena_com_unmask_intr(rx_ring->ena_com_io_cq);
 
-		return rx_work_done;
+		ret = rx_work_done;
+	} else {
+		ret = budget;
 	}
 
-	return budget;
+	u64_stats_update_begin(&rx_ring->syncp);
+	tx_ring->tx_stats.napi_comp += napi_comp_call;
+	tx_ring->tx_stats.tx_poll++;
+	u64_stats_update_end(&rx_ring->syncp);
+
+	return ret;
 }
 
 static irqreturn_t ena_intr_msix_mgmnt(int irq, void *data)
@@ -1002,11 +1070,10 @@ static irqreturn_t ena_intr_msix_mgmnt(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-/**
- * ena_intr_msix_io - MSIX Interrupt Handler for Tx/Rx
+/* ena_intr_msix_io - MSIX Interrupt Handler for Tx/Rx
  * @irq: interrupt number
  * @data: pointer to a network interface private napi device structure
- **/
+ */
 static irqreturn_t ena_intr_msix_io(int irq, void *data)
 {
 	struct ena_napi *ena_napi = data;
@@ -1022,15 +1089,16 @@ static int ena_enable_msix(struct ena_adapter *adapter,
 	int i, msix_vecs, rc;
 
 	if (adapter->msix_enabled) {
-		dev_err(&adapter->pdev->dev,
-			"Error, MSI-X is already enabled\n");
+		netif_err(adapter, probe, adapter->netdev,
+			  "Error, MSI-X is already enabled\n");
 		return -EPERM;
 	}
 
 	/* Reserved the max msix vectors we might need */
 	msix_vecs = ENA_MAX_MSIX_VEC(num_queues);
 
-	dev_dbg(&adapter->pdev->dev, "trying to enable MSIX, vectors %d\n",
+	netif_dbg(adapter, probe, adapter->netdev,
+		  "trying to enable MSIX, vectors %d\n",
 		msix_vecs);
 
 	adapter->msix_entries = devm_kzalloc(&adapter->pdev->dev,
@@ -1038,9 +1106,9 @@ static int ena_enable_msix(struct ena_adapter *adapter,
 					     GFP_KERNEL);
 
 	if (!adapter->msix_entries) {
-		dev_err(&adapter->pdev->dev,
-			"failed to allocate msix_entries, vectors %d\n",
-			msix_vecs);
+		netif_err(adapter, probe, adapter->netdev,
+			  "failed to allocate msix_entries, vectors %d\n",
+			  msix_vecs);
 		return -ENOMEM;
 	}
 
@@ -1049,19 +1117,19 @@ static int ena_enable_msix(struct ena_adapter *adapter,
 
 	rc = pci_enable_msix(adapter->pdev, adapter->msix_entries, msix_vecs);
 	if (rc != 0) {
-		dev_err(&adapter->pdev->dev,
-			"failed to enable MSIX, vectors %d rc %d\n",
+		netif_err(adapter, probe, adapter->netdev,
+			  "failed to enable MSIX, vectors %d rc %d\n",
 			msix_vecs, rc);
 		return -ENOSPC;
 	}
 
-	dev_dbg(&adapter->pdev->dev,
-		"enable MSIX, vectors %d\n", msix_vecs);
+	netif_dbg(adapter, probe, adapter->netdev, "enable MSIX, vectors %d\n",
+		  msix_vecs);
 
 	if (msix_vecs >= 1) {
 		if (ena_init_rx_cpu_rmap(adapter))
-			dev_warn(&adapter->pdev->dev,
-				 "failed to map irqs to cpus\n");
+			netif_warn(adapter, probe, adapter->netdev,
+				   "failed to map irqs to cpus\n");
 	}
 
 	adapter->msix_vecs = msix_vecs;
@@ -1119,13 +1187,14 @@ static int ena_request_mgmnt_irq(struct ena_adapter *adapter)
 	rc = request_irq(irq->vector, irq->handler, flags, irq->name,
 			 irq->data);
 	if (rc) {
-		netdev_err(adapter->netdev, "failed to request admin irq\n");
+		netif_err(adapter, probe, adapter->netdev,
+			  "failed to request admin irq\n");
 		return rc;
 	}
 
-		netdev_dbg(adapter->netdev,
-			   "set affinity hint of mgmnt irq.to 0x%lx (irq vector: %d)\n",
-			   irq->affinity_hint_mask.bits[0], irq->vector);
+		netif_dbg(adapter, probe, adapter->netdev,
+			  "set affinity hint of mgmnt irq.to 0x%lx (irq vector: %d)\n",
+			  irq->affinity_hint_mask.bits[0], irq->vector);
 
 		irq_set_affinity_hint(irq->vector, &irq->affinity_hint_mask);
 
@@ -1139,7 +1208,8 @@ static int ena_request_io_irq(struct ena_adapter *adapter)
 	int rc = 0, i, k;
 
 	if (!adapter->msix_enabled) {
-		netdev_err(adapter->netdev, "failed to request irq\n");
+		netif_err(adapter, ifup, adapter->netdev,
+			  "failed to request irq\n");
 		return -EINVAL;
 	}
 
@@ -1148,15 +1218,15 @@ static int ena_request_io_irq(struct ena_adapter *adapter)
 		rc = request_irq(irq->vector, irq->handler, flags, irq->name,
 				 irq->data);
 		if (rc) {
-			netdev_err(adapter->netdev,
-				   "failed to request irq. index %d rc %d\n",
+			netif_err(adapter, ifup, adapter->netdev,
+				  "failed to request irq. index %d rc %d\n",
 				   i, rc);
 			goto err;
 		}
 
-		netdev_dbg(adapter->netdev,
-			   "set affinity hint of irq. index %d to 0x%lx (irq vector: %d)\n",
-			   i, irq->affinity_hint_mask.bits[0], irq->vector);
+		netif_dbg(adapter, ifup, adapter->netdev,
+			  "set affinity hint of irq. index %d to 0x%lx (irq vector: %d)\n",
+			  i, irq->affinity_hint_mask.bits[0], irq->vector);
 
 		irq_set_affinity_hint(irq->vector, &irq->affinity_hint_mask);
 	}
@@ -1272,9 +1342,37 @@ static void ena_restore_ethtool_params(struct ena_adapter *adapter)
 	adapter->rx_frames = 1;
 }
 
-static void ena_up_complete(struct ena_adapter *adapter)
+/* Configure the Rx forwarding */
+static int ena_rss_configure(struct ena_adapter *adapter)
 {
-	int i;
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	int rc;
+
+	/* Set indirect table */
+	rc = ena_com_indirect_table_set(ena_dev);
+	if (unlikely(rc && rc != -EPERM))
+		return rc;
+
+	/* Configure hash function (if supported) */
+	rc = ena_com_set_hash_function(ena_dev);
+	if (unlikely(rc && (rc != -EPERM)))
+		return rc;
+
+	/* Configure hash inputs (if supported) */
+	rc = ena_com_set_hash_ctrl(ena_dev);
+	if (unlikely(rc && (rc != -EPERM)))
+		return rc;
+
+	return 0;
+}
+
+static int ena_up_complete(struct ena_adapter *adapter)
+{
+	int rc, i;
+
+	rc = ena_rss_configure(adapter);
+	if (rc)
+		return rc;
 
 	ena_init_napi(adapter);
 
@@ -1294,6 +1392,8 @@ static void ena_up_complete(struct ena_adapter *adapter)
 	 */
 	for (i = 0; i < adapter->num_queues; i++)
 		napi_schedule(&adapter->ena_napi[i].napi);
+
+	return 0;
 }
 
 static int ena_create_io_tx_queue(struct ena_adapter *adapter, int qid)
@@ -1315,9 +1415,9 @@ static int ena_create_io_tx_queue(struct ena_adapter *adapter, int qid)
 				     ena_dev->tx_mem_queue_type, msix_vector,
 				     adapter->tx_ring_size);
 	if (rc) {
-		netdev_err(adapter->netdev,
-			   "failed to create io TX queue num %d rc: %d\n",
-			   qid, rc);
+		netif_err(adapter, ifup, adapter->netdev,
+			  "failed to create io TX queue num %d rc: %d\n",
+			  qid, rc);
 		return rc;
 	}
 
@@ -1325,9 +1425,9 @@ static int ena_create_io_tx_queue(struct ena_adapter *adapter, int qid)
 				     &tx_ring->ena_com_io_sq,
 				     &tx_ring->ena_com_io_cq);
 	if (rc) {
-		netdev_err(adapter->netdev,
-			   "failed to get tx queue handlers. TX queue num %d rc: %d\n",
-			   qid, rc);
+		netif_err(adapter, ifup, adapter->netdev,
+			  "failed to get tx queue handlers. TX queue num %d rc: %d\n",
+			  qid, rc);
 		ena_com_destroy_io_queue(ena_dev, ena_qid);
 	}
 
@@ -1370,12 +1470,13 @@ static int ena_create_io_rx_queue(struct ena_adapter *adapter, int qid)
 
 	rc = ena_com_create_io_queue(adapter->ena_dev, ena_qid,
 				     ENA_COM_IO_QUEUE_DIRECTION_RX,
-				     ENA_MEM_QUEUE_TYPE_HOST_MEMORY,
+				     ENA_ADMIN_PLACEMENT_POLICY_HOST,
 				     msix_vector,
 				     adapter->rx_ring_size);
 	if (rc) {
-		pr_err("failed to create io RX queue  num %d rc: %d\n",
-		       qid, rc);
+		netif_err(adapter, ifup, adapter->netdev,
+			  "failed to create io RX queue  num %d rc: %d\n",
+			  qid, rc);
 		return rc;
 	}
 
@@ -1383,9 +1484,9 @@ static int ena_create_io_rx_queue(struct ena_adapter *adapter, int qid)
 				     &rx_ring->ena_com_io_sq,
 				     &rx_ring->ena_com_io_cq);
 	if (rc) {
-		netdev_err(adapter->netdev,
-			   "failed to get rx queue handlers. RX queue  num %d rc: %d\n",
-			   qid, rc);
+		netif_err(adapter, ifup, adapter->netdev,
+			  "failed to get rx queue handlers. RX queue  num %d rc: %d\n",
+			  qid, rc);
 		ena_com_destroy_io_queue(ena_dev, ena_qid);
 		return rc;
 	}
@@ -1446,15 +1547,23 @@ static int ena_up(struct ena_adapter *adapter)
 	if (rc)
 		goto err_create_rx_queues;
 
+	rc = ena_up_complete(adapter);
+	if (rc)
+		goto err_up;
+
 	if (adapter->link_status)
 		netif_carrier_on(adapter->netdev);
 
-	ena_up_complete(adapter);
+	u64_stats_update_begin(&adapter->syncp);
+	adapter->dev_stats.interface_up++;
+	u64_stats_update_end(&adapter->syncp);
 
 	adapter->up = true;
 
 	return rc;
 
+err_up:
+	ena_destroy_all_rx_queues(adapter);
 err_create_rx_queues:
 	ena_destroy_all_tx_queues(adapter);
 err_create_tx_queues:
@@ -1468,75 +1577,16 @@ err_req_irq:
 	return rc;
 }
 
-#if 0
-static int
-ena_flow_steer(struct net_device *netdev, const struct sk_buff *skb,
-	       u16 rxq_index, u32 flow_id)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-	int rc = 0;
-
-	if ((skb->protocol != htons(ETH_P_IP)) &&
-	    (skb->protocol != htons(ETH_P_IPV6)))
-		return -EPROTONOSUPPORT;
-
-	if (skb->protocol == htons(ETH_P_IP)) {
-		if (ip_is_fragment(ip_hdr(skb)))
-			return -EPROTONOSUPPORT;
-		if ((ip_hdr(skb)->protocol != IPPROTO_TCP) &&
-		    (ip_hdr(skb)->protocol != IPPROTO_UDP))
-			return -EPROTONOSUPPORT;
-	}
-
-	if (skb->protocol == htons(ETH_P_IPV6)) {
-		/* ipv6 with extension not supported yet */
-		if ((ipv6_hdr(skb)->nexthdr != IPPROTO_TCP) &&
-		    (ipv6_hdr(skb)->nexthdr != IPPROTO_UDP))
-			return -EPROTONOSUPPORT;
-	}
-	rc = flow_id & (ENA_RX_THASH_TABLE_SIZE - 1);
-
-	adapter->rss_ind_tbl[rc] = rxq_index;
-	/* return the below func
-	 * ena_thash_table_set(&adapter->hal_adapter, rc,
-	 * adapter->udma_num, rxq_index);
-	 */
-	if (skb->protocol == htons(ETH_P_IP)) {
-		int nhoff = skb_network_offset(skb);
-		const struct iphdr *ip =
-			(const struct iphdr *)(skb->data + nhoff);
-		const __be16 *ports =
-			(const __be16 *)(skb->data + nhoff + 4 * ip->ihl);
-
-		netdev_info(adapter->netdev,
-			    "steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d]\n",
-			    (ip->protocol == IPPROTO_TCP) ? "TCP" : "UDP",
-			    &ip->saddr, ntohs(ports[0]),
-			    &ip->daddr, ntohs(ports[1]),
-			    rxq_index, flow_id, rc);
-	} else {
-		struct ipv6hdr *ip6h = ipv6_hdr(skb);
-		const __be16 *ports = (const __be16 *)skb_transport_header(skb);
-
-		netdev_info(adapter->netdev,
-			    "steering %s %pI6c:%u:%pI6c:%u to queue %u [flow %u filter %d]\n",
-			    (ipv6_hdr(skb)->nexthdr == IPPROTO_TCP) ? "TCP" : "UDP",
-			    &ip6h->saddr,
-			    ntohs(ports[0]),
-			    &ip6h->daddr,
-			    ntohs(ports[1]),
-			    rxq_index, flow_id, rc);
-	}
-
-	return rc;
-}
-#endif /* if 0 */
 static void ena_down(struct ena_adapter *adapter)
 {
-	netdev_info(adapter->netdev, "%s\n", __func__);
+	netif_info(adapter, ifdown, adapter->netdev, "%s\n", __func__);
 
 	adapter->up = false;
 
+	u64_stats_update_begin(&adapter->syncp);
+	adapter->dev_stats.interface_down++;
+	u64_stats_update_end(&adapter->syncp);
+
 	netif_carrier_off(adapter->netdev);
 	netif_tx_disable(adapter->netdev);
 
@@ -1553,8 +1603,7 @@ static void ena_down(struct ena_adapter *adapter)
 	ena_free_all_rx_resources(adapter);
 }
 
-/**
- * ena_open - Called when a network interface is made active
+/* ena_open - Called when a network interface is made active
  * @netdev: network interface device structure
  *
  * Returns 0 on success, negative value on failure
@@ -1564,7 +1613,7 @@ static void ena_down(struct ena_adapter *adapter)
  * for transmit and receive operations are allocated, the interrupt
  * handler is registered with the OS, the watchdog timer is started,
  * and the stack is notified that the interface is ready.
- **/
+ */
 static int ena_open(struct net_device *netdev)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -1572,12 +1621,16 @@ static int ena_open(struct net_device *netdev)
 
 	/* Notify the stack of the actual queue counts. */
 	rc = netif_set_real_num_tx_queues(netdev, adapter->num_queues);
-	if (rc)
+	if (rc) {
+		netif_err(adapter, ifup, netdev, "Can't set num tx queues\n");
 		return rc;
+	}
 
 	rc = netif_set_real_num_rx_queues(netdev, adapter->num_queues);
-	if (rc)
+	if (rc) {
+		netif_err(adapter, ifup, netdev, "Can't set num rx queues\n");
 		return rc;
+	}
 
 	rc = ena_up(adapter);
 	if (rc)
@@ -1586,8 +1639,7 @@ static int ena_open(struct net_device *netdev)
 	return rc;
 }
 
-/**
- * ena_close - Disables a network interface
+/* ena_close - Disables a network interface
  * @netdev: network interface device structure
  *
  * Returns 0, this is not allowed to fail
@@ -1601,7 +1653,7 @@ static int ena_close(struct net_device *netdev)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
-	netdev_dbg(adapter->netdev, "%s\n", __func__);
+	netif_dbg(adapter, ifdown, netdev, "%s\n", __func__);
 
 	if (adapter->up)
 		ena_down(adapter);
@@ -1611,309 +1663,6 @@ static int ena_close(struct net_device *netdev)
 	return 0;
 }
 
-static int ena_get_settings(struct net_device *netdev,
-			    struct ethtool_cmd *ecmd)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-	struct ena_com_dev *ena_dev = adapter->ena_dev;
-	struct ena_admin_get_feature_link_desc *link;
-	struct ena_admin_get_feat_resp feat_resp;
-	int rc;
-
-	rc = ena_com_get_link_params(ena_dev, &feat_resp);
-	if (rc)
-		return rc;
-
-	link = &feat_resp.u.link;
-
-	ethtool_cmd_speed_set(ecmd, link->speed);
-
-	if (link->flags & ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_MASK)
-		ecmd->duplex = DUPLEX_FULL;
-	else
-		ecmd->duplex = DUPLEX_HALF;
-
-	if (link->flags & ENA_ADMIN_GET_FEATURE_LINK_DESC_AUTONEG_MASK)
-		ecmd->autoneg = AUTONEG_ENABLE;
-	else
-		ecmd->autoneg = AUTONEG_DISABLE;
-
-	return 0;
-}
-
-static int ena_get_coalesce(struct net_device *net_dev,
-			    struct ethtool_coalesce *coalesce)
-{
-	struct ena_adapter *adapter = netdev_priv(net_dev);
-
-	coalesce->tx_coalesce_usecs = adapter->tx_usecs;
-	coalesce->rx_coalesce_usecs = adapter->rx_usecs;
-	coalesce->rx_max_coalesced_frames = adapter->rx_frames;
-	coalesce->tx_max_coalesced_frames = adapter->tx_frames;
-	coalesce->use_adaptive_rx_coalesce = false;
-
-	return 0;
-}
-
-static int ena_ethtool_set_coalesce(struct net_device *net_dev,
-				    struct ethtool_coalesce *coalesce)
-{
-	struct ena_adapter *adapter = netdev_priv(net_dev);
-	bool tx_enable = true;
-	bool rx_enable = true;
-	int i, rc;
-
-	/* The above params are unsupported by our driver */
-	if (coalesce->rx_coalesce_usecs_irq ||
-	    coalesce->rx_max_coalesced_frames_irq ||
-	    coalesce->tx_coalesce_usecs_irq ||
-	    coalesce->tx_max_coalesced_frames_irq ||
-	    coalesce->stats_block_coalesce_usecs ||
-	    coalesce->use_adaptive_rx_coalesce ||
-	    coalesce->use_adaptive_tx_coalesce ||
-	    coalesce->pkt_rate_low ||
-	    coalesce->rx_coalesce_usecs_low ||
-	    coalesce->rx_max_coalesced_frames_low ||
-	    coalesce->tx_coalesce_usecs_low ||
-	    coalesce->tx_max_coalesced_frames_low ||
-	    coalesce->pkt_rate_high ||
-	    coalesce->rx_coalesce_usecs_high ||
-	    coalesce->rx_max_coalesced_frames_high ||
-	    coalesce->tx_coalesce_usecs_high ||
-	    coalesce->tx_max_coalesced_frames_high ||
-	    coalesce->rate_sample_interval)
-		return -EINVAL;
-
-	if ((coalesce->rx_coalesce_usecs == 0) &&
-	    (coalesce->rx_max_coalesced_frames == 0))
-		return -EINVAL;
-
-	if ((coalesce->tx_coalesce_usecs == 0) &&
-	    (coalesce->tx_max_coalesced_frames == 0))
-		return -EINVAL;
-
-	if ((coalesce->rx_coalesce_usecs == 0) &&
-	    (coalesce->rx_max_coalesced_frames == 1))
-		rx_enable = false;
-
-	if ((coalesce->tx_coalesce_usecs == 0) &&
-	    (coalesce->tx_max_coalesced_frames == 1))
-		tx_enable = false;
-
-	/* Set tx interrupt moderation */
-	for (i = 0; i < adapter->num_queues; i++) {
-		rc = ena_com_set_interrupt_moderation(adapter->ena_dev,
-						      ENA_IO_TXQ_IDX(i),
-						      tx_enable,
-						      coalesce->tx_max_coalesced_frames,
-						      coalesce->tx_coalesce_usecs);
-		if (rc) {
-			netdev_info(adapter->netdev,
-				    "setting interrupt moderation for TX queue %d failed. err: %d\n",
-				    i, rc);
-			goto err;
-		}
-	}
-
-	/* Set rx interrupt moderation */
-	for (i = 0; i < adapter->num_queues; i++) {
-		rc = ena_com_set_interrupt_moderation(adapter->ena_dev,
-						      ENA_IO_RXQ_IDX(i),
-						      rx_enable,
-						      coalesce->tx_max_coalesced_frames,
-						      coalesce->rx_coalesce_usecs);
-
-		if (rc) {
-			netdev_info(adapter->netdev,
-				    "setting interrupt moderation for RX queue %d failed. err: %d\n",
-				    i, rc);
-			goto err;
-		}
-	}
-
-	adapter->tx_usecs = coalesce->tx_coalesce_usecs;
-	adapter->rx_usecs = coalesce->rx_coalesce_usecs;
-
-	adapter->rx_frames = coalesce->rx_max_coalesced_frames_high;
-	adapter->tx_frames = coalesce->tx_max_coalesced_frames_high;
-
-	return 0;
-
-err:
-	return rc;
-}
-
-static int ena_nway_reset(struct net_device *netdev)
-{
-	return -ENODEV;
-}
-
-static u32 ena_get_msglevel(struct net_device *netdev)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-
-	return adapter->msg_enable;
-}
-
-static void ena_set_msglevel(struct net_device *netdev, u32 value)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-
-	adapter->msg_enable = value;
-}
-
-static struct rtnl_link_stats64 *ena_get_stats64(struct net_device *netdev,
-						 struct rtnl_link_stats64 *stats)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-	struct ena_admin_basic_stats ena_stats;
-	int rc;
-
-	if (!adapter->up)
-		return NULL;
-
-	rc = ena_com_get_dev_basic_stats(adapter->ena_dev, &ena_stats);
-	if (rc)
-		return NULL;
-
-	stats->tx_bytes = ((u64)ena_stats.tx_bytes_high << 32) |
-		ena_stats.tx_bytes_low;
-	stats->rx_bytes = ((u64)ena_stats.rx_bytes_high << 32) |
-		ena_stats.rx_bytes_low;
-
-	stats->rx_packets = ((u64)ena_stats.rx_pkts_high << 32) |
-		ena_stats.rx_pkts_low;
-	stats->tx_packets = ((u64)ena_stats.tx_pkts_high << 32) |
-		ena_stats.tx_pkts_low;
-
-	stats->rx_dropped = ((u64)ena_stats.rx_drops_high << 32) |
-		ena_stats.rx_drops_low;
-
-	stats->multicast = 0;
-	stats->collisions = 0;
-
-	stats->rx_length_errors = 0;
-	stats->rx_crc_errors = 0;
-	stats->rx_frame_errors = 0;
-	stats->rx_fifo_errors = 0;
-	stats->rx_missed_errors = 0;
-	stats->tx_window_errors = 0;
-
-	stats->rx_errors = 0;
-	stats->tx_errors = 0;
-
-	return stats;
-}
-
-static void ena_get_drvinfo(struct net_device *dev,
-			    struct ethtool_drvinfo *info)
-{
-	struct ena_adapter *adapter = netdev_priv(dev);
-
-	strlcpy(info->driver, DRV_MODULE_NAME, sizeof(info->driver));
-	strlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));
-	strlcpy(info->bus_info, pci_name(adapter->pdev),
-		sizeof(info->bus_info));
-}
-
-static void ena_get_ringparam(struct net_device *netdev,
-			      struct ethtool_ringparam *ring)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-	struct ena_ring *tx_ring = &adapter->tx_ring[0];
-	struct ena_ring *rx_ring = &adapter->rx_ring[0];
-
-	ring->rx_max_pending = ENA_DEFAULT_RX_DESCS;
-	ring->tx_max_pending = ENA_DEFAULT_TX_DESCS;
-	ring->rx_pending = rx_ring->ring_size;
-	ring->tx_pending = tx_ring->ring_size;
-}
-
-static int ena_get_rxnfc(struct net_device *netdev,
-			 struct ethtool_rxnfc *info, u32 *rules __always_unused)
-{
-	/*struct ena_adapter *adapter = netdev_priv(netdev);*/
-
-	switch (info->cmd) {
-	case ETHTOOL_GRXRINGS:
-		info->data = ENA_MAX_NUM_IO_QUEUES;
-		return 0;
-	default:
-		netdev_err(netdev, "Command parameters not supported\n");
-		return -EOPNOTSUPP;
-	}
-}
-
-static u32 ena_get_rxfh_indir_size(struct net_device *netdev)
-{
-	/* TODO pending on rxfh defenition */
-	return ENA_RX_RSS_TABLE_SIZE;
-}
-
-static int ena_get_rxfh_indir(struct net_device *netdev,
-			      u32 *indir, u8 *key, u8 *hfunc)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-	int i;
-
-	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++)
-		indir[i] = adapter->rss_ind_tbl[i];
-
-	return 0;
-}
-
-static int ena_set_rxfh_indir(struct net_device *netdev,
-			      const u32 *indir, const u8 *key,
-			      const u8 hfunc)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-	size_t i;
-
-	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++) {
-		adapter->rss_ind_tbl[i] = indir[i];
-
-		/* TODO FSS table
-		 * al_eth_thash_table_set(&adapter->hal_adapter, i,
-		 * adapter->udma_num, indir[i]);
-		 */
-	}
-
-	return 0;
-}
-
-static void ena_get_channels(struct net_device *netdev,
-			     struct ethtool_channels *channels)
-{
-	struct ena_adapter *adapter = netdev_priv(netdev);
-
-	channels->max_rx = ENA_MAX_NUM_IO_QUEUES;
-	channels->max_tx = ENA_MAX_NUM_IO_QUEUES;
-	channels->max_other = 0;
-	channels->max_combined = 0;
-	channels->rx_count = adapter->num_queues;
-	channels->tx_count = adapter->num_queues;
-	channels->other_count = 0;
-	channels->combined_count = 0;
-}
-
-static const struct ethtool_ops ena_ethtool_ops = {
-	.get_settings		= ena_get_settings,
-	.set_settings		= NULL,
-	.get_drvinfo		= ena_get_drvinfo,
-	.get_msglevel		= ena_get_msglevel,
-	.set_msglevel		= ena_set_msglevel,
-	.nway_reset		= ena_nway_reset,
-	.get_link		= ethtool_op_get_link,
-	.get_coalesce		= ena_get_coalesce,
-	.set_coalesce		= ena_ethtool_set_coalesce,
-	.get_ringparam		= ena_get_ringparam,
-	.get_rxnfc		= ena_get_rxnfc,
-	.get_rxfh_indir_size    = ena_get_rxfh_indir_size,
-	.get_rxfh		= ena_get_rxfh_indir,
-	.set_rxfh		= ena_set_rxfh_indir,
-	.get_channels		= ena_get_channels,
-};
-
 static void ena_tx_csum(struct ena_com_tx_ctx *ena_tx_ctx, struct sk_buff *skb)
 {
 	u32 mss = skb_shinfo(skb)->gso_size;
@@ -1933,20 +1682,20 @@ static void ena_tx_csum(struct ena_com_tx_ctx *ena_tx_ctx, struct sk_buff *skb)
 
 		switch (skb->protocol) {
 		case htons(ETH_P_IP):
-			ena_tx_ctx->l3_proto = ena_eth_io_l3_proto_ipv4;
+			ena_tx_ctx->l3_proto = ENA_ETH_IO_L3_PROTO_IPV4;
 			if (mss)
 				ena_tx_ctx->l3_csum_enable = true;
 			if (ip_hdr(skb)->protocol == IPPROTO_TCP)
-				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_tcp;
+				ena_tx_ctx->l4_proto = ENA_ETH_IO_L4_PROTO_TCP;
 			else
-				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_udp;
+				ena_tx_ctx->l4_proto = ENA_ETH_IO_L4_PROTO_UDP;
 			break;
 		case htons(ETH_P_IPV6):
-			ena_tx_ctx->l3_proto = ena_eth_io_l3_proto_ipv6;
+			ena_tx_ctx->l3_proto = ENA_ETH_IO_L3_PROTO_IPV6;
 			if (ip_hdr(skb)->protocol == IPPROTO_TCP)
-				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_tcp;
+				ena_tx_ctx->l4_proto = ENA_ETH_IO_L4_PROTO_TCP;
 			else
-				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_udp;
+				ena_tx_ctx->l4_proto = ENA_ETH_IO_L4_PROTO_UDP;
 			break;
 		default:
 			break;
@@ -1965,8 +1714,7 @@ static void ena_tx_csum(struct ena_com_tx_ctx *ena_tx_ctx, struct sk_buff *skb)
 	}
 }
 
-/* Called with netif_tx_lock.
- */
+/* Called with netif_tx_lock. */
 static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 				  struct net_device *dev)
 {
@@ -2005,7 +1753,7 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 	ena_buf = tx_info->bufs;
 	tx_info->skb = skb;
 
-	if (tx_ring->tx_mem_queue_type == ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY) {
+	if (tx_ring->tx_mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {
 		/* prepared the push buffer */
 		push_len = min_t(u32, len, ENA_MAX_PUSH_PKT_SIZE);
 		header_len = push_len;
@@ -2023,9 +1771,12 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 		dma = dma_map_single(tx_ring->dev, skb->data + push_len,
 				     len - push_len, DMA_TO_DEVICE);
 		if (dma_mapping_error(tx_ring->dev, dma)) {
+			u64_stats_update_begin(&tx_ring->syncp);
+			tx_ring->tx_stats.dma_mapping_err++;
+			u64_stats_update_end(&tx_ring->syncp);
 			dev_kfree_skb(skb);
 			netdev_warn(adapter->netdev, "failed to map skb\n");
-			return NETDEV_TX_OK;
+			return NETDEV_TX_BUSY;
 		}
 		ena_buf->paddr = dma;
 		ena_buf->len = len - push_len;
@@ -2043,6 +1794,9 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 				   "frag[%d]: addr:0x%llx, len 0x%x\n", i,
 				   (unsigned long long)tx_info->bufs[i].paddr,
 				   tx_info->bufs[i].len);
+		u64_stats_update_begin(&tx_ring->syncp);
+		tx_ring->tx_stats.unsupported_desc_num++;
+		u64_stats_update_end(&tx_ring->syncp);
 		goto dma_error;
 	}
 
@@ -2052,8 +1806,12 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 		len = skb_frag_size(frag);
 		dma = skb_frag_dma_map(tx_ring->dev, frag, 0, len,
 				       DMA_TO_DEVICE);
-		if (dma_mapping_error(tx_ring->dev, dma))
+		if (dma_mapping_error(tx_ring->dev, dma)) {
+			u64_stats_update_begin(&tx_ring->syncp);
+			tx_ring->tx_stats.dma_mapping_err++;
+			u64_stats_update_end(&tx_ring->syncp);
 			goto dma_error;
+		}
 		ena_buf->paddr = dma;
 		ena_buf->len = len;
 		ena_buf++;
@@ -2061,8 +1819,6 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 
 	tx_info->num_of_bufs += last_frag;
 
-	netdev_tx_sent_queue(txq, skb->len);
-
 	memset(&ena_tx_ctx, 0x0, sizeof(struct ena_com_tx_ctx));
 	ena_tx_ctx.ena_bufs = tx_info->bufs;
 	ena_tx_ctx.push_header = push_hdr;
@@ -2079,9 +1835,21 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 
 	if (unlikely(rc)) {
 		netdev_err(adapter->netdev, "failed to prepare tx bufs\n");
+		u64_stats_update_begin(&tx_ring->syncp);
+		tx_ring->tx_stats.queue_stop++;
+		tx_ring->tx_stats.prepare_ctx_err++;
+		u64_stats_update_end(&tx_ring->syncp);
 		netif_tx_stop_queue(txq);
 		goto dma_error;
 	}
+
+	netdev_tx_sent_queue(txq, skb->len);
+
+	u64_stats_update_begin(&tx_ring->syncp);
+	tx_ring->tx_stats.cnt++;
+	tx_ring->tx_stats.bytes += skb->len;
+	u64_stats_update_end(&tx_ring->syncp);
+
 	tx_info->tx_descs = nb_hw_desc;
 
 	tx_ring->next_to_use = ENA_TX_RING_IDX_NEXT(next_to_use,
@@ -2102,6 +1870,9 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 			__func__, qid);
 
 		netif_tx_stop_queue(txq);
+		u64_stats_update_begin(&tx_ring->syncp);
+		tx_ring->tx_stats.queue_stop++;
+		u64_stats_update_end(&tx_ring->syncp);
 
 		/* There is a rare condition where this function decide to
 		 * stop the queue but meanwhile clean_tx_irq updates
@@ -2113,13 +1884,21 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
 		smp_rmb();
 
 		if (ena_com_sq_empty_space(tx_ring->ena_com_io_sq)
-				> ENA_TX_WAKEUP_THRESH)
+				> ENA_TX_WAKEUP_THRESH) {
 			netif_tx_wake_queue(txq);
+			u64_stats_update_begin(&tx_ring->syncp);
+			tx_ring->tx_stats.queue_wakeup++;
+			u64_stats_update_end(&tx_ring->syncp);
+		}
 	}
 
-	if (netif_xmit_stopped(txq) || !skb->xmit_more)
+	if (netif_xmit_stopped(txq) || !skb->xmit_more) {
 		/* trigger the dma engine */
 		ena_com_write_sq_doorbell(tx_ring->ena_com_io_sq);
+		u64_stats_update_begin(&tx_ring->syncp);
+		tx_ring->tx_stats.doorbells++;
+		u64_stats_update_end(&tx_ring->syncp);
+	}
 
 	return NETDEV_TX_OK;
 
@@ -2174,6 +1953,49 @@ static u16 ena_select_queue(struct net_device *dev, struct sk_buff *skb
 	return qid;
 }
 
+static struct rtnl_link_stats64 *ena_get_stats64(struct net_device *netdev,
+						 struct rtnl_link_stats64 *stats)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_admin_basic_stats ena_stats;
+	int rc;
+
+	if (!adapter->up)
+		return NULL;
+
+	rc = ena_com_get_dev_basic_stats(adapter->ena_dev, &ena_stats);
+	if (rc)
+		return NULL;
+
+	stats->tx_bytes = ((u64)ena_stats.tx_bytes_high << 32) |
+		ena_stats.tx_bytes_low;
+	stats->rx_bytes = ((u64)ena_stats.rx_bytes_high << 32) |
+		ena_stats.rx_bytes_low;
+
+	stats->rx_packets = ((u64)ena_stats.rx_pkts_high << 32) |
+		ena_stats.rx_pkts_low;
+	stats->tx_packets = ((u64)ena_stats.tx_pkts_high << 32) |
+		ena_stats.tx_pkts_low;
+
+	stats->rx_dropped = ((u64)ena_stats.rx_drops_high << 32) |
+		ena_stats.rx_drops_low;
+
+	stats->multicast = 0;
+	stats->collisions = 0;
+
+	stats->rx_length_errors = 0;
+	stats->rx_crc_errors = 0;
+	stats->rx_frame_errors = 0;
+	stats->rx_fifo_errors = 0;
+	stats->rx_missed_errors = 0;
+	stats->tx_window_errors = 0;
+
+	stats->rx_errors = 0;
+	stats->tx_errors = 0;
+
+	return stats;
+}
+
 static const struct net_device_ops ena_netdev_ops = {
 	.ndo_open		= ena_open,
 	.ndo_stop		= ena_close,
@@ -2187,21 +2009,20 @@ static const struct net_device_ops ena_netdev_ops = {
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= ena_netpoll,
 #endif /* CONFIG_NET_POLL_CONTROLLER */
-#if 0
-	.ndo_rx_flow_steer      = ena_flow_steer,
-#endif
 };
 
 static void ena_device_io_suspend(struct work_struct *work)
 {
 	struct ena_adapter *adapter =
 		container_of(work, struct ena_adapter, suspend_io_task);
-
 	struct net_device *netdev = adapter->netdev;
 
 	/* ena_napi_disable_all disable only the IO handeling.
 	 * We are still subject to AENQ keep alive watchdog.
 	 */
+	u64_stats_update_begin(&adapter->syncp);
+	adapter->dev_stats.io_suspend++;
+	u64_stats_update_begin(&adapter->syncp);
 	ena_napi_disable_all(adapter);
 	netif_tx_lock(netdev);
 	netif_device_detach(netdev);
@@ -2212,9 +2033,12 @@ static void ena_device_io_resume(struct work_struct *work)
 {
 	struct ena_adapter *adapter =
 		container_of(work, struct ena_adapter, resume_io_task);
-
 	struct net_device *netdev = adapter->netdev;
 
+	u64_stats_update_begin(&adapter->syncp);
+	adapter->dev_stats.io_resume++;
+	u64_stats_update_end(&adapter->syncp);
+
 	netif_device_attach(netdev);
 	ena_napi_enable_all(adapter);
 }
@@ -2235,7 +2059,7 @@ static int ena_device_validate_params(struct ena_adapter *adapter,
 	}
 
 	if ((get_feat_ctx->max_queues.max_cq_num < adapter->num_queues) ||
-	    (get_feat_ctx->max_queues.max_cq_num < adapter->num_queues)) {
+	    (get_feat_ctx->max_queues.max_sq_num < adapter->num_queues)) {
 		dev_err(dev, "Error, device doesn't support enough queues\n");
 		return -1;
 	}
@@ -2275,6 +2099,10 @@ static int ena_device_init(struct ena_com_dev *ena_dev, struct pci_dev *pdev,
 	}
 
 	dma_width = ena_com_get_dma_width(ena_dev);
+	if (dma_width < 0) {
+		dev_err(dev, "Invalid dma width value %d", dma_width);
+		goto err_mmio_read_less;
+	}
 
 	rc = pci_set_dma_mask(pdev, DMA_BIT_MASK(dma_width));
 	if (rc) {
@@ -2365,7 +2193,7 @@ static void ena_fw_reset_device(struct work_struct *work)
 	bool dev_up;
 	int rc;
 
-	del_timer_sync(&adapter->watchdog_timer);
+	del_timer_sync(&adapter->timer_service);
 
 	rtnl_lock();
 
@@ -2409,7 +2237,7 @@ static void ena_fw_reset_device(struct work_struct *work)
 
 	rc = ena_device_validate_params(adapter, &get_feat_ctx);
 	if (rc) {
-		dev_err(&pdev->dev, "Can not reserve msix vectors\n");
+		dev_err(&pdev->dev, "Validation of device parameters failed\n");
 		goto err_device_destroy;
 	}
 
@@ -2435,8 +2263,7 @@ static void ena_fw_reset_device(struct work_struct *work)
 		}
 	}
 
-	mod_timer(&adapter->watchdog_timer,
-		  round_jiffies(jiffies + ENA_DEVICE_KALIVE_TIMEOUT));
+	mod_timer(&adapter->timer_service, round_jiffies(jiffies + HZ));
 
 	rtnl_unlock();
 
@@ -2458,26 +2285,68 @@ err:
 		"Reset attempt failed. Can not reset the device\n");
 }
 
-static void ena_watchdog_expire(unsigned long data)
+static void ena_timer_service(unsigned long data)
 {
 	struct ena_adapter *adapter = (struct ena_adapter *)data;
+	unsigned long keep_alive_expired;
 
-	netdev_err(adapter->netdev, "[%s] ERROR!!! WD expired\n",
-		   __func__);
+	/* Check for keep alive expression */
+	keep_alive_expired = round_jiffies(adapter->last_keep_alive_jiffies +
+					   ENA_DEVICE_KALIVE_TIMEOUT);
+	if (unlikely(time_is_before_jiffies(keep_alive_expired))) {
+		netdev_err(adapter->netdev, "[%s] ERROR!!! WD expired\n",
+			   __func__);
 
-	INIT_WORK(&adapter->reset_task, ena_fw_reset_device);
-	schedule_work(&adapter->reset_task);
+		u64_stats_update_begin(&adapter->syncp);
+		adapter->dev_stats.wd_expired++;
+		u64_stats_update_end(&adapter->syncp);
+
+		ena_dmup_stats_to_dmesg(adapter);
+
+		schedule_work(&adapter->reset_task);
+	}
+
+	if (unlikely(!ena_com_get_admin_running_state(adapter->ena_dev))) {
+		netdev_err(adapter->netdev,
+			   "[%s] ENA admin queue is in running state\n",
+			   __func__);
+
+		u64_stats_update_begin(&adapter->syncp);
+		adapter->dev_stats.admin_q_pause++;
+		u64_stats_update_end(&adapter->syncp);
+
+		ena_dmup_stats_to_dmesg(adapter);
+
+		schedule_work(&adapter->reset_task);
+	}
+
+	/* Reset the timer */
+	mod_timer(&adapter->timer_service, jiffies + HZ);
 }
 
 static int ena_calc_io_queue_num(struct pci_dev *pdev,
-				 struct ena_com_dev_get_features_ctx
-				 *get_feat_ctx)
+				 struct ena_com_dev *ena_dev,
+				 struct ena_com_dev_get_features_ctx *get_feat_ctx)
 {
-	int io_queue_num;
+	int io_sq_num, io_queue_num;
+
+	/* In case of LLq use the llq number in the get feature cmd */
+	if (ena_dev->tx_mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {
+		io_sq_num = get_feat_ctx->max_queues.max_llq_num;
+
+		if (io_sq_num == 0) {
+			dev_err(&pdev->dev,
+				"Trying to use LLQ but llq_num is 0. Fall back into regular queues\n");
+
+			ena_dev->tx_mem_queue_type = ENA_ADMIN_PLACEMENT_POLICY_HOST;
+			io_sq_num = get_feat_ctx->max_queues.max_sq_num;
+		}
+	} else {
+		io_sq_num = get_feat_ctx->max_queues.max_sq_num;
+	}
 
 	io_queue_num = min_t(int, num_possible_cpus(), ENA_MAX_NUM_IO_QUEUES);
-	io_queue_num = min_t(int, io_queue_num,
-			     get_feat_ctx->max_queues.max_sq_num);
+	io_queue_num = min_t(int, io_queue_num, io_sq_num);
 	io_queue_num = min_t(int, io_queue_num,
 			     get_feat_ctx->max_queues.max_cq_num);
 	/* 1 IRQ for for mgmnt and 1 IRQs for each IO direction */
@@ -2491,11 +2360,11 @@ static int ena_calc_io_queue_num(struct pci_dev *pdev,
 
 static int ena_set_push_mode(struct ena_com_dev *ena_dev)
 {
-	if ((push_mode == 0) || (push_mode > ENA_MEM_QUEUE_TYPE_MAX_TYPES))
+	if ((push_mode == 0) || (push_mode > ENA_ADMIN_PLACEMENT_POLICY_DEV))
 		return -1;
 
 	ena_dev->tx_mem_queue_type =
-		(enum ena_com_memory_queue_type)push_mode;
+		(enum ena_admin_placement_policy_type)push_mode;
 
 	return 0;
 }
@@ -2561,8 +2430,71 @@ static void ena_set_conf_feat_params(struct ena_adapter *adapter,
 	adapter->max_mtu = feat->dev_attr.max_mtu;
 }
 
-/**
- * ena_probe - Device Initialization Routine
+static int ena_rss_init_default(struct ena_adapter *adapter)
+{
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	u8 key[ENA_HASH_KEY_SIZE];
+	int rc, i;
+	u32 val;
+
+	rc = ena_com_rss_init(ena_dev, ENA_RX_RSS_TABLE_LOG_SIZE);
+	if (unlikely(rc)) {
+		netdev_err(adapter->netdev, "Cannot init indirect table\n");
+		goto err_rss_init;
+	}
+
+	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++) {
+		val = ethtool_rxfh_indir_default(i, adapter->num_queues);
+		rc = ena_com_indirect_table_fill_entry(ena_dev,
+						       ENA_IO_RXQ_IDX(val),
+						       i);
+		if (unlikely(rc && (rc != -EPERM))) {
+			netdev_err(adapter->netdev,
+				   "Cannot fill indirect table\n");
+			goto err_fill_indir;
+		}
+	}
+	rc = ena_com_indirect_table_set(ena_dev);
+	if (unlikely(rc)) {
+		netdev_err(adapter->netdev, "Cannot init indirect table\n");
+		goto err_fill_indir;
+	}
+
+	netdev_rss_key_fill(key, ENA_HASH_KEY_SIZE);
+	rc = ena_com_fill_hash_function(ena_dev, ENA_ADMIN_TOEPLITZ, key,
+					ENA_HASH_KEY_SIZE, 0x0);
+	if (unlikely(rc && (rc != -EPERM))) {
+		netdev_err(adapter->netdev, "Cannot fill hash function\n");
+		goto err_fill_indir;
+	}
+
+	rc = ena_com_set_default_hash_ctrl(ena_dev);
+	if (unlikely(rc && (rc != -EPERM))) {
+		netdev_err(adapter->netdev, "Cannot fill hash control\n");
+		goto err_fill_indir;
+	}
+
+	return 0;
+
+err_fill_indir:
+	ena_com_rss_destroy(ena_dev);
+err_rss_init:
+
+	return rc;
+}
+
+static void ena_release_bars(struct ena_com_dev *ena_dev, struct pci_dev *pdev)
+{
+	int release_bars;
+
+	release_bars = (1 << ENA_REG_BAR);
+	if (ena_dev->tx_mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV)
+		release_bars |= (1 << ENA_MEM_BAR);
+
+	pci_release_selected_regions(pdev, release_bars);
+}
+
+/* ena_probe - Device Initialization Routine
  * @pdev: PCI device information struct
  * @ent: entry in ena_pci_tbl
  *
@@ -2571,7 +2503,7 @@ static void ena_set_conf_feat_params(struct ena_adapter *adapter,
  * ena_probe initializes an adapter identified by a pci_dev structure.
  * The OS initialization, configuring of the adapter private structure,
  * and a hardware reset occur.
- **/
+ */
 static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
 	struct ena_com_dev_get_features_ctx get_feat_ctx;
@@ -2581,7 +2513,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	struct ena_com_dev *ena_dev = NULL;
 	static int adapters_found;
 	int io_queue_num;
-	int rc, i;
+	int rc;
 
 	dev_dbg(&pdev->dev, "%s\n", __func__);
 
@@ -2625,7 +2557,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		goto err_free_ena_dev;
 	}
 
-	if (ena_dev->tx_mem_queue_type != ENA_MEM_QUEUE_TYPE_HOST_MEMORY) {
+	if (ena_dev->tx_mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {
 		ena_dev->mem_bar =
 			ioremap_wc(pci_resource_start(pdev, ENA_MEM_BAR),
 				   pci_resource_len(pdev, ENA_MEM_BAR));
@@ -2634,7 +2566,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 				"failed to remap mem bar %d disable push mode\n",
 				ENA_MEM_BAR);
 			ena_dev->tx_mem_queue_type =
-				ENA_MEM_QUEUE_TYPE_HOST_MEMORY;
+				ENA_ADMIN_PLACEMENT_POLICY_HOST;
 		}
 	}
 
@@ -2649,7 +2581,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		goto err_free_ena_dev;
 	}
 
-	io_queue_num = ena_calc_io_queue_num(pdev, &get_feat_ctx);
+	io_queue_num = ena_calc_io_queue_num(pdev, ena_dev, &get_feat_ctx);
 	dev_info(&pdev->dev, "create %d io queues\n", io_queue_num);
 
 	/* dev zeroed in init_etherdev */
@@ -2689,7 +2621,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	netdev->netdev_ops = &ena_netdev_ops;
 	netdev->watchdog_timeo = TX_TIMEOUT;
-	netdev->ethtool_ops = &ena_ethtool_ops;
+	ena_set_ethtool_ops(netdev);
 
 #if defined(NETIF_F_MQ_TX_LOCK_OPT)
 	netdev->features &= ~NETIF_F_MQ_TX_LOCK_OPT;
@@ -2698,21 +2630,18 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	netdev->priv_flags |= IFF_UNICAST_FLT;
 #endif /* IFF_UNICAST_FLT */
 
-	/* TODO RSS table size should be retrieved from the device */
-	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++)
-		adapter->rss_ind_tbl[i] =
-			ethtool_rxfh_indir_default(i, io_queue_num);
+	init_timer(&adapter->timer_service);
+	adapter->timer_service.expires = round_jiffies(jiffies + HZ);
+	adapter->timer_service.function = ena_timer_service;
+	adapter->timer_service.data = (unsigned long)adapter;
 
-	init_timer(&adapter->watchdog_timer);
-	adapter->watchdog_timer.expires =
-		round_jiffies(jiffies + ENA_DEVICE_KALIVE_TIMEOUT);
-	adapter->watchdog_timer.function = ena_watchdog_expire;
-	adapter->watchdog_timer.data = (unsigned long)adapter;
+	add_timer(&adapter->timer_service);
 
-	add_timer(&adapter->watchdog_timer);
+	u64_stats_init(&adapter->syncp);
 
 	INIT_WORK(&adapter->suspend_io_task, ena_device_io_suspend);
 	INIT_WORK(&adapter->resume_io_task, ena_device_io_resume);
+	INIT_WORK(&adapter->reset_task, ena_fw_reset_device);
 
 	rc = ena_enable_msix_and_set_admin_interrupts(adapter, io_queue_num);
 	if (rc) {
@@ -2727,12 +2656,19 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		goto err_free_msix;
 	}
 
+	rc = ena_rss_init_default(adapter);
+	/* TODO temporarily allow the device to run without RSS (until device will fully support) */
+	if (rc && (rc != -EPERM)) {
+		dev_err(&pdev->dev, "Cannot init RSS rc: %d\n", rc);
+		goto err_terminate_sysfs;
+	}
+
 	memcpy(adapter->netdev->perm_addr, adapter->mac_addr, netdev->addr_len);
 
 	rc = register_netdev(netdev);
 	if (rc) {
 		dev_err(&pdev->dev, "Cannot register net device\n");
-		goto err_terminate_sysfs;
+		goto err_rss;
 	}
 
 	netdev_info(netdev, "%s found at mem %lx, mac addr %pM Queues %d\n",
@@ -2744,13 +2680,16 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	return 0;
 
+err_rss:
+	ena_com_rss_destroy(ena_dev);
 err_terminate_sysfs:
 	ena_sysfs_terminate(&pdev->dev);
 err_free_msix:
-	ena_disable_msix(adapter);
+	ena_com_dev_reset(ena_dev);
 	ena_free_mgmnt_irq(adapter);
+	ena_disable_msix(adapter);
 err_worker_destroy:
-	del_timer(&adapter->watchdog_timer);
+	del_timer(&adapter->timer_service);
 	cancel_work_sync(&adapter->suspend_io_task);
 	cancel_work_sync(&adapter->resume_io_task);
 	free_netdev(netdev);
@@ -2760,7 +2699,7 @@ err_free_ena_dev:
 	pci_set_drvdata(pdev, NULL);
 	devm_kfree(&pdev->dev, ena_dev);
 err_free_region:
-	pci_release_regions(pdev);
+	ena_release_bars(ena_dev, pdev);
 err_disable_device:
 	pci_disable_device(pdev);
 	return rc;
@@ -2804,7 +2743,6 @@ static void ena_remove(struct pci_dev *pdev)
 	struct ena_adapter *adapter = pci_get_drvdata(pdev);
 	struct ena_com_dev *ena_dev;
 	struct net_device *dev;
-	int release_bars;
 
 	if (!adapter)
 		/* This device didn't load properly and it's resources
@@ -2815,17 +2753,14 @@ static void ena_remove(struct pci_dev *pdev)
 	ena_dev = adapter->ena_dev;
 	dev = adapter->netdev;
 
-	release_bars = (1 << ENA_REG_BAR);
-	if (ena_dev->tx_mem_queue_type != ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
-		release_bars |= (1 << ENA_MEM_BAR);
-
-	pci_release_selected_regions(pdev, release_bars);
 
 	unregister_netdev(dev);
 
+	ena_release_bars(ena_dev, pdev);
+
 	ena_sysfs_terminate(&pdev->dev);
 
-	del_timer_sync(&adapter->watchdog_timer);
+	del_timer_sync(&adapter->timer_service);
 
 	cancel_work_sync(&adapter->reset_task);
 
@@ -2847,6 +2782,10 @@ static void ena_remove(struct pci_dev *pdev)
 
 	ena_com_wait_for_abort_completion(ena_dev);
 
+	ena_com_admin_destroy(ena_dev);
+
+	ena_com_rss_destroy(ena_dev);
+
 	pci_set_drvdata(pdev, NULL);
 
 	pci_disable_device(pdev);
@@ -2901,8 +2840,7 @@ static void ena_keep_alive_wd(void *adapter_data,
 {
 	struct ena_adapter *adapter = (struct ena_adapter *)adapter_data;
 
-	mod_timer(&adapter->watchdog_timer,
-		  round_jiffies(jiffies + ENA_DEVICE_KALIVE_TIMEOUT));
+	adapter->last_keep_alive_jiffies = jiffies;
 }
 
 static void ena_notification(void *adapter_data,
@@ -2910,20 +2848,20 @@ static void ena_notification(void *adapter_data,
 {
 	struct ena_adapter *adapter = (struct ena_adapter *)adapter_data;
 
-	ENA_ASSERT(aenq_e->aenq_common_desc.group == ena_admin_notification,
+	ENA_ASSERT(aenq_e->aenq_common_desc.group == ENA_ADMIN_NOTIFICATION,
 		   "Invalid group(%x) expected %x\n",
 		   aenq_e->aenq_common_desc.group,
-		   ena_admin_notification);
+		   ENA_ADMIN_NOTIFICATION);
 
 	switch (aenq_e->aenq_common_desc.syndrom) {
-	case ena_admin_suspend:
+	case ENA_ADMIN_SUSPEND:
 		/* Suspend just the IO queues.
 		 * We deliberately don't suspend admin so the timer and
 		 * the keep_alive events should remain.
 		 */
 		schedule_work(&adapter->suspend_io_task);
 		break;
-	case ena_admin_resume:
+	case ENA_ADMIN_RESUME:
 		schedule_work(&adapter->resume_io_task);
 		break;
 	default:
@@ -2945,9 +2883,9 @@ static void unimplemented_aenq_handler(void *data,
 
 static struct ena_aenq_handlers aenq_handlers = {
 	.handlers = {
-		[ena_admin_link_change] = ena_update_on_link_change,
-		[ena_admin_notification] = ena_notification,
-		[ena_admin_keep_alive] = ena_keep_alive_wd,
+		[ENA_ADMIN_LINK_CHANGE] = ena_update_on_link_change,
+		[ENA_ADMIN_NOTIFICATION] = ena_notification,
+		[ENA_ADMIN_KEEP_ALIVE] = ena_keep_alive_wd,
 	},
 	.unimplemented_handler = unimplemented_aenq_handler
 };
diff --git a/drivers/amazon/ena/ena_netdev.h b/drivers/amazon/ena/ena_netdev.h
index 3af77c9..cb64998 100644
--- a/drivers/amazon/ena/ena_netdev.h
+++ b/drivers/amazon/ena/ena_netdev.h
@@ -42,6 +42,14 @@
 #include "ena_com.h"
 #include "ena_eth_com.h"
 
+#define DRV_MODULE_NAME		"ena"
+#ifndef DRV_MODULE_VERSION
+#define DRV_MODULE_VERSION      "0.2"
+#endif
+#define DRV_MODULE_RELDATE      "OCT 14, 2015"
+
+#define DEVICE_NAME	"Elastic Network Adapter (ENA)"
+
 /* 1 for AENQ + ADMIN */
 #define ENA_MAX_MSIX_VEC(io_queues)	(1 + (io_queues))
 
@@ -64,12 +72,17 @@
  */
 #define ENA_DEFAULT_MIN_RX_BUFF_ALLOC_SIZE 600
 
+#define ENA_MIN_MTU		128
+
 #define ENA_NAME_MAX_LEN	20
 #define ENA_IRQNAME_SIZE	40
 
 #define ENA_PKT_MAX_BUFS	19
 
-#define ENA_RX_THASH_TABLE_SIZE	256
+#define ENA_RX_RSS_TABLE_LOG_SIZE  7
+#define ENA_RX_RSS_TABLE_SIZE	(1 << ENA_RX_RSS_TABLE_LOG_SIZE)
+
+#define ENA_HASH_KEY_SIZE	40
 
 /* The number of tx packet completions that will be handled each napi poll
  * cycle is ring_size / ENA_TX_POLL_BUDGET_DEVIDER.
@@ -99,8 +112,6 @@
  */
 #define ENA_DEVICE_KALIVE_TIMEOUT	(3 * HZ)
 
-#define ENA_RX_RSS_TABLE_SIZE	ENA_RX_THASH_TABLE_SIZE
-
 struct ena_irq {
 	irq_handler_t handler;
 	void *data;
@@ -140,6 +151,31 @@ struct ena_rx_buffer {
 	struct ena_com_buf ena_buf;
 } ____cacheline_aligned;
 
+struct ena_stats_tx {
+	u64 cnt;
+	u64 bytes;
+	u64 queue_stop;
+	u64 prepare_ctx_err;
+	u64 queue_wakeup;
+	u64 dma_mapping_err;
+	u64 unsupported_desc_num;
+	u64 napi_comp;
+	u64 tx_poll;
+	u64 doorbells;
+};
+
+struct ena_stats_rx {
+	u64 cnt;
+	u64 bytes;
+	u64 refil_partial;
+	u64 bad_csum;
+	u64 page_alloc_fail;
+	u64 skb_alloc_fail;
+	u64 dma_mapping_err;
+	u64 bad_desc_num;
+	u64 small_copy_len_pkt;
+};
+
 struct ena_ring {
 	/* Holds the empty requests for TX out of order completions */
 	u16 *free_tx_ids;
@@ -153,6 +189,7 @@ struct ena_ring {
 	struct pci_dev *pdev;
 	struct napi_struct *napi;
 	struct net_device *netdev;
+	struct ena_adapter *adapter;
 	struct ena_com_io_cq *ena_com_io_cq;
 	struct ena_com_io_sq *ena_com_io_sq;
 
@@ -164,15 +201,27 @@ struct ena_ring {
 
 	int ring_size; /* number of tx/rx_buffer_info's entries */
 
-	/* Count how many times the driver wasn't able to allocate new pages */
-	u32 alloc_fail_cnt;
-	u32 bad_checksum;
+	enum ena_admin_placement_policy_type tx_mem_queue_type;
 
-	enum ena_com_memory_queue_type tx_mem_queue_type;
+	struct ena_com_rx_buf_info ena_bufs[ENA_PKT_MAX_BUFS];
 
-	struct ena_com_buf ena_bufs[ENA_PKT_MAX_BUFS];
+	struct u64_stats_sync syncp;
+	union {
+		struct ena_stats_tx tx_stats;
+		struct ena_stats_rx rx_stats;
+	};
 } ____cacheline_aligned;
 
+struct ena_stats_dev {
+	u64 tx_timeout;
+	u64 io_suspend;
+	u64 io_resume;
+	u64 wd_expired;
+	u64 interface_up;
+	u64 interface_down;
+	u64 admin_q_pause;
+};
+
 /* adapter specific private data structure */
 struct ena_adapter {
 	struct ena_com_dev *ena_dev;
@@ -199,9 +248,6 @@ struct ena_adapter {
 	u32 tx_ring_size;
 	u32 rx_ring_size;
 
-	/* RSS*/
-	u8 rss_ind_tbl[ENA_RX_RSS_TABLE_SIZE];
-
 	u32 msg_enable;
 
 	u8 mac_addr[ETH_ALEN];
@@ -223,11 +269,20 @@ struct ena_adapter {
 
 	struct ena_irq irq_tbl[ENA_MAX_MSIX_VEC(ENA_MAX_NUM_IO_QUEUES)];
 
-	/* watchdog timer */
+	/* timer service */
 	struct work_struct reset_task;
 	struct work_struct suspend_io_task;
 	struct work_struct resume_io_task;
-	struct timer_list watchdog_timer;
+	struct timer_list timer_service;
+
+	unsigned long last_keep_alive_jiffies;
+
+	struct u64_stats_sync syncp;
+	struct ena_stats_dev dev_stats;
 };
 
+void ena_set_ethtool_ops(struct net_device *netdev);
+
+void ena_dmup_stats_to_dmesg(struct ena_adapter *adapter);
+
 #endif /* !(ENA_H) */
diff --git a/drivers/amazon/ena/ena_regs_defs.h b/drivers/amazon/ena/ena_regs_defs.h
index febade7..26fe6ef8 100644
--- a/drivers/amazon/ena/ena_regs_defs.h
+++ b/drivers/amazon/ena/ena_regs_defs.h
@@ -1,46 +1,41 @@
-/******************************************************************************
-Copyright (C) 2015 Annapurna Labs Ltd.
-
-This file may be licensed under the terms of the Annapurna Labs Commercial
-License Agreement.
-
-Alternatively, this file can be distributed under the terms of the GNU General
-Public License V2 as published by the Free Software Foundation and can be
-found at http://www.gnu.org/licenses/gpl-2.0.html
-
-Alternatively, redistribution and use in source and binary forms, with or
-without modification, are permitted provided that the following conditions are
-met:
-
-    *  Redistributions of source code must retain the above copyright notice,
-this list of conditions and the following disclaimer.
-
-    *  Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer in
-the documentation and/or other materials provided with the
-distribution.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
-ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
-(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
-LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
-ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-******************************************************************************/
-
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
 #ifndef _ENA_REGS_H_
 #define _ENA_REGS_H_
 
 /* ENA Global Registers, for the entire PCIe function */
 struct ena_regs_ena_registers {
 	/* word 0 : */
-	/*
-	 * ENA specification version
+	/* ENA specification version [RO]
 	 * 7:0 : minor_version - Minor version
 	 * 15:8 : major_version - Major version.
 	 * 31:16 : reserved16
@@ -48,8 +43,7 @@ struct ena_regs_ena_registers {
 	u32 version;
 
 	/* word 1 : */
-	/*
-	 * ENA controller version
+	/* ENA controller version [RO]
 	 * 7:0 : subminor_version - Sub Minor version
 	 * 15:8 : minor_version - Minor version
 	 * 23:16 : major_version - Major version.
@@ -58,8 +52,7 @@ struct ena_regs_ena_registers {
 	u32 controller_version;
 
 	/* word 2 : */
-	/*
-	 * capabilities register
+	/* capabilities register [RO]
 	 * 0 : contiguous_queue_required - If set, requires
 	 *    that each queue ring occupies a contiguous
 	 *    physical memory space.
@@ -74,18 +67,17 @@ struct ena_regs_ena_registers {
 	 */
 	u32 caps;
 
-	/* word 3 : capabilities extended register */
+	/* word 3 : capabilities extended register [RO] */
 	u32 caps_ext;
 
-	/* word 4 : admin queue base address bits [31:0] */
+	/* word 4 : admin queue base address bits [31:0] [WO] */
 	u32 aq_base_lo;
 
-	/* word 5 : admin queue base address bits [63:32] */
+	/* word 5 : admin queue base address bits [63:32] [WO] */
 	u32 aq_base_hi;
 
 	/* word 6 : */
-	/*
-	 * admin queue capabilities register
+	/* admin queue capabilities register [WO]
 	 * 15:0 : aq_depth - admin queue depth in entries
 	 * 31:16 : aq_entry_size - admin queue entry size in
 	 *    32-bit words
@@ -95,15 +87,14 @@ struct ena_regs_ena_registers {
 	/* word 7 :  */
 	u32 reserved;
 
-	/* word 8 : admin completion queue base address bits [31:0]. */
+	/* word 8 : admin completion queue base address bits [31:0]. [WO] */
 	u32 acq_base_lo;
 
-	/* word 9 : admin completion queue base address bits [63:32]. */
+	/* word 9 : admin completion queue base address bits [63:32]. [WO] */
 	u32 acq_base_hi;
 
 	/* word 10 : */
-	/*
-	 * admin completion queue capabilities register
+	/* admin completion queue capabilities register [WO]
 	 * 15:0 : acq_depth - admin completion queue depth in
 	 *    entries
 	 * 31:16 : acq_entry_size - admin completion queue
@@ -111,61 +102,54 @@ struct ena_regs_ena_registers {
 	 */
 	u32 acq_caps;
 
-	/* word 11 : AQ Doorbell */
+	/* word 11 : AQ Doorbell [WO] */
 	u32 aq_db;
 
-	/*
-	 * word 12 : ACQ tail pointer, indicates where new completions will
-	 * be placed
+	/* word 12 : ACQ tail pointer, indicates where new completions will
+	 * be placed [RO]
 	 */
 	u32 acq_tail;
 
 	/* word 13 : */
-	/*
-	 * Asynchronous Event Notification Queue capabilities register
+	/* Asynchronous Event Notification Queue capabilities register [WO]
 	 * 15:0 : aenq_depth - queue depth in entries
 	 * 31:16 : aenq_entry_size - queue entry size in
 	 *    32-bit words
 	 */
 	u32 aenq_caps;
 
-	/*
-	 * word 14 : Asynchronous Event Notification Queue base address
-	 * bits [31:0]
+	/* word 14 : Asynchronous Event Notification Queue base address
+	 * bits [31:0] [WO]
 	 */
 	u32 aenq_base_lo;
 
-	/*
-	 * word 15 : Asynchronous Event Notification Queue base address
-	 * bits [63:32]
+	/* word 15 : Asynchronous Event Notification Queue base address
+	 * bits [63:32] [WO]
 	 */
 	u32 aenq_base_hi;
 
-	/*
-	 * word 16 : AENQ Head Doorbell, indicates the entries that have
-	 * been processed by the host
+	/* word 16 : AENQ Head Doorbell, indicates the entries that have
+	 * been processed by the host [WO]
 	 */
 	u32 aenq_head_db;
 
-	/*
-	 * word 17 : AENQ tail pointer, indicates where new entries will be
-	 * placed
+	/* word 17 : AENQ tail pointer, indicates where new entries will be
+	 * placed [RO]
 	 */
 	u32 aenq_tail;
 
-	/* word 18 :  */
+	/* word 18 :  [RO] */
 	u32 intr_cause;
 
-	/* word 19 :  */
+	/* word 19 :  [RW] */
 	u32 intr_mask;
 
-	/* word 20 :  */
+	/* word 20 :  [WO] */
 	u32 intr_clear;
 
 	/* word 21 : */
-	/*
-	 * Device Control Register, some of these features may not be
-	 *    implemented or supported for a given client
+	/* Device Control Register, some of these features may not be
+	 *    implemented or supported for a given client [WO]
 	 * 0 : dev_reset - If set, indicates request for a
 	 *    reset, this bit will only be cleared when the
 	 *    reset operation finished and can not be cleared by
@@ -185,8 +169,7 @@ struct ena_regs_ena_registers {
 	u32 dev_ctl;
 
 	/* word 22 : */
-	/*
-	 * Device Status Register
+	/* Device Status Register [RO]
 	 * 0 : ready - device ready to received admin commands
 	 * 1 : aq_restart_in_progress - this bit is set while
 	 *    aq_restart in process
@@ -210,26 +193,23 @@ struct ena_regs_ena_registers {
 	u32 dev_sts;
 
 	/* word 23 : */
-	/*
-	 * MMIO Read Less Register
+	/* MMIO Read Less Register. host must initialize the mmio_resp_lo/hi
+	 *    before issueing new register read request [WO]
 	 * 15:0 : req_id - request id
 	 * 31:16 : reg_off - register offset
 	 */
-	u32 mmio_read;
+	u32 mmio_reg_read;
 
-	/* word 24 : read response will be sent to this address. bits [31:0] */
+	/* word 24 : read response address bits [31:0] [WO] */
 	u32 mmio_resp_lo;
 
-	/*
-	 * word 25 : read response will be sent to this address. bits
-	 * [63:32]
-	 */
+	/* word 25 : read response address bits [64:32] [WO] */
 	u32 mmio_resp_hi;
 };
 
 /* admin interrupt register */
 #define ENA_REGS_ADMIN_INTERRUPT_ACQ	0x1 /* Admin Completion queue */
-#define ENA_REGS_ADMIN_INTERRUPT_AENQ	0x2 /* Asynchronous Event Notification Queue */
+#define ENA_REGS_ADMIN_INTERRUPT_AENQ	0x2 /* Async Event Notification Queue */
 
 /* ena_registers offsets */
 #define ENA_REGS_VERSION_OFF		0x0
@@ -254,7 +234,7 @@ struct ena_regs_ena_registers {
 #define ENA_REGS_INTR_CLEAR_OFF		0x50
 #define ENA_REGS_DEV_CTL_OFF		0x54
 #define ENA_REGS_DEV_STS_OFF		0x58
-#define ENA_REGS_MMIO_READ_OFF		0x5c
+#define ENA_REGS_MMIO_REG_READ_OFF		0x5c
 #define ENA_REGS_MMIO_RESP_LO_OFF		0x60
 #define ENA_REGS_MMIO_RESP_HI_OFF		0x64
 
@@ -320,9 +300,9 @@ struct ena_regs_ena_registers {
 #define ENA_REGS_DEV_STS_QUIESCENT_STATE_ACHIEVED_SHIFT		7
 #define ENA_REGS_DEV_STS_QUIESCENT_STATE_ACHIEVED_MASK		0x80
 
-/* mmio_read register */
-#define ENA_REGS_MMIO_READ_REQ_ID_MASK		0xffff
-#define ENA_REGS_MMIO_READ_REG_OFF_SHIFT		16
-#define ENA_REGS_MMIO_READ_REG_OFF_MASK		0xffff0000
+/* mmio_reg_read register */
+#define ENA_REGS_MMIO_REG_READ_REQ_ID_MASK		0xffff
+#define ENA_REGS_MMIO_REG_READ_REG_OFF_SHIFT		16
+#define ENA_REGS_MMIO_REG_READ_REG_OFF_MASK		0xffff0000
 
 #endif /*_ENA_REGS_H_ */
diff --git a/drivers/amazon/ena/ena_sysfs.c b/drivers/amazon/ena/ena_sysfs.c
index bb4abbf..cc3ddbd 100644
--- a/drivers/amazon/ena/ena_sysfs.c
+++ b/drivers/amazon/ena/ena_sysfs.c
@@ -98,18 +98,15 @@ static struct device_attribute dev_attr_small_copy_len = {
  *****************************************************************************/
 int ena_sysfs_init(struct device *dev)
 {
-	int status = 0;
-
 	if (device_create_file(dev, &dev_attr_small_copy_len))
 		dev_info(dev, "failed to create small_copy_len sysfs entry");
 
-	return status;
+	return 0;
 }
 
 /******************************************************************************
  *****************************************************************************/
-void ena_sysfs_terminate(
-	struct device *dev)
+void ena_sysfs_terminate(struct device *dev)
 {
 	device_remove_file(dev, &dev_attr_small_copy_len);
 }
diff --git a/drivers/amazon/ena/ena_sysfs.h b/drivers/amazon/ena/ena_sysfs.h
index 1ac83bd..dc0d4c9 100644
--- a/drivers/amazon/ena/ena_sysfs.h
+++ b/drivers/amazon/ena/ena_sysfs.h
@@ -53,4 +53,3 @@ static inline void ena_sysfs_terminate(struct device *dev)
 #endif /* CONFIG_SYSFS */
 
 #endif /* __ENA_SYSFS_H__ */
-
-- 
2.7.4

