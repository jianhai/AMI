From 6bf8f4f0d204bac18a649b1271a26bda2923504c Mon Sep 17 00:00:00 2001
From: Munehisa Kamata <kamatam@amazon.com>
Date: Wed, 2 Dec 2015 19:24:00 +0000
Subject: ena: import Elastic Network Adapter (ENA) driver

Signed-off-by: Munehisa Kamata <kamatam@amazon.com>
Reviewed-by: Netanel Belgazal <netanel@annapurnalabs.com>
Reviewed-by: Rashika Kheria <rashika@amazon.de>
Reviewed-by: Matt Nierzwicki <nierzwic@amazon.com>

CR: https://cr.amazon.com/r/5132188/
---
 drivers/amazon/Kconfig               |   11 +-
 drivers/amazon/Makefile              |    1 +
 drivers/amazon/ena/Makefile          |    2 +
 drivers/amazon/ena/README            |  281 ++++
 drivers/amazon/ena/ena_admin_defs.h  | 1475 +++++++++++++++++
 drivers/amazon/ena/ena_com.c         | 1659 +++++++++++++++++++
 drivers/amazon/ena/ena_com.h         |  364 +++++
 drivers/amazon/ena/ena_common_defs.h |   52 +
 drivers/amazon/ena/ena_eth_com.h     |  576 +++++++
 drivers/amazon/ena/ena_eth_io_defs.h |  512 ++++++
 drivers/amazon/ena/ena_gen_info.h    |   37 +
 drivers/amazon/ena/ena_includes.h    |    4 +
 drivers/amazon/ena/ena_netdev.c      | 2956 ++++++++++++++++++++++++++++++++++
 drivers/amazon/ena/ena_netdev.h      |  233 +++
 drivers/amazon/ena/ena_pci_id_tbl.h  |   77 +
 drivers/amazon/ena/ena_regs_defs.h   |  328 ++++
 drivers/amazon/ena/ena_sysfs.c       |  115 ++
 drivers/amazon/ena/ena_sysfs.h       |   56 +
 18 files changed, 8738 insertions(+), 1 deletion(-)
 create mode 100644 drivers/amazon/ena/Makefile
 create mode 100644 drivers/amazon/ena/README
 create mode 100644 drivers/amazon/ena/ena_admin_defs.h
 create mode 100644 drivers/amazon/ena/ena_com.c
 create mode 100644 drivers/amazon/ena/ena_com.h
 create mode 100644 drivers/amazon/ena/ena_common_defs.h
 create mode 100644 drivers/amazon/ena/ena_eth_com.h
 create mode 100644 drivers/amazon/ena/ena_eth_io_defs.h
 create mode 100644 drivers/amazon/ena/ena_gen_info.h
 create mode 100644 drivers/amazon/ena/ena_includes.h
 create mode 100644 drivers/amazon/ena/ena_netdev.c
 create mode 100644 drivers/amazon/ena/ena_netdev.h
 create mode 100644 drivers/amazon/ena/ena_pci_id_tbl.h
 create mode 100644 drivers/amazon/ena/ena_regs_defs.h
 create mode 100644 drivers/amazon/ena/ena_sysfs.c
 create mode 100644 drivers/amazon/ena/ena_sysfs.h

diff --git a/drivers/amazon/Kconfig b/drivers/amazon/Kconfig
index 2e164b6..c563a22 100644
--- a/drivers/amazon/Kconfig
+++ b/drivers/amazon/Kconfig
@@ -1,5 +1,5 @@
 #
-# Intel network device configuration
+# Amazon network device configuration
 #
 
 config VENDOR_AMAZON
@@ -34,4 +34,13 @@ config AMAZON_IXGBEVF
 	  will be called ixgbevf.  MSI-X interrupt support is required
 	  for this driver to work correctly.
 
+config AMAZON_ENA
+	tristate "Elastic Network Adapter (ENA) support"
+	depends on PCI_MSI
+	---help---
+	  This driver supports Elastic Network Adapter (ENA)
+	  
+	  To compile this driver as a module, choose M here.
+	  The module will be called ena.
+
 endif # VENDOR_AMAZON
diff --git a/drivers/amazon/Makefile b/drivers/amazon/Makefile
index 362be4b..bfa9ca0 100644
--- a/drivers/amazon/Makefile
+++ b/drivers/amazon/Makefile
@@ -1,5 +1,6 @@
 # Amazon Driver Updates
 
 obj-$(CONFIG_AMAZON_IXGBEVF)	+= ixgbevf/
+obj-$(CONFIG_AMAZON_ENA)	+= ena/
 
 ########################
diff --git a/drivers/amazon/ena/Makefile b/drivers/amazon/ena/Makefile
new file mode 100644
index 0000000..5ce8892
--- /dev/null
+++ b/drivers/amazon/ena/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_AMAZON_ENA) += ena.o
+ena-objs := ena_com.o ena_netdev.o ena_sysfs.o
diff --git a/drivers/amazon/ena/README b/drivers/amazon/ena/README
new file mode 100644
index 0000000..78a2b20
--- /dev/null
+++ b/drivers/amazon/ena/README
@@ -0,0 +1,281 @@
+Linux kernel driver for Elastic Network Adapter (ENA)
+This network driver is speed-independent, providing a high-performance,
+low-latency network interface for virtual machines and bare metal environment.
+ENA negotiates supported features with the device, to enable future upgrades and
+various systems with a single driver.
+
+Files Structure:
+===============
+ena_com/ena_defs - Holds the ENA descriptors and ENA register structures.
+ena_com - Communication layer. This layer is responsible for handling all
+communications between the ENA device and the driver.
+ena_netdev.c/.h - Main driver.
+
+Arch:
+====
+This driver implements a standard Linux Ethernet driver. The kernel communicates
+with the driver using the net_device_ops (defined at include/linux/netdevice.h).
+the driver uses the PCI interface for probing the adapter and various other
+management functions.
+
+Management Interface:
+====================
+All management of the ENA device is performed by 2 queues.
+The Admin queue (AQ) and the asynchronous event notification queue (AENQ).
+The admin queue has the following commands:
+* Create IO submission queue.
+* Create IO completion queue.
+* Destroy IO submission queue.
+* Destroy IO completion queue.
+* Suspend IO submission queue.
+* Flush IO submission queue.
+* Get feature.
+* Set feature.
+* Configure AENQ.
+* Get statistics.
+
+The create and and destroy commands are fully implemented. The get/set feature
+is partially implemented and the rest of the commands will be implemented in a
+later version.
+To examine all the properties of the set/get feature command, refer to
+ena_com/ena_defs/ena_admin_defs.h.
+
+AENQ is a single-side queue (from the ENA device to the host driver), which
+sends events that cannot be reported using the Admin Completion Queue (ACQ).
+Each event is divided into group and syndrom.
+The events are:
+	Group		Syndrom                         Notes
+	Link change	- X -
+	Fatal error	- X -
+	Warning		Thermal threshold		PF only
+	Warning		Logging fifo overflow		PF only
+	Warning		Dirty page logging		PF only
+	Warning		Malicious mmio access		PF only
+	Warning 	CQ full
+	notification	suspend traffic
+	notification	resume traffic
+	Keep Alive	- X -
+
+Currently the driver implements handlers for:
+link change, suspend/resume trafic, keep alive tracking and fatal error.
+
+Queue Operating Modes:
+======================
+
+The driver supports 2 Queue Operation modes for Transmit (Tx) queues:
+* Low Latency Queue (LLQ) or push-mode.
+ ** In this mode the driver pushes the transmit descriptors and the first 128
+bytes of the packet directly to the ENA device memory space. The rest of the
+packet payload is fetched by the device. For this operation mode the host use
+a dedicated PCIe device memory BAR that is non-cacheable but with write-combine
+enabled.
+
+* Regular mode
+** In this mode the ENA device fetches the ENA Tx descriptors and the packet
+data from host memory.
+
+For RX queues the ENA device supports only the regular mode.
+
+***Note: Not all the ENA devices support LLQ, and this feature is negotiated
+with the device upon initialization.
+
+If the ENA device does not support LLQ mode, the driver will fall back to
+regular mode.
+
+Interrupt Modes:
+================
+The ENA device is built with modern muti-core and parallel processing in mind,
+thus it has an interrupt controller that can generate an MSI-X interrupt per
+queue.
+
+The driver assigns a single MSI-X vector per queue pair (for both the Tx and the
+Rx directions). In addition, the driver assigns another dedicated MSI-X
+vector for management (for admin completions and asynchronous events
+notification queues).
+
+Management interrupt registration is performed when the Linux kernel probes the
+adapter, and it is de-registered when the adapter is removed.
+The I/O queues interrupts registeration is performed when the linux interface
+of the adapter is opened, and it is de-registered when
+the interface is closed. The systems interrupts status can be viewed in the
+/proc/interrupts pseudo file.
+
+The registered interrupt name is:
+ena-mgmnt@pci:<pci device name of the adapter>
+and for each queue, an interrupt is registered with the following name:
+<interface name><queue index>.
+
+Memory Allocations:
+==================
+DMA Coherent buffers for the following DMA rings:
+- Tx submission ring (For regular mode; for LLQ mode it is allocated using
+kzalloc)
+- Tx completion ring.
+- Rx submission ring.
+- Rx completion ring.
+- Admin submition ring.
+- Admin completion ring.
+- AENQ ring.
+- MMIO Readless mechanisem buffer.
+
+The ENA device admin, aenq and the mmio read and buffers are allocated on
+probe and freed on termination.
+
+Regular allocations:
+- Tx buffers info ring.
+- Tx free indexes ring.
+- Rx buffers info ring.
+- MSI-X table.
+- ENA device.
+
+Tx/Rx buffers and the MSI-X table are allocated on Open and freed on Close.
+
+Rx buffer allocation modes:
+
+The driver supports two allocation modes:
+1. Frag allocation (default): Buffer is allocated using netdev_alloc_frag().
+2. Page allocation: Buffer is allocated using alloc_page().
+
+Rx buffers allocation in both modes is performed as follows:
+1. When enabling an interface -- open().
+2. Once per Rx poll for all the frames received and not copied to the new
+   allocated SKB (len < small_packet_len).
+
+These buffers are freed on close().
+
+The small_packet_len is initialized by default to ENA_DEFAULT_SMALL_PACKET_LEN
+and can be configured
+by the sysfs pseudo file
+/sys/bus/pci/devices/<pci device name of the adapter>/small_packet_len.
+
+SKB:
+The driver-allocated SKB for frames received from Rx handling using
+NAPI context. The allocation method depends on size of the packet.
+If the frame length is larger than small_packet_len,
+build_skb() is used, otherwise netdev_alloc_skb_ip_align() is
+used, the buffer content is copied (by CPU) to the skb and the buffer
+is recycled.
+
+MULTIQUEUE:
+===========
+The driver supports multiqueue mode for both Tx and Rx.
+This mode has various benefits when queues are allocated to different CPU
+cores/threads.
+1. Reduced CPU/thread/process contention on a given Ethernet port, when
+transmitting a packet.
+2. Cache miss rate on transmit completion is reduced, particularly for data
+cache lines
+   that hold the sk_buff structures.
+3. Increased process-level parallelism when handling received packets.
+4. Increased data cache hit rate by steering kernel processing of packets to the
+CPU, where the application thread consuming the packet is running.
+5. In hardware interrupt re-direction.
+
+RSS:
+For TCP/UDP packets the ENA device calculates the 4-tuple and steers the packet
+according to this value (module #queue). Other packets are steered to queue 0.
+
+VFs steering:
+To steer between VFs, the ENA device uses the least significant byte of the
+destination MAC address to determain the VF index. ARP packets are passed to PF.
+This logic will be fixed in the next releases.
+
+Interrupts Affinity:
+-------------------
+To utilize the multiqueue benefits, the user must set the interrupts affinity of
+each of the queue's pair.  The general recommendation is to have the interrupt
+of Tx and Rx queues N routed to core N.
+
+DATA PATH:
+==========
+Tx:
+---
+end_start_xmit() is called by the stack. This function does the following:
+- map data buffers (skb->data and frags).
+- Populate ena_buf for the push buffer (if the driver works in push mode.)
+- Prepare ENA bufs for the remaining frags.
+- Allocate a new request ID from the empty req_id ring (The request ID is the
+index of the packet in the Tx ring. This is used for OOO TX completions.)
+- Add the packet to the proper place in the Tx ring.
+- Call ena_prepare_tx(), an ENA communication layer that converts the ena_bufs
+  to ENA descriptors (and adds meta ENA descriptors when needed.)
+  This function also copies the ENA descriptors and the push buffer to the
+device
+  memory space (depending on the working mode.)
+- Write doorbell to ENA device.
+- When the ENA device finishes sending the packet, a completion interrupt is
+raised.
+- The interrupt handler schedules NAPI.
+- The ena_clean_tx_irq function is called. This function handles the completion
+descriptors generated by the ENA,
+  with a single completion descriptor per completed packet.
+Retrives the req_id from the completion descriptor. Gets the tx_info of the
+packet, according to the req_id. Unmaps the data buffers and returns req_id
+to the empty requester-id ring.
+  The function stops when the completion descriptors are completed or the
+budget is reached.
+
+Rx:
+---
+- When a packet is received by the ENA device, it is written to the host
+memory, where the previous ENA RX buffer was allocated. The driver makes sure
+to set the INT bit in each of the ENA Rx descriptors, so an interrupt will be
+triggered (if unmasked )when a new packet is written to that descriptor.
+- The interrupt handler schedules NAPI.
+- The aena_clean_rx_irq function is called. This functions calls aena_rx_pkt(),
+an ENA communication layer function, which returns the number of descriptors
+used for a new unhandled packet, and zero if no new packet is found.
+Then it calls the ena_clean_rx_irq() function.
+- ena_eth_rx_skb checks packet length:
+  If the packet is too small (len less than small_packet_len), the driver
+allocates an SKB structure for the new packet, and copies the packet payload
+into the SKB data buffer.
+In this way the original data buffer is not passed to the stack and is reused
+for the next Rx packets.
+Otherwise the function unmaps the Rx buffer, then allocates he new SKB
+structure and hooks the Rx buffer to the SKB frags.
+  (*) Copying the packet payload into the SKB data buffer for short packets is
+a common optimization in Linux network drivers.
+This method saves allocating and mapping large buffers.
+  (**) Allocating SKB on packet reception is also a common method, which has the
+following benefits:
+  (i) SKB is used and accessed immediately after allocation.
+This reduces possible cache misses.
+  (ii) The number of 'inuse' sk_buff can be reduced to the very minimum,
+especially when packets are dropped by the stack.
+- The new SKB is updated with the necessary information (protocol, checksum hw
+calc result, etc), and then passed to the network stack, using the NAPI interface
+function napi_gro_receive().
+
+Note:
+The Rx and Tx queues share the same interrut so for each interrupt issued by
+the device both ena_clean_rx_irq and ena_clean_tx_irq will be called.
+
+MMIO Readless mechanisem:
+There are two new WO registers in the MMIO Regs.
+Register Index - index of the register that should be read + coockie
+MMIO Response Addr  (High/Low) address in Device Driver memory where the
+response should be written by the device.
+Whenever the driver wants to issue a read he writes the register offset
+and a coockie to the register index register.
+The device will write back the data to the MMIO response reg together
+with the cokie.
+
+Keep Alive mechanisem:
+The device sends to the driver a Keep alive event every 1 sec.
+When the driver doesn't receive the keep alive event for a long
+period he will try to reset the ena device.
+
+
+Missing features:
+================
+ - Rx queue select
+ - Ethtool for rxnfs (RFS)
+ - CSUM for IPv6
+ - counters for failed - sync with sysfs
+ - adaptive coalescing
+ - update completion head
+ - tx_pkt_watchdog proper handeling
+ - reset SQ in failure
+ - multicast
+ - dynamic ring size
diff --git a/drivers/amazon/ena/ena_admin_defs.h b/drivers/amazon/ena/ena_admin_defs.h
new file mode 100644
index 0000000..5fb67c9
--- /dev/null
+++ b/drivers/amazon/ena/ena_admin_defs.h
@@ -0,0 +1,1475 @@
+/******************************************************************************
+Copyright (C) 2015 Annapurna Labs Ltd.
+
+This file may be licensed under the terms of the Annapurna Labs Commercial
+License Agreement.
+
+Alternatively, this file can be distributed under the terms of the GNU General
+Public License V2 as published by the Free Software Foundation and can be
+found at http://www.gnu.org/licenses/gpl-2.0.html
+
+Alternatively, redistribution and use in source and binary forms, with or
+without modification, are permitted provided that the following conditions are
+met:
+
+    *  Redistributions of source code must retain the above copyright notice,
+this list of conditions and the following disclaimer.
+
+    *  Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+******************************************************************************/
+
+#ifndef _ENA_ADMIN_H_
+#define _ENA_ADMIN_H_
+
+/* admin commands opcodes */
+enum ena_admin_aq_opcode {
+	/* create submission queue */
+	ena_admin_create_sq = 1,
+
+	/* destroy submission queue */
+	ena_admin_destroy_sq = 2,
+
+	/* create completion queue */
+	ena_admin_create_cq = 3,
+
+	/* destroy completion queue */
+	ena_admin_destroy_cq = 4,
+
+	/* suspend submission queue */
+	ena_admin_suspend_sq = 5,
+
+	/* resume submission queue */
+	ena_admin_resume_sq = 6,
+
+	/* flush submission queue */
+	ena_admin_flush_sq = 7,
+
+	/* get capabilities of particular feature */
+	ena_admin_get_feature = 8,
+
+	/* get capabilities of particular feature */
+	ena_admin_set_feature = 9,
+
+	/* enabling events in AENQ */
+	ena_admin_async_event_request = 10,
+
+	/* get statistics */
+	ena_admin_get_stats = 11,
+};
+
+/* privileged amdin commands opcodes */
+enum ena_admin_aq_opcode_privileged {
+	/* get device capabilities */
+	ena_admin_identify = 48,
+
+	/* configure device */
+	ena_admin_configure_pf_device = 49,
+
+	/* setup SRIOV PCIe Virtual Function capabilities */
+	ena_admin_setup_vf = 50,
+
+	/* load firmware to the controller */
+	ena_admin_load_firmware = 52,
+
+	/* commit previously loaded firmare */
+	ena_admin_commit_firmware = 53,
+
+	/* quiesce virtual function */
+	ena_admin_quiesce_vf = 54,
+
+	/* load virtual function from migrates context */
+	ena_admin_migrate_vf = 55,
+};
+
+/* admin command completion status codes */
+enum ena_admin_aq_completion_status {
+	/* Request completed successfully */
+	ena_admin_success = 0,
+
+	/* no resources to satisfy request */
+	ena_admin_resource_allocation_failure = 1,
+
+	/* Bad opcode in request descriptor */
+	ena_admin_bad_opcode = 2,
+
+	/* Unsupported opcode in request descriptor */
+	ena_admin_unsupported_opcode = 3,
+
+	/* Wrong request format */
+	ena_admin_malformed_request = 4,
+
+	/*
+	 * One of parameters is not valid. Provided in ACQ entry
+	 * extended_status
+	 */
+	ena_admin_illegal_parameter = 5,
+
+	/* unexpected error */
+	ena_admin_unknown_error = 6,
+};
+
+/* get/set feature subcommands opcodes */
+enum ena_admin_aq_feature_id {
+	/* list of all supported attributes/capabilities in the ENA */
+	ena_admin_device_attributes = 1,
+
+	/* max number of supported queues per for every queues type */
+	ena_admin_max_queues_num = 2,
+
+	/* low latency queues capabilities (max entry size, depth) */
+	ena_admin_llq_config = 3,
+
+	/* power management capabilities */
+	ena_admin_power_management_config = 4,
+
+	/* MAC address filters support, multicast, broadcast, and promiscous */
+	ena_admin_mac_filters_config = 5,
+
+	/* VLAN membership, frame format, etc.  */
+	ena_admin_vlan_config = 6,
+
+	/*
+	 * Available size for various on-chip memory resources, accessible
+	 * by the driver
+	 */
+	ena_admin_on_device_memory_config = 7,
+
+	/* L2 bridging capabilities inside ENA */
+	ena_admin_l2_bridg_config = 8,
+
+	/* L3 routing capabilities inside ENA */
+	ena_admin_l3_router_config = 9,
+
+	/* Receive Side Scaling (RSS) function */
+	ena_admin_rss_hash_function = 10,
+
+	/* stateless TCP/UDP/IP offload capabilities. */
+	ena_admin_stateless_offload_config = 11,
+
+	/* Multiple tuples flow table configuration */
+	ena_admin_rss_redirection_table_config = 12,
+
+	/* Data center bridging (DCB) capabilities */
+	ena_admin_dcb_config = 13,
+
+	/* max MTU, current MTU */
+	ena_admin_mtu = 14,
+
+	/*
+	 * Virtual memory address translation capabilities for userland
+	 * queues
+	 */
+	ena_admin_va_translation_config = 15,
+
+	/* traffic class capabilities */
+	ena_admin_tc_config = 16,
+
+	/* traffic class capabilities */
+	ena_admin_encryption_config = 17,
+
+	/* Receive Side Scaling (RSS) hash input */
+	ena_admin_rss_hash_input = 18,
+
+	/* overlay tunnels configuration */
+	ena_admin_tunnel_config = 19,
+
+	/* interrupt moderation: count,interval,adaptive */
+	ena_admin_interrupt_moderation = 20,
+
+	/* 1588v2 and Timing configuration */
+	ena_admin_1588_config = 21,
+
+	/* End-to-End invariant CRC configuration */
+	ena_admin_e2e_crc_config = 22,
+
+	/*
+	 * Packet Header format templates configuration for input and
+	 * output parsers
+	 */
+	ena_admin_pkt_header_templates_config = 23,
+
+	/* Direct Data Placement (DDP) configuration */
+	ena_admin_ddp_config = 24,
+
+	/* Wake on LAN configuration */
+	ena_admin_wol_config = 25,
+
+	/* AENQ configuration */
+	ena_admin_aenq_config = 26,
+
+	/* Link configuration */
+	ena_admin_link_config = 27,
+
+	/* Host attributes configuration */
+	ena_admin_host_attr_config = 28,
+
+	/* Number of valid opcodes */
+	ena_admin_features_opcode_num = 32,
+};
+
+/* descriptors and headers placement */
+enum ena_admin_placement_policy_type {
+	/* descriptors and headers are in OS memory */
+	ena_admin_placement_policy_host = 1,
+
+	/*
+	 * descriptors and headers in device memory (a.k.a Low Latency
+	 * Queue)
+	 */
+	ena_admin_placement_policy_dev = 3,
+};
+
+/* link speeds */
+enum ena_admin_link_types {
+	ena_admin_link_speed_1G = 0x1,
+
+	ena_admin_link_speed_2_half_G = 0x2,
+
+	ena_admin_link_speed_5G = 0x4,
+
+	ena_admin_link_speed_10G = 0x8,
+
+	ena_admin_link_speed_25G = 0x10,
+
+	ena_admin_link_speed_40G = 0x20,
+
+	ena_admin_link_speed_50G = 0x40,
+
+	ena_admin_link_speed_100G = 0x80,
+
+	ena_admin_link_speed_200G = 0x100,
+
+	ena_admin_link_speed_400G = 0x200,
+};
+
+/* completion queue update policy */
+enum ena_admin_completion_policy_type {
+	/* cqe for each sq descriptor */
+	ena_admin_completion_policy_desc = 0,
+
+	/* cqe upon request in sq descriptor */
+	ena_admin_completion_policy_desc_on_denamd = 1,
+
+	/*
+	 * current queue head pointer is updated in OS memory upon sq
+	 * descriptor request
+	 */
+	ena_admin_completion_policy_head_on_deman = 2,
+
+	/*
+	 * current queue head pointer is updated in OS memory for each sq
+	 * descriptor
+	 */
+	ena_admin_completion_policy_head = 3,
+};
+
+/* type of get statistics command */
+enum ena_admin_get_stats_type {
+	/* Basic statistics */
+	ena_admin_get_stats_type_basic = 0,
+
+	/* Extended statistics */
+	ena_admin_get_stats_type_extended = 1,
+};
+
+/* scope of get statistics command */
+enum ena_admin_get_stats_scope {
+	ena_admin_specific_queue = 0,
+
+	ena_admin_eth_traffic = 1,
+};
+
+/* ENA Admin Queue (AQ) common descriptor */
+struct ena_admin_aq_common_desc {
+	/* word 0 : */
+	/*
+	 * command identificator to associate it with the completion
+	 * 11:0 : command_id
+	 * 15:12 : reserved12
+	 */
+	u16 command_id;
+
+	/* as appears in ena_aq_opcode */
+	u8 opcode;
+
+	/*
+	 * 0 : phase
+	 * 1 : ctrl_data - control buffer address valid
+	 * 2 : ctrl_data_indirect - control buffer address
+	 *    points to list of pages with addresses of control
+	 *    buffers
+	 * 7:3 : reserved3
+	 */
+	u8 flags;
+};
+
+/*
+ * used in ena_aq_entry. Can point directly to control data, or to a page
+ * list chunk. Used also at the end of indirect mode page list chunks, for
+ * chaining.
+ */
+struct ena_admin_ctrl_buff_info {
+	/*
+	 * word 0 : indicates length of the buffer pointed by
+	 * control_buffer_address.
+	 */
+	u32 length;
+
+	/* words 1:2 : points to control buffer (direct or indirect) */
+	struct ena_common_mem_addr address;
+};
+
+/* submission queue full identification */
+struct ena_admin_sq {
+	/* word 0 : */
+	/* queue id */
+	u16 sq_idx;
+
+	/*
+	 * 4:0 : sq_type - 0x1 - ethernet queue; 0x2 - fabric
+	 *    queue; 0x3 fabric queue with RDMA; 0x4 - DPDK queue
+	 * 7:5 : sq_direction - 0x1 - Tx; 0x2 - Rx; 0x3 - SRQ
+	 */
+	u8 sq_identity;
+
+	u8 reserved1;
+};
+
+/* AQ entry format */
+struct ena_admin_aq_entry {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1:3 :  */
+	union {
+		/* command specific inline data */
+		u32 inline_data_w1[3];
+
+		/*
+		 * words 1:3 : points to control buffer (direct or
+		 * indirect, chained if needed)
+		 */
+		struct ena_admin_ctrl_buff_info control_buffer;
+	} u;
+
+	/* command specific inline data */
+	u32 inline_data_w4[12];
+};
+
+/* ENA Admin Completion Queue (ACQ) common descriptor */
+struct ena_admin_acq_common_desc {
+	/* word 0 : */
+	/*
+	 * command identifier to associate it with the aq descriptor
+	 * 11:0 : command_id
+	 * 15:12 : reserved12
+	 */
+	u16 command;
+
+	/* status of request execution */
+	u8 status;
+
+	/*
+	 * 0 : phase
+	 * 7:1 : reserved1
+	 */
+	u8 flags;
+
+	/* word 1 : */
+	/* provides additional info */
+	u16 extended_status;
+
+	/*
+	 * submission queue head index, serves as a hint what AQ entries can
+	 *    be revoked
+	 */
+	u16 sq_head_indx;
+};
+
+/* ACQ entry format */
+struct ena_admin_acq_entry {
+	/* words 0:1 :  */
+	struct ena_admin_acq_common_desc acq_common_descriptor;
+
+	/* response type specific data */
+	u32 response_specific_data[14];
+};
+
+/*
+ * ENA AQ Create Submission Queue command. Placed in control buffer pointed
+ * by AQ entry
+ */
+struct ena_admin_aq_create_sq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* word 1 : */
+	/*
+	 * 4:0 : sq_type - 0x1 - ethernet queue; 0x2 - fabric
+	 *    queue; 0x3 fabric queue with RDMA; 0x4 - DPDK queue
+	 * 7:5 : sq_direction - 0x1 - Tx; 0x2 - Rx; 0x3 - SRQ
+	 */
+	u8 sq_identity;
+
+	/*
+	 * 0 : virtual_addressing_support - whether the
+	 *    specific queue is requested to handle Userland
+	 *    virtual addresses, which burdens the ENA perfom VA
+	 *    to Physical address translation
+	 * 3:1 : traffic_class - Default traffic class for
+	 *    packets transmitted on his queue
+	 * 7:4 : rx_fixed_sgl_size - In case this value is
+	 *    larger than 0, then each Rx packet, will consumed
+	 *    a fixed number of Rx Descriptors equal to
+	 *    rx_fixed_sgl_size, this feature will enable
+	 *    capabilities like header split and aligning
+	 *    packets to fixed numbers of pages. NOTE: that
+	 *    queue depth is still in number of Rx Descriptors.
+	 *    It is the programmer responsibility to make sure
+	 *    each set of SGL has enough buffer to accept the
+	 *    maximal receive packet length
+	 */
+	u8 sq_caps_1;
+
+	/*
+	 * 3:0 : placement_policy - Describing where the SQ
+	 *    descriptor ring and the SQ packet headers reside:
+	 *    0x1 - descriptors and headers are in OS memory,
+	 *    0x3 - descriptors and headers in device memory
+	 *    (a.k.a Low Latency Queue)
+	 * 6:4 : completion_policy - Describing what policy
+	 *    to use for generation completion entry (cqe) in
+	 *    the CQ associated with this SQ: 0x0 - cqe for each
+	 *    sq descriptor, 0x1 - cqe upon request in sq
+	 *    descriptor, 0x2 - current queue head pointer is
+	 *    updated in OS memory upon sq descriptor request
+	 *    0x3 - current queue head pointer is updated in OS
+	 *    memory for each sq descriptor
+	 * 7 : reserved7
+	 */
+	u8 sq_caps_2;
+
+	/*
+	 * 0 : is_physically_contiguous - Described if the
+	 *    queue ring memory is allocated in physical
+	 *    contiguous pages or split.
+	 * 7:1 : reserved1
+	 */
+	u8 sq_caps_3;
+
+	/* word 2 : */
+	/*
+	 * associated completion queue id. This CQ must be created prior to
+	 *    SQ creation
+	 */
+	u16 cq_idx;
+
+	/* submission queue depth in # of entries */
+	u16 sq_depth;
+
+	/*
+	 * words 3:4 : SQ physical base address in OS memory. This field
+	 * should not be used for Low Latency queues. Has to be page
+	 * aligned.
+	 */
+	struct ena_common_mem_addr sq_ba;
+
+	/*
+	 * words 5:6 : specifies queue head writeback location in OS
+	 * memory. Valid if completion_policy is set to 0x3. Has to be
+	 * cache aligned
+	 */
+	struct ena_common_mem_addr sq_head_writeback;
+
+	/* word 7 : */
+	/* protection domain - needed if address translation is supported */
+	u16 pd;
+
+	/* reserved */
+	u16 reserved16_w8;
+
+	/* word 8 : reserved word */
+	u32 reserved0_w9;
+};
+
+/* submission queue direction */
+enum ena_admin_sq_direction {
+	ena_admin_sq_direction_tx = 1,
+
+	ena_admin_sq_direction_rx = 2,
+
+	/* Shared Receive queue */
+	ena_admin_sq_direction_srq = 3,
+};
+
+/* submission queue type */
+enum ena_admin_sq_type {
+	/* ethernet queue */
+	ena_admin_eth = 1,
+
+	/* fabric queue */
+	ena_admin_fabric = 2,
+
+	/* fabric queue with RDMA */
+	ena_admin_fabric_rdma = 3,
+
+	/* DPDK queue */
+	ena_admin_dpdk = 4,
+};
+
+/*
+ * ENA Response for Create SQ Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_create_sq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+
+	/* word 2 : */
+	/* sq identifier */
+	u16 sq_idx;
+
+	/* sq depth in # of entries */
+	u16 sq_actual_depth;
+
+	/*
+	 * word 3 : queue doorbell address as and offset to PCIe MMIO REG
+	 * BAR
+	 */
+	u32 sq_doorbell_offset;
+
+	/*
+	 * word 4 : low latency queue ring base address as an offset to
+	 * PCIe MMIO LLQ_MEM BAR
+	 */
+	u32 llq_descriptors_offset;
+
+	/*
+	 * word 5 : low latency queue headers' memory as an offset to PCIe
+	 * MMIO LLQ_MEM BAR
+	 */
+	u32 llq_headers_offset;
+};
+
+/*
+ * ENA AQ Destroy Submission Queue command. Placed in control buffer
+ * pointed by AQ entry
+ */
+struct ena_admin_aq_destroy_sq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1 :  */
+	struct ena_admin_sq sq;
+};
+
+/*
+ * ENA Response for Destroy SQ Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_destroy_sq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+};
+
+/* ENA AQ Create Completion Queue command */
+struct ena_admin_aq_create_cq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* word 1 : */
+	/*
+	 * 4:0 : cq_type - 0x1 - eth cq; 0x2 - fabric cq; 0x3
+	 *    fabric cq with RDMA; 0x4 - DPDK cq
+	 * 5 : interrupt_mode_enabled - if set, cq operates
+	 *    in interrupt mode, otherwise - polling
+	 * 7:6 : reserved6
+	 */
+	u8 cq_caps_1;
+
+	/*
+	 * 4:0 : cq_entry_size_words - size of CQ entry in
+	 *    32-bit words, valid values: 4, 8.
+	 * 7:5 : reserved7
+	 */
+	u8 cq_caps_2;
+
+	/* completion queue depth in # of entries */
+	u16 cq_depth;
+
+	/* word 2 : msix vector assigned to this cq */
+	u32 msix_vector;
+
+	/*
+	 * words 3:4 : cq physical base address in OS memory. CQ must be
+	 * physically contiguous
+	 */
+	struct ena_common_mem_addr cq_ba;
+};
+
+/*
+ * ENA Response for Create CQ Command. Appears in ACQ entry as response
+ * specific data
+ */
+struct ena_admin_acq_create_cq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+
+	/* word 2 : */
+	/* cq identifier */
+	u16 cq_idx;
+
+	/* actual cq depth in # of entries */
+	u16 cq_actual_depth;
+
+	/* word 3 : doorbell address as an offset to PCIe MMIO REG BAR */
+	u32 cq_doorbell_offset;
+
+	/*
+	 * word 4 : completion head doorbell address as an offset to PCIe
+	 * MMIO REG BAR
+	 */
+	u32 cq_head_db_offset;
+
+	/*
+	 * word 5 : interrupt unmask register address as an offset into
+	 * PCIe MMIO REG BAR
+	 */
+	u32 cq_interrupt_unmask_register;
+
+	/* word 6 : value to be written into interrupt unmask register */
+	u32 cq_interrupt_unmask_value;
+
+	/*
+	 * word 7 : interrupt moderation register address as an offset into
+	 * PCIe MMIO REG BAR. 1 usec granularity
+	 */
+	u32 cq_interrupt_moderation_register;
+};
+
+/*
+ * ENA AQ Destroy Completion Queue command. Placed in control buffer
+ * pointed by AQ entry
+ */
+struct ena_admin_aq_destroy_cq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* word 1 : */
+	/* associated queue id. */
+	u16 cq_idx;
+
+	u16 reserved1;
+};
+
+/*
+ * ENA Response for Destroy CQ Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_destroy_cq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+};
+
+/*
+ * ENA AQ Suspend Submission Queue command. Placed in control buffer
+ * pointed by AQ entry
+ */
+struct ena_admin_aq_suspend_sq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1 :  */
+	struct ena_admin_sq sq;
+};
+
+/*
+ * ENA Response for Suspend SQ Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_suspend_sq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+};
+
+/*
+ * ENA AQ Resume Submission Queue command. Placed in control buffer pointed
+ * by AQ entry
+ */
+struct ena_admin_aq_resume_sq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1 :  */
+	struct ena_admin_sq sq;
+};
+
+/*
+ * ENA Response for Resume SQ Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_resume_sq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+};
+
+/*
+ * ENA AQ Flush Submission Queue command. Placed in control buffer pointed
+ * by AQ entry
+ */
+struct ena_admin_aq_flush_sq_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1 :  */
+	struct ena_admin_sq sq;
+};
+
+/*
+ * ENA Response for Flush SQ Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_flush_sq_resp_desc {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+};
+
+/*
+ * ENA AQ Get Statistics command. Extended statistics are placed in control
+ * buffer pointed by AQ entry
+ */
+struct ena_admin_aq_get_stats_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1:3 :  */
+	union {
+		/* command specific inline data */
+		u32 inline_data_w1[3];
+
+		/*
+		 * words 1:3 : points to control buffer (direct or
+		 * indirect, chained if needed)
+		 */
+		struct ena_admin_ctrl_buff_info control_buffer;
+	} u;
+
+	/* word 4 : */
+	/* stats type as defined in enum ena_admin_get_stats_type */
+	u8 type;
+
+	/* stats scope defined in enum ena_admin_get_stats_scope */
+	u8 scope;
+
+	u16 reserved3;
+
+	/* word 5 : */
+	/* queue id. used when scope is specific_queue */
+	u16 queue_idx;
+
+	/*
+	 * device id, value 0xFFFF means mine. only privileged device can get
+	 *    stats of other device
+	 */
+	u16 device_id;
+};
+
+/* Basic Statistics Command. */
+struct ena_admin_basic_stats {
+	/* word 0 :  */
+	u32 tx_bytes_low;
+
+	/* word 1 :  */
+	u32 tx_bytes_high;
+
+	/* word 2 :  */
+	u32 tx_pkts_low;
+
+	/* word 3 :  */
+	u32 tx_pkts_high;
+
+	/* word 4 :  */
+	u32 rx_bytes_low;
+
+	/* word 5 :  */
+	u32 rx_bytes_high;
+
+	/* word 6 :  */
+	u32 rx_pkts_low;
+
+	/* word 7 :  */
+	u32 rx_pkts_high;
+
+	/* word 8 :  */
+	u32 rx_drops_low;
+
+	/* word 9 :  */
+	u32 rx_drops_high;
+};
+
+/*
+ * ENA Response for Get Statistics Command. Appears in ACQ entry as
+ * response_specific_data
+ */
+struct ena_admin_acq_get_stats_resp {
+	/* words 0:1 : Common Admin Queue completion descriptor */
+	struct ena_admin_acq_common_desc acq_common_desc;
+
+	/* words 2:11 :  */
+	struct ena_admin_basic_stats basic_stats;
+};
+
+/*
+ * ENA Get/Set Feature common descriptor. Appears as inline word in
+ * ena_aq_entry
+ */
+struct ena_admin_get_set_feature_common_desc {
+	/* word 0 : */
+	/*
+	 * 1:0 : select - 0x1 - current value; 0x3 - default
+	 *    value
+	 * 7:3 : reserved3
+	 */
+	u8 flags;
+
+	/* as appears in ena_feature_id */
+	u8 feature_id;
+
+	/* reserved16 */
+	u16 reserved16;
+};
+
+/* ENA Device Attributes Feature descriptor. */
+struct ena_admin_device_attr_feature_desc {
+	/* word 0 : implementation id */
+	u32 impl_id;
+
+	/* word 1 : device version */
+	u32 device_version;
+
+	/*
+	 * word 2 : bit map of which bits are supported value of 1
+	 * indicated that this feature is supported and can perform SET/GET
+	 * for it
+	 */
+	u32 supported_features;
+
+	/* word 3 :  */
+	u32 reserved3;
+
+	/*
+	 * word 4 : Indicates how many bits are used physical address
+	 * access. Typically 48
+	 */
+	u32 phys_addr_width;
+
+	/*
+	 * word 5 : Indicates how many bits are used virtual address
+	 * access. Typically 48
+	 */
+	u32 virt_addr_width;
+
+	/* unicast MAC address (in Network byte order) */
+	u8 mac_addr[6];
+
+	u8 reserved7[2];
+
+	/* word 8 : Max supported MTU value */
+	u32 max_mtu;
+};
+
+/* ENA Max Queues Feature descriptor. */
+struct ena_admin_queue_feature_desc {
+	/* word 0 : Max number of submission queues (including LLQs) */
+	u32 max_sq_num;
+
+	/* word 1 : Max submission queue depth */
+	u32 max_sq_depth;
+
+	/* word 2 : Max number of completion queues */
+	u32 max_cq_num;
+
+	/* word 3 : Max completion queue depth */
+	u32 max_cq_depth;
+
+	/* word 4 : Max number of LLQ submission queues */
+	u32 max_llq_num;
+
+	/* word 5 : Max submission queue depth of LLQ */
+	u32 max_llq_depth;
+
+	/* word 6 : Max header size for LLQ */
+	u32 max_llq_header_size;
+};
+
+/* ENA MTU Set Feature descriptor. */
+struct ena_admin_set_feature_mtu_desc {
+	/* word 0 : mtu size including L2 */
+	u32 mtu;
+};
+
+/* ENA host attributes Set Feature descriptor. */
+struct ena_admin_set_feature_host_attr_desc {
+	/* word 0 : driver version */
+	u32 driver_version;
+
+	/*
+	 * words 1:2 : 4KB of dying gasp log. This buffer is filled on
+	 * fatal error.
+	 */
+	struct ena_common_mem_addr dying_gasp_log;
+};
+
+/* ENA Interrupt Moderation metrics. */
+struct ena_admin_intr_moder_metrics_desc {
+	/* word 0 : */
+	u16 count;
+
+	/* interval in us */
+	u16 interval;
+};
+
+/* ENA Link Get Feature descriptor. */
+struct ena_admin_get_feature_link_desc {
+	/* word 0 : Link speed in Mb */
+	u32 speed;
+
+	/*
+	 * word 1 : supported speeds (bit field of enum ena_admin_link
+	 * types)
+	 */
+	u32 supported;
+
+	/* word 2 : */
+	/*
+	 * 0 : autoneg - auto negotiation
+	 * 1 : duplex - Full Duplex
+	 * 31:2 : reserved2
+	 */
+	u32 flags;
+};
+
+/* ENA Interrupt Moderation Set Feature descriptor. */
+struct ena_admin_set_feature_intr_moder_desc {
+	/* word 0 : */
+	/* associated queue id. */
+	u16 cq_idx;
+
+	/* 2:0 : sq_direction - 0x1 - Tx; 0x2 - Rx */
+	u8 queue_identity;
+
+	u8 reserved1;
+
+	/* word 1 : */
+	/*
+	 * 0 : enable
+	 * 31:1 : reserved1
+	 */
+	u32 flags;
+
+	/* words 2 :  */
+	struct ena_admin_intr_moder_metrics_desc intr_moder_metrics;
+};
+
+/* ENA AENQ Feature descriptor. */
+struct ena_admin_feature_aenq_desc {
+	/* word 0 : bitmask for AENQ groups the device can report */
+	u32 supported_groups;
+
+	/* word 1 : bitmask for AENQ groups to report */
+	u32 enabled_groups;
+};
+
+/* ENA Stateless Offload Feature descriptor. */
+struct ena_admin_feature_offload_desc {
+	/* word 0 : */
+	/*
+	 * Trasmit side stateless offload
+	 * 0 : TX_L3_csum_ipv4 - IPv4 checksum
+	 * 1 : TX_L4_ipv4_csum_part - TCP/UDP over IPv4
+	 *    checksum, the checksum field should be initialized
+	 *    with pseudo header checksum
+	 * 2 : TX_L4_ipv4_csum_full - TCP/UDP over IPv4
+	 *    checksum
+	 * 3 : TX_L4_ipv6_csum_part - TCP/UDP over IPv6
+	 *    checksum, the checksum field should be initialized
+	 *    with pseudo header checksum
+	 * 4 : TX_L4_ipv6_csum_full - TCP/UDP over IPv6
+	 *    checksum
+	 * 5 : tso_ipv4 - TCP/IPv4 Segmentation Offloading
+	 * 6 : tso_ipv6 - TCP/IPv6 Segmentation Offloading
+	 * 7 : tso_ecn - TCP Segmentation with ECN
+	 */
+	u32 tx;
+
+	/* word 1 : */
+	/*
+	 * Receive side supported stateless offload
+	 * 0 : RX_L3_csum_ipv4 - IPv4 checksum
+	 * 1 : RX_L4_ipv4_csum - TCP/UDP/IPv4 checksum
+	 * 2 : RX_L4_ipv6_csum - TCP/UDP/IPv6 checksum
+	 */
+	u32 rx_supported;
+
+	/* word 2 : */
+	/* Receive side enabled stateless offload */
+	u32 rx_enabled;
+};
+
+/* hash functions */
+enum ena_admin_hash_functions {
+	/* Toeplitz hash */
+	ena_admin_toeplitz = 1,
+
+	/* CRC32 hash */
+	ena_admin_crc32 = 2,
+};
+
+/* ENA RSS Flow Hash Function */
+struct ena_admin_feature_rss_flow_hash_function {
+	/* word 0 : */
+	/*
+	 * supported hash functions
+	 * 7:0 : funcs - supported hash functions (bitmask
+	 *    accroding to ena_admin_hash_functions)
+	 */
+	u32 supported_func;
+
+	/* word 1 : */
+	/*
+	 * selected hash func
+	 * 7:0 : selected_func - selected hash function
+	 *    (bitmask accroding to ena_admin_hash_functions)
+	 */
+	u32 selected_func;
+
+	/* word 2 : initial value */
+	u32 init_val;
+
+	/* Toeplitz keys */
+	u32 key[10];
+};
+
+/* RSS flow hash protocols */
+enum ena_admin_flow_hash_proto {
+	/* tcp/ipv4 */
+	ena_admin_rss_tcp4 = 0,
+
+	/* udp/ipv4 */
+	ena_admin_rss_udp4 = 1,
+
+	/* tcp/ipv6 */
+	ena_admin_rss_tcp6 = 2,
+
+	/* udp/ipv6 */
+	ena_admin_rss_udp6 = 3,
+
+	/* ipv4 not tcp/udp */
+	ena_admin_rss_ip4 = 4,
+
+	/* ipv6 not tcp/udp */
+	ena_admin_rss_ip6 = 5,
+
+	/* fragmented ipv4 */
+	ena_admin_rss_ip4_frag = 6,
+
+	/* not ipv4/6 */
+	ena_admin_rss_not_ip = 7,
+
+	/* max number of protocols */
+	ena_admin_rss_proto_count = 16,
+};
+
+/* RSS flow hash fields */
+enum ena_admin_flow_hash_fields {
+	/* Ethernet Dest Addr */
+	ena_admin_rss_l2_da = 0,
+
+	/* Ethernet Src Addr */
+	ena_admin_rss_l2_sa = 1,
+
+	/* ipv4/6 Dest Addr */
+	ena_admin_rss_l3_da = 2,
+
+	/* ipv4/6 Src Addr */
+	ena_admin_rss_l3_sa = 5,
+
+	/* tcp/udp Dest Port */
+	ena_admin_rss_l4_dp = 6,
+
+	/* tcp/udp Src Port */
+	ena_admin_rss_l4_sp = 7,
+};
+
+/* hash input fields for flow protocol */
+struct ena_admin_proto_input {
+	/* word 0 : */
+	/* flow hash fields (bitwise according to ena_admin_flow_hash_fields) */
+	u16 fields;
+
+	/*
+	 * 0 : inner - for tunneled packet, select the fields
+	 *    from inner header
+	 */
+	u16 flags;
+};
+
+/* ENA RSS hash control buffer structure */
+struct ena_admin_feature_rss_hash_control {
+	/* supported input fields */
+	struct ena_admin_proto_input supported_input_fields[ena_admin_rss_proto_count];
+
+	/* selected input fields */
+	struct ena_admin_proto_input selected_input_fields[ena_admin_rss_proto_count];
+
+	/* supported input fields for inner header */
+	struct ena_admin_proto_input supported_inner_input_fields[ena_admin_rss_proto_count];
+
+	/* selected input fields */
+	struct ena_admin_proto_input selected_inner_input_fields[ena_admin_rss_proto_count];
+};
+
+/* ENA RSS flow hash input */
+struct ena_admin_feature_rss_flow_hash_input {
+	/* word 0 : */
+	/*
+	 * supported hash input sorting
+	 * 1 : L3_sort - support swap L3 addresses if DA
+	 *    smaller than SA
+	 * 2 : L4_sort - support swap L4 ports if DP smaller
+	 *    SP
+	 */
+	u16 supported_input_sort;
+
+	/*
+	 * enabled hash input sorting
+	 * 1 : enable_L3_sort - enable swap L3 addresses if
+	 *    DA smaller than SA
+	 * 2 : enable_L4_sort - enable swap L4 ports if DP
+	 *    smaller than SP
+	 */
+	u16 enabled_input_sort;
+};
+
+/* ENA RSS redirection table entry */
+struct ena_admin_rss_redirection_table_entry {
+	/* word 0 : */
+	/* cq identifier */
+	u16 cq_idx;
+
+	u16 reserved;
+};
+
+/* ENA RSS redirection table */
+struct ena_admin_feature_rss_redirection_table {
+	/* word 0 : */
+	/* min supported table size (2^min_size) */
+	u16 min_size;
+
+	/* max supported table size (2^max_size) */
+	u16 max_size;
+
+	/* word 1 : */
+	/* table size (2^size) */
+	u16 size;
+
+	u16 reserved;
+};
+
+/* ENA Get Feature command */
+struct ena_admin_get_feat_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1 :  */
+	struct ena_admin_get_set_feature_common_desc feat_common;
+
+	/* words 2:15 :  */
+	union {
+		/* raw words */
+		u32 raw[14];
+	} u;
+};
+
+/* ENA Get Feature command response */
+struct ena_admin_get_feat_resp {
+	/* words 0:1 :  */
+	struct ena_admin_acq_common_desc acq_common_desc;
+
+	/* words 2:15 :  */
+	union {
+		/* raw words */
+		u32 raw[14];
+
+		/* words 2:10 : Get Device Attributes */
+		struct ena_admin_device_attr_feature_desc dev_attr;
+
+		/* words 2:5 : Max queues num */
+		struct ena_admin_queue_feature_desc max_queue;
+
+		/* words 2:3 : AENQ configuration */
+		struct ena_admin_feature_aenq_desc aenq;
+
+		/* words 2:4 : Get Link configuration */
+		struct ena_admin_get_feature_link_desc link;
+
+		/* words 2:4 : offload configuration */
+		struct ena_admin_feature_offload_desc offload;
+
+		/* words 2:14 : rss flow hash function */
+		struct ena_admin_feature_rss_flow_hash_function flow_hash_func;
+
+		/* words 2 : rss flow hash input */
+		struct ena_admin_feature_rss_flow_hash_input flow_hash_input;
+
+		/* words 2:3 : rss redirection table */
+		struct ena_admin_feature_rss_redirection_table redirection_table;
+	} u;
+};
+
+/* ENA Set Feature command */
+struct ena_admin_set_feat_cmd {
+	/* words 0 :  */
+	struct ena_admin_aq_common_desc aq_common_descriptor;
+
+	/* words 1 :  */
+	struct ena_admin_get_set_feature_common_desc feat_common;
+
+	/* words 2:15 :  */
+	union {
+		/* raw words */
+		u32 raw[14];
+
+		/* words 2 : mtu size */
+		struct ena_admin_set_feature_mtu_desc mtu;
+
+		/* words 2:4 : host attributes */
+		struct ena_admin_set_feature_host_attr_desc host_attr;
+
+		/* words 2:4 : interrupt moderation */
+		struct ena_admin_set_feature_intr_moder_desc intr_moder;
+
+		/* words 2:3 : AENQ configuration */
+		struct ena_admin_feature_aenq_desc aenq;
+
+		/* words 2:14 : rss flow hash function */
+		struct ena_admin_feature_rss_flow_hash_function flow_hash_func;
+
+		/* words 2 : rss flow hash input */
+		struct ena_admin_feature_rss_flow_hash_input flow_hash_input;
+
+		/* words 2:3 : rss redirection table */
+		struct ena_admin_feature_rss_redirection_table redirection_table;
+	} u;
+};
+
+/* ENA Set Feature command response */
+struct ena_admin_set_feat_resp {
+	/* words 0:1 :  */
+	struct ena_admin_acq_common_desc acq_common_desc;
+
+	/* words 2:15 :  */
+	union {
+		/* raw words */
+		u32 raw[14];
+	} u;
+};
+
+/* ENA Asynchronous Event Notification Queue descriptor.  */
+struct ena_admin_aenq_common_desc {
+	/* word 0 : */
+	u16 group;
+
+	u16 syndrom;
+
+	/* word 1 : */
+	/* 0 : phase */
+	u8 flags;
+
+	u8 reserved1[3];
+
+	/* word 2 : Timestamp LSB */
+	u32 timestamp_low;
+
+	/* word 3 : Timestamp MSB */
+	u32 timestamp_high;
+};
+
+/* asynchronous event notification groups */
+enum ena_admin_aenq_group {
+	/* Link State Change */
+	ena_admin_link_change = 0,
+
+	ena_admin_fatal_error = 1,
+
+	ena_admin_warning = 2,
+
+	ena_admin_notification = 3,
+
+	ena_admin_keep_alive = 4,
+
+	ena_admin_aenq_groups_num = 5,
+};
+
+/* syndrom of AENQ warning group */
+enum ena_admin_aenq_warning_syndrom {
+	ena_admin_thermal = 0,
+
+	ena_admin_logging_fifo = 1,
+
+	ena_admin_dirty_page_logging_fifo = 2,
+
+	ena_admin_malicious_mmio_access = 3,
+
+	ena_admin_cq_full = 4,
+};
+
+/* syndorm of AENQ notification group */
+enum ena_admin_aenq_notification_syndrom {
+	ena_admin_suspend = 0,
+
+	ena_admin_resume = 1,
+};
+
+/* ENA Asynchronous Event Notification generic descriptor.  */
+struct ena_admin_aenq_entry {
+	/* words 0:3 :  */
+	struct ena_admin_aenq_common_desc aenq_common_desc;
+
+	/* command specific inline data */
+	u32 inline_data_w4[12];
+};
+
+/* ENA Asynchronous Event Notification Queue Link Change descriptor.  */
+struct ena_admin_aenq_link_change_desc {
+	/* words 0:3 :  */
+	struct ena_admin_aenq_common_desc aenq_common_desc;
+
+	/* word 4 : */
+	/* 0 : link_status */
+	u32 flags;
+};
+
+/* ENA MMIO Readless response interface */
+struct ena_admin_ena_mmio_read_less_resp {
+	/* word 0 : */
+	/* request id */
+	u16 req_id;
+
+	/* register offset */
+	u16 reg_off;
+
+	/* word 1 : value is valid when poll is cleared */
+	u32 reg_val;
+};
+
+/* aq_common_desc */
+#define ENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK		GENMASK(12, 0)
+#define ENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK		BIT(0)
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_SHIFT		1
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_MASK		BIT(1)
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_SHIFT		2
+#define ENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK		BIT(2)
+
+/* sq */
+#define ENA_ADMIN_SQ_SQ_TYPE_MASK		GENMASK(5, 0)
+#define ENA_ADMIN_SQ_SQ_DIRECTION_SHIFT		5
+#define ENA_ADMIN_SQ_SQ_DIRECTION_MASK		GENMASK(8, 5)
+
+/* acq_common_desc */
+#define ENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK		GENMASK(12, 0)
+#define ENA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK		BIT(0)
+
+/* aq_create_sq_cmd */
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_TYPE_MASK		GENMASK(5, 0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT		5
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK		GENMASK(8, 5)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_VIRTUAL_ADDRESSING_SUPPORT_MASK		BIT(0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_TRAFFIC_CLASS_SHIFT		1
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_TRAFFIC_CLASS_MASK		GENMASK(4, 1)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_RX_FIXED_SGL_SIZE_SHIFT		4
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_RX_FIXED_SGL_SIZE_MASK		GENMASK(8, 4)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK		GENMASK(4, 0)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT		4
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK		GENMASK(7, 4)
+#define ENA_ADMIN_AQ_CREATE_SQ_CMD_IS_PHYSICALLY_CONTIGUOUS_MASK		BIT(0)
+
+/* aq_create_cq_cmd */
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_TYPE_MASK		GENMASK(5, 0)
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_SHIFT		5
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_MASK		BIT(5)
+#define ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK		GENMASK(5, 0)
+
+/* get_set_feature_common_desc */
+#define ENA_ADMIN_GET_SET_FEATURE_COMMON_DESC_SELECT_MASK		GENMASK(2, 0)
+
+/* get_feature_link_desc */
+#define ENA_ADMIN_GET_FEATURE_LINK_DESC_AUTONEG_MASK		BIT(0)
+#define ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_SHIFT		1
+#define ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_MASK		BIT(1)
+
+/* set_feature_intr_moder_desc */
+#define ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_SQ_DIRECTION_MASK		GENMASK(3, 0)
+#define ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_ENABLE_MASK		BIT(0)
+
+/* feature_offload_desc */
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L3_CSUM_IPV4_MASK		BIT(0)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_SHIFT		1
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_MASK		BIT(1)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_FULL_SHIFT		2
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_FULL_MASK		BIT(2)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_SHIFT		3
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_MASK		BIT(3)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_FULL_SHIFT		4
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_FULL_MASK		BIT(4)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_SHIFT		5
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_MASK		BIT(5)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_SHIFT		6
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_MASK		BIT(6)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_SHIFT		7
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_MASK		BIT(7)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L3_CSUM_IPV4_MASK		BIT(0)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_SHIFT		1
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_MASK		BIT(1)
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_SHIFT		2
+#define ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_MASK		BIT(2)
+
+/* feature_rss_flow_hash_function */
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_FUNCTION_FUNCS_MASK		GENMASK(8, 0)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_FUNCTION_SELECTED_FUNC_MASK		GENMASK(8, 0)
+
+/* proto_input */
+#define ENA_ADMIN_PROTO_INPUT_INNER_MASK		BIT(0)
+
+/* feature_rss_flow_hash_input */
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_SHIFT		1
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_MASK		BIT(1)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_SHIFT		2
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_MASK		BIT(2)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L3_SORT_SHIFT		1
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L3_SORT_MASK		BIT(1)
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L4_SORT_SHIFT		2
+#define ENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_ENABLE_L4_SORT_MASK		BIT(2)
+
+/* aenq_common_desc */
+#define ENA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK		BIT(0)
+
+/* aenq_link_change_desc */
+#define ENA_ADMIN_AENQ_LINK_CHANGE_DESC_LINK_STATUS_MASK		BIT(0)
+
+#endif /*_ENA_ADMIN_H_ */
diff --git a/drivers/amazon/ena/ena_com.c b/drivers/amazon/ena/ena_com.c
new file mode 100644
index 0000000..905b3fd
--- /dev/null
+++ b/drivers/amazon/ena/ena_com.c
@@ -0,0 +1,1659 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "ena_com.h"
+#include "ena_gen_info.h"
+
+/*****************************************************************************/
+/*****************************************************************************/
+
+/* Timeout in micro-sec */
+#define ADMIN_CMD_TIMEOUT_US (10 * 1000000)
+
+#define ENA_ASYNC_QUEUE_DEPTH 4
+#define ENA_ADMIN_QUEUE_DEPTH 32
+
+/* TODO get spec version from ena_defs */
+/* Minimal spec 0.9 */
+#define MIN_ENA_VER (((0) << ENA_REGS_VERSION_MAJOR_VERSION_SHIFT) | (9))
+
+#define ENA_CTRL_MAJOR		0
+#define ENA_CTRL_MINOR		0
+#define ENA_CTRL_SUB_MINOR	1
+
+#define MIN_ENA_CTRL_VER \
+	(((ENA_CTRL_MAJOR) << \
+	(ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_SHIFT)) | \
+	((ENA_CTRL_MINOR) << \
+	(ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_SHIFT)) | \
+	(ENA_CTRL_SUB_MINOR))
+
+#define ENA_DMA_ADDR_TO_UINT32_LOW(x)	((u32)((u64)(x)))
+#define ENA_DMA_ADDR_TO_UINT32_HIGH(x)	((u32)(((u64)(x)) >> 32))
+
+/*****************************************************************************/
+/*****************************************************************************/
+/*****************************************************************************/
+
+enum ena_cmd_status {
+	ENA_CMD_ILLIGAL,
+	ENA_CMD_SUBMITTED,
+	ENA_CMD_COMPLETED,
+	/* Abort - canceled by the driver */
+	ENA_CMD_ABORTED,
+	/* fail - failed to execute (by the HW) */
+	ENA_CMD_FAILED
+} ____cacheline_aligned;
+
+struct ena_comp_ctx {
+	struct completion wait_event;
+	struct ena_admin_acq_entry *user_cqe;
+	u32 comp_size;
+	enum ena_cmd_status status;
+	/* status from the device */
+	u8 comp_status;
+	u8 cmd_opcode;
+	bool occupied;
+};
+
+static inline void ena_com_mem_addr_set(struct ena_common_mem_addr *ena_addr,
+					dma_addr_t addr)
+{
+	ena_addr->mem_addr_low = (u32)addr;
+	ena_addr->mem_addr_high = (addr >> 32) & 0xffff;
+
+	ENA_ASSERT((addr >> ENA_MAX_PHYS_ADDR_SIZE_BITS) == 0,
+		   "Invalid addr (address have more than 48 bits");
+}
+
+static int ena_com_admin_init_sq(struct ena_com_admin_queue *queue)
+{
+	queue->sq.entries =
+		dma_alloc_coherent(queue->q_dmadev,
+				   ADMIN_SQ_SIZE(queue->q_depth),
+				   &queue->sq.dma_addr,
+				   GFP_KERNEL | __GFP_ZERO);
+
+	if (!queue->sq.entries)
+		return -ENOMEM;
+
+	queue->sq.head = 0;
+	queue->sq.tail = 0;
+	queue->sq.phase = 1;
+
+	queue->sq.db_addr = NULL;
+
+	return 0;
+}
+
+static int ena_com_admin_init_cq(struct ena_com_admin_queue *queue)
+{
+	queue->cq.entries =
+		dma_alloc_coherent(queue->q_dmadev,
+				   ADMIN_CQ_SIZE(queue->q_depth),
+				   &queue->cq.dma_addr,
+				   GFP_KERNEL | __GFP_ZERO);
+	if (!queue->cq.entries)
+		return -ENOMEM;
+
+	queue->cq.head = 0;
+	queue->cq.phase = 1;
+
+	return 0;
+}
+
+static int ena_com_admin_init_aenq(struct ena_com_dev *dev,
+				   struct ena_aenq_handlers *aenq_handlers)
+{
+	u32 aenq_caps;
+
+	dev->aenq.q_depth = ENA_ASYNC_QUEUE_DEPTH;
+	dev->aenq.entries =
+		dma_alloc_coherent(dev->dmadev,
+				   ADMIN_AENQ_SIZE(dev->aenq.q_depth),
+				   &dev->aenq.dma_addr,
+				   GFP_KERNEL | __GFP_ZERO);
+	if (!dev->aenq.entries)
+		return -ENOMEM;
+
+	dev->aenq.head = dev->aenq.q_depth;
+	dev->aenq.phase = 1;
+
+	dev->reg_bar->aenq_base_lo =
+		ENA_DMA_ADDR_TO_UINT32_LOW(dev->aenq.dma_addr);
+	dev->reg_bar->aenq_base_hi =
+		ENA_DMA_ADDR_TO_UINT32_HIGH(dev->aenq.dma_addr);
+
+	aenq_caps = 0;
+	aenq_caps |= dev->aenq.q_depth & ENA_REGS_AENQ_CAPS_AENQ_DEPTH_MASK;
+	aenq_caps |= (sizeof(struct ena_admin_aenq_entry) <<
+		ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_SHIFT) &
+		ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_MASK;
+	writel(aenq_caps, &dev->reg_bar->aenq_caps);
+
+	dev->aenq.aenq_handlers = aenq_handlers;
+
+	return 0;
+}
+
+static inline void comp_ctxt_release(struct ena_com_admin_queue *queue,
+				     struct ena_comp_ctx *comp_ctx)
+{
+	comp_ctx->occupied = false;
+	atomic_dec(&queue->outstanding_cmds);
+}
+
+static struct ena_comp_ctx *get_comp_ctxt(struct ena_com_admin_queue *queue,
+					  u16 command_id, bool capture)
+{
+	if (unlikely(command_id >= queue->q_depth)) {
+		ena_trc_err("command id is larger than the queue size. cmd_id: %u queue size %d\n",
+			    command_id, queue->q_depth);
+		return ERR_PTR(-EINVAL);
+	}
+
+	if (unlikely(queue->comp_ctx[command_id].occupied && capture))
+		return ERR_PTR(-ENOSPC);
+
+	if (capture) {
+		atomic_inc(&queue->outstanding_cmds);
+		queue->comp_ctx[command_id].occupied = true;
+	}
+
+	return &queue->comp_ctx[command_id];
+}
+
+static struct ena_comp_ctx *__ena_com_submit_admin_cmd(
+		struct ena_com_admin_queue *admin_queue,
+		struct ena_admin_aq_entry *cmd,
+		size_t cmd_size_in_bytes,
+		struct ena_admin_acq_entry *comp,
+		size_t comp_size_in_bytes)
+{
+	struct ena_comp_ctx *comp_ctx;
+	u16 tail_masked, cmd_id;
+	u16 queue_size_mask;
+	u16 cnt;
+
+	queue_size_mask = admin_queue->q_depth - 1;
+
+	tail_masked = admin_queue->sq.tail & queue_size_mask;
+
+	/* In case of queue FULL */
+	cnt = admin_queue->sq.tail - admin_queue->sq.head;
+	if (cnt >= admin_queue->q_depth) {
+		ena_trc_dbg("admin queue is FULL (tail %d head %d depth: %d)\n",
+			    admin_queue->sq.tail,
+			    admin_queue->sq.head,
+			    admin_queue->q_depth);
+		return ERR_PTR(-ENOSPC);
+	}
+
+	cmd_id = admin_queue->curr_cmd_id;
+	admin_queue->curr_cmd_id = (admin_queue->curr_cmd_id + 1) &
+		queue_size_mask;
+
+	cmd->aq_common_descriptor.flags |= admin_queue->sq.phase &
+		ENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK;
+
+	cmd->aq_common_descriptor.command_id |= cmd_id &
+		ENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK;
+
+	comp_ctx = get_comp_ctxt(admin_queue, cmd_id, true);
+	if (unlikely(IS_ERR(comp_ctx)))
+		return comp_ctx;
+
+	comp_ctx->status = ENA_CMD_SUBMITTED;
+	comp_ctx->comp_size = (u32)comp_size_in_bytes;
+	comp_ctx->user_cqe = comp;
+	comp_ctx->cmd_opcode = cmd->aq_common_descriptor.opcode;
+
+	reinit_completion(&comp_ctx->wait_event);
+
+	memcpy(&admin_queue->sq.entries[tail_masked], cmd, cmd_size_in_bytes);
+
+	admin_queue->sq.tail++;
+
+	if (unlikely((admin_queue->sq.tail & queue_size_mask) == 0))
+		admin_queue->sq.phase = 1 - admin_queue->sq.phase;
+
+	writel(admin_queue->sq.tail, admin_queue->sq.db_addr);
+
+	return comp_ctx;
+}
+
+static inline int ena_com_init_comp_ctxt(struct ena_com_admin_queue *queue)
+{
+	size_t size = queue->q_depth * sizeof(struct ena_comp_ctx);
+	struct ena_comp_ctx *comp_ctx;
+	u16 i;
+
+	queue->comp_ctx = devm_kzalloc(queue->q_dmadev, size, GFP_KERNEL);
+	if (unlikely(!queue->comp_ctx))
+		return -ENOMEM;
+
+	for (i = 0; i < queue->q_depth; i++) {
+		comp_ctx = get_comp_ctxt(queue, i, false);
+		init_completion(&comp_ctx->wait_event);
+	}
+
+	return 0;
+}
+
+static struct ena_comp_ctx *ena_com_submit_admin_cmd(
+		struct ena_com_admin_queue *admin_queue,
+		struct ena_admin_aq_entry *cmd,
+		size_t cmd_size_in_bytes,
+		struct ena_admin_acq_entry *comp,
+		size_t comp_size_in_bytes)
+{
+	unsigned long flags;
+	struct ena_comp_ctx *comp_ctx;
+
+	spin_lock_irqsave(&admin_queue->q_lock, flags);
+	if (unlikely(!admin_queue->running_state)) {
+		spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+		return ERR_PTR(-ENODEV);
+	}
+	comp_ctx = __ena_com_submit_admin_cmd(admin_queue, cmd,
+					      cmd_size_in_bytes,
+					      comp,
+					      comp_size_in_bytes);
+	spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+
+	return comp_ctx;
+}
+
+static int ena_com_init_io_sq(struct ena_com_dev *ena_dev,
+			      struct ena_com_io_sq *io_sq)
+{
+	size_t size;
+
+	memset(&io_sq->desc_addr, 0x0, sizeof(struct ena_com_io_desc_addr));
+
+	size = (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+			IO_TX_SQ_SIZE(io_sq->q_depth) :
+			IO_RX_SQ_SIZE(io_sq->q_depth);
+
+	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+		io_sq->desc_addr.virt_addr =
+			dma_alloc_coherent(ena_dev->dmadev,
+					   size,
+					   &io_sq->desc_addr.phys_addr,
+					   GFP_KERNEL | __GFP_ZERO);
+	else
+		io_sq->desc_addr.virt_addr =
+			devm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);
+
+	if (!io_sq->desc_addr.virt_addr)
+		return -ENOMEM;
+
+	io_sq->tail = 0;
+	io_sq->next_to_comp = 0;
+	io_sq->phase = 1;
+
+	io_sq->desc_entry_size =
+		(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+		sizeof(struct ena_eth_io_tx_desc) :
+		sizeof(struct ena_eth_io_rx_desc);
+
+	return 0;
+}
+
+static int ena_com_init_io_cq(struct ena_com_dev *ena_dev,
+			      struct ena_com_io_cq *io_cq)
+{
+	size_t size;
+
+	memset(&io_cq->cdesc_addr, 0x0, sizeof(struct ena_com_io_desc_addr));
+
+	size = (io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+			IO_TX_CQ_SIZE(io_cq->q_depth) :
+			IO_RX_CQ_SIZE(io_cq->q_depth);
+
+	io_cq->cdesc_addr.virt_addr =
+		dma_alloc_coherent(ena_dev->dmadev,
+				   size,
+				   &io_cq->cdesc_addr.phys_addr,
+				   GFP_KERNEL | __GFP_ZERO);
+	if (!io_cq->cdesc_addr.virt_addr)
+		return -ENOMEM;
+
+	io_cq->phase = 1;
+	io_cq->head = 0;
+
+	/* Use the basic completion descriptor for Rx */
+	io_cq->cdesc_entry_size_in_bytes =
+		(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+		sizeof(struct ena_eth_io_tx_cdesc) :
+		sizeof(struct ena_eth_io_rx_cdesc_base);
+
+	return 0;
+}
+
+static void ena_com_handle_single_admin_completion(
+		struct ena_com_admin_queue *admin_queue,
+		struct ena_admin_acq_entry *cqe)
+{
+	struct ena_comp_ctx *comp_ctx;
+	u16 cmd_id;
+
+	cmd_id = cqe->acq_common_descriptor.command &
+		ENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK;
+
+	comp_ctx = get_comp_ctxt(admin_queue, cmd_id, false);
+	ENA_ASSERT(comp_ctx, "null comp_ctx\n");
+	comp_ctx->status = ENA_CMD_COMPLETED;
+	comp_ctx->comp_status = cqe->acq_common_descriptor.status;
+
+	if (comp_ctx->user_cqe)
+		memcpy(comp_ctx->user_cqe, (void *)cqe, comp_ctx->comp_size);
+
+	complete(&comp_ctx->wait_event);
+}
+
+static void ena_com_handle_admin_completion(
+		struct ena_com_admin_queue *admin_queue)
+{
+	struct ena_admin_acq_entry *cqe = NULL;
+	u16 comp_num = 0;
+	u16 head_masked;
+	u8 phase;
+
+	head_masked = admin_queue->cq.head & (admin_queue->q_depth - 1);
+	phase = admin_queue->cq.phase;
+
+	cqe = &admin_queue->cq.entries[head_masked];
+
+	/* Go over all the completions */
+	while ((cqe->acq_common_descriptor.flags &
+			ENA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK) == phase) {
+		ena_com_handle_single_admin_completion(admin_queue, cqe);
+
+		head_masked++;
+		comp_num++;
+		if (unlikely(head_masked == admin_queue->q_depth)) {
+			head_masked = 0;
+			phase = !phase;
+		}
+
+		cqe = &admin_queue->cq.entries[head_masked];
+	}
+
+	admin_queue->cq.head += comp_num;
+	admin_queue->cq.phase = phase;
+	admin_queue->sq.head += comp_num;
+}
+
+static int ena_com_comp_status_to_errno(u8 comp_status)
+{
+	if (unlikely(comp_status != 0))
+		ena_trc_err("admin command failed[%u]\n", comp_status);
+
+	if (unlikely(comp_status > ena_admin_unknown_error))
+		return -EINVAL;
+
+	switch (comp_status) {
+	case ena_admin_success:
+		return 0;
+	case ena_admin_resource_allocation_failure:
+		return -ENOMEM;
+	case ena_admin_bad_opcode:
+	case ena_admin_unsupported_opcode:
+	case ena_admin_malformed_request:
+	case ena_admin_illegal_parameter:
+	case ena_admin_unknown_error:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int ena_com_wait_and_process_admin_cq_polling(
+		struct ena_comp_ctx *comp_ctx,
+		struct ena_com_admin_queue *admin_queue)
+{
+	unsigned long flags;
+	u32 timeout;
+	int ret;
+
+	timeout = ((uint32_t)jiffies_to_usecs(jiffies)) + ADMIN_CMD_TIMEOUT_US;
+
+	while (comp_ctx->status == ENA_CMD_SUBMITTED) {
+		if (((uint32_t)jiffies_to_usecs(jiffies)) > timeout) {
+			/* ENA didn't have any completion */
+			admin_queue->running_state = false;
+			ret = -EPERM;
+			goto err;
+		}
+
+		spin_lock_irqsave(&admin_queue->q_lock, flags);
+		ena_com_handle_admin_completion(admin_queue);
+		spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+
+		msleep(100);
+	}
+
+	if (unlikely(comp_ctx->status == ENA_CMD_ABORTED)) {
+		ena_trc_err("Command was aborted\n");
+		ret = -ENODEV;
+		goto err;
+	}
+
+	ENA_ASSERT(comp_ctx->status == ENA_CMD_COMPLETED,
+		   "Invalid comp status %d\n", comp_ctx->status);
+
+	ret = ena_com_comp_status_to_errno(comp_ctx->comp_status);
+err:
+
+	comp_ctxt_release(admin_queue, comp_ctx);
+	return ret;
+}
+
+static int ena_com_wait_and_process_admin_cq_interrupts(
+		struct ena_comp_ctx *comp_ctx,
+		struct ena_com_admin_queue *admin_queue)
+{
+	unsigned long flags;
+	int ret = 0;
+
+	wait_for_completion_timeout(&comp_ctx->wait_event,
+				    usecs_to_jiffies(ADMIN_CMD_TIMEOUT_US));
+
+	/* We have here 3 scenarios.
+	 * 1) Timeout expired but nothing happened
+	 * 2) The command was completed but we didn't get the MSI-X interrupt
+	 * 3) The command completion and MSI-X were received successfully.
+	 */
+	if (unlikely(comp_ctx->status == ENA_CMD_SUBMITTED)) {
+		spin_lock_irqsave(&admin_queue->q_lock, flags);
+		ena_com_handle_admin_completion(admin_queue);
+		spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+
+		if (comp_ctx->status == ENA_CMD_COMPLETED)
+			ena_trc_err("The ena device have completion but the driver didn't receive any MSI-X interrupt (cmd %d)\n",
+				    comp_ctx->cmd_opcode);
+		else
+			ena_trc_err("The ena device doensn't send any completion for the cmd (cmd %d)\n",
+				    comp_ctx->cmd_opcode);
+
+		admin_queue->running_state = false;
+		ret = -EPERM;
+		goto err;
+	}
+
+	ret = ena_com_comp_status_to_errno(comp_ctx->comp_status);
+err:
+	comp_ctxt_release(admin_queue, comp_ctx);
+	return ret;
+}
+
+/* This method read the hardware device register through posting writes
+ * and waiting for response
+ * On timeout the function will return 0xFFFFFFFF
+ */
+static u32 ena_com_reg_bar_read32(struct ena_com_dev *ena_dev,
+				  u16 offset)
+{
+	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
+	volatile struct ena_admin_ena_mmio_read_less_resp *read_resp =
+		mmio_read->read_resp;
+	u32 mmio_read_reg, ret;
+	unsigned long flags;
+	int i;
+
+	if (unlikely(offset > sizeof(struct ena_regs_ena_registers))) {
+		ena_trc_err("trying to read reg bar with invalid offset %x\n",
+			    offset);
+		return 0xFFFFFFFF;
+	}
+
+	might_sleep();
+
+	spin_lock_irqsave(&mmio_read->lock, flags);
+	mmio_read->seq_num++;
+
+	read_resp->req_id = mmio_read->seq_num + 0xDEAD;
+	mmio_read_reg = (offset << ENA_REGS_MMIO_READ_REG_OFF_SHIFT) &
+			ENA_REGS_MMIO_READ_REG_OFF_MASK;
+	mmio_read_reg |= mmio_read->seq_num &
+			ENA_REGS_MMIO_READ_REQ_ID_MASK;
+
+	writel(mmio_read_reg, &ena_dev->reg_bar->mmio_read);
+
+	for (i = 0; i < ENA_REG_READ_TIMEOUT; i++) {
+		if (read_resp->req_id == mmio_read->seq_num)
+			break;
+
+		udelay(1);
+	}
+
+	if (unlikely(i == ENA_REG_READ_TIMEOUT)) {
+		ena_trc_err("reading reg failed for timeout. expected: req id[%hu] offset[%hu] actual: req id[%hu] offset[%hu]\n",
+			    mmio_read->seq_num,
+			    offset,
+			    read_resp->req_id,
+			    read_resp->reg_off);
+		ret = 0xFFFFFFFF;
+		goto err;
+	}
+
+	ENA_ASSERT(read_resp->reg_off == offset,
+		   "Invalid MMIO read return value");
+
+	ret = read_resp->reg_val;
+err:
+	spin_unlock_irqrestore(&mmio_read->lock, flags);
+
+	return ret;
+}
+
+/* There are two types to wait for completion.
+ * Polling mode - wait until the completion is available.
+ * Async mode - wait on wait queue until the completion is ready
+ * (or the timeout expired).
+ * It is expected that the IRQ called ena_com_handle_admin_completion
+ * to mark the completions.
+ */
+static int ena_com_wait_and_process_admin_cq(
+		struct ena_comp_ctx *comp_ctx,
+		struct ena_com_admin_queue *admin_queue)
+{
+	if (admin_queue->polling)
+		return ena_com_wait_and_process_admin_cq_polling(comp_ctx,
+			admin_queue);
+	else
+		return ena_com_wait_and_process_admin_cq_interrupts(comp_ctx,
+			admin_queue);
+}
+
+static int ena_com_destroy_io_sq(struct ena_com_dev *ena_dev,
+				 struct ena_com_io_sq *io_sq)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_admin_aq_destroy_sq_cmd destroy_cmd;
+	struct ena_admin_acq_destroy_sq_resp_desc destroy_resp;
+	u8 direction;
+	int ret;
+
+	memset(&destroy_cmd, 0x0, sizeof(struct ena_admin_aq_destroy_sq_cmd));
+
+	if (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)
+		direction = ena_admin_sq_direction_tx;
+	else
+		direction = ena_admin_sq_direction_rx;
+
+	destroy_cmd.sq.sq_identity |= (direction <<
+		ENA_ADMIN_SQ_SQ_DIRECTION_SHIFT) &
+		ENA_ADMIN_SQ_SQ_DIRECTION_MASK;
+
+	destroy_cmd.sq.sq_idx = io_sq->idx;
+	destroy_cmd.aq_common_descriptor.opcode = ena_admin_destroy_sq;
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&destroy_cmd,
+					    sizeof(destroy_cmd),
+					    (struct ena_admin_acq_entry *)&destroy_resp,
+					    sizeof(destroy_resp));
+
+	if (unlikely(ret))
+		ena_trc_err("failed to create io cq error: %d\n", ret);
+
+	return ret;
+}
+
+static void ena_com_io_queue_free(struct ena_com_dev *ena_dev,
+				  struct ena_com_io_sq *io_sq,
+				  struct ena_com_io_cq *io_cq)
+{
+	size_t size;
+
+	if (io_cq->cdesc_addr.virt_addr) {
+		size = (io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+				IO_TX_CQ_SIZE(io_cq->q_depth) :
+				IO_RX_CQ_SIZE(io_cq->q_depth);
+
+		dma_free_coherent(ena_dev->dmadev,
+				  size,
+				  io_cq->cdesc_addr.virt_addr,
+				  io_cq->cdesc_addr.phys_addr);
+		io_cq->cdesc_addr.virt_addr = NULL;
+	}
+
+	if (io_sq->desc_addr.virt_addr) {
+		size = (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?
+				IO_TX_SQ_SIZE(io_sq->q_depth) :
+				IO_RX_SQ_SIZE(io_sq->q_depth);
+
+		if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+			dma_free_coherent(ena_dev->dmadev,
+					  size,
+					  io_sq->desc_addr.virt_addr,
+					  io_sq->desc_addr.phys_addr);
+		else
+			devm_kfree(ena_dev->dmadev, io_sq->desc_addr.virt_addr);
+
+		io_sq->desc_addr.virt_addr = NULL;
+	}
+}
+
+static int wait_for_reset_state(struct ena_com_dev *ena_dev,
+				u32 timeout, u16 exp_state)
+{
+	u32 val, i;
+
+	for (i = 0; i < timeout; i++) {
+		val = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);
+
+		if (unlikely(val == 0xFFFFFFFF)) {
+			ena_trc_err("Reg read timeout occur\n");
+			return -ETIME;
+		}
+
+		if ((val & ENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK) ==
+			exp_state)
+			return 0;
+
+		/* The resolution of the timeout is 100ms */
+		msleep(100);
+	}
+
+	return -ETIME;
+}
+
+static int ena_com_get_feature(struct ena_com_dev *ena_dev,
+			       struct ena_admin_get_feat_resp *get_resp,
+			       enum ena_admin_aq_feature_id feature_id)
+{
+	struct ena_com_admin_queue *admin_queue;
+	struct ena_admin_get_feat_cmd get_cmd;
+	int ret;
+
+	if (!ena_dev) {
+		ena_trc_err("%s : ena_dev is NULL\n", __func__);
+		return -ENODEV;
+	}
+	memset(&get_cmd, 0x0, sizeof(get_cmd));
+	admin_queue = &ena_dev->admin_queue;
+
+	get_cmd.aq_common_descriptor.opcode = ena_admin_get_feature;
+	get_cmd.aq_common_descriptor.flags = 0;
+
+	get_cmd.feat_common.feature_id = feature_id;
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)
+					    &get_cmd,
+					    sizeof(get_cmd),
+					    (struct ena_admin_acq_entry *)
+					    get_resp,
+					    sizeof(*get_resp));
+
+	if (unlikely(ret))
+		ena_trc_err("Failed to get feature. error: %d\n", ret);
+
+	return ret;
+}
+
+/*****************************************************************************/
+/*******************************      API       ******************************/
+/*****************************************************************************/
+
+int ena_com_execute_admin_command(struct ena_com_admin_queue *admin_queue,
+				  struct ena_admin_aq_entry *cmd,
+				  size_t cmd_size,
+				  struct ena_admin_acq_entry *comp,
+				  size_t comp_size)
+{
+	struct ena_comp_ctx *comp_ctx;
+	int ret = 0;
+
+	comp_ctx = ena_com_submit_admin_cmd(admin_queue, cmd, cmd_size,
+					    comp, comp_size);
+	if (unlikely(IS_ERR(comp_ctx))) {
+		ena_trc_err("Failed to submit command [%ld]\n",
+			    PTR_ERR(comp_ctx));
+		return PTR_ERR(comp_ctx);
+	}
+
+	ret = ena_com_wait_and_process_admin_cq(comp_ctx, admin_queue);
+	if (unlikely(ret))
+		ena_trc_err("Failed to process command. ret = %d\n", ret);
+
+	return ret;
+}
+
+int ena_com_create_io_sq(struct ena_com_dev *ena_dev,
+			 struct ena_com_io_sq *io_sq, u16 cq_idx)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_admin_aq_create_sq_cmd create_cmd;
+	struct ena_admin_acq_create_sq_resp_desc cmd_completion;
+	u8 direction, policy;
+	int ret;
+
+	memset(&create_cmd, 0x0, sizeof(struct ena_admin_aq_create_sq_cmd));
+
+	create_cmd.aq_common_descriptor.opcode = ena_admin_create_sq;
+	create_cmd.aq_common_descriptor.flags = 0;
+
+	create_cmd.sq_identity = 0;
+	create_cmd.sq_identity |= ena_admin_eth &
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_TYPE_MASK;
+
+	if (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)
+		direction = ena_admin_sq_direction_tx;
+	else
+		direction = ena_admin_sq_direction_rx;
+
+	create_cmd.sq_identity |= (direction <<
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT) &
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK;
+
+	switch (io_sq->mem_queue_type) {
+	case ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY:
+		policy = ena_admin_placement_policy_dev;
+		break;
+	case ENA_MEM_QUEUE_TYPE_MIXED_MEMORY:
+		ena_trc_err("Mixed mode placement policy is currently unsupported");
+		return -EINVAL;
+	case ENA_MEM_QUEUE_TYPE_HOST_MEMORY:
+		policy = ena_admin_placement_policy_host;
+		break;
+	default:
+		ena_trc_err("Invalid placement policy %u\n",
+			    io_sq->mem_queue_type);
+		return -EINVAL;
+	}
+
+	create_cmd.sq_caps_2 |= policy &
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK;
+
+	create_cmd.sq_caps_2 |= (ena_admin_completion_policy_desc <<
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT) &
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK;
+
+	create_cmd.sq_caps_3 |=
+		ENA_ADMIN_AQ_CREATE_SQ_CMD_IS_PHYSICALLY_CONTIGUOUS_MASK;
+
+	create_cmd.cq_idx = cq_idx;
+	create_cmd.sq_depth = io_sq->q_depth;
+
+	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+		ena_com_mem_addr_set(&create_cmd.sq_ba,
+				     io_sq->desc_addr.phys_addr);
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&create_cmd,
+					    sizeof(create_cmd),
+					    (struct ena_admin_acq_entry *)&cmd_completion,
+					    sizeof(cmd_completion));
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to create IO SQ. error: %d\n", ret);
+		return ret;
+	}
+
+	io_sq->idx = cmd_completion.sq_idx;
+	io_sq->q_depth = cmd_completion.sq_actual_depth;
+
+	if (io_sq->q_depth != cmd_completion.sq_actual_depth) {
+		ena_trc_err("sq depth mismatch: requested[%u], result[%u]\n",
+			    io_sq->q_depth,
+			    cmd_completion.sq_actual_depth);
+		return -ENOSPC;
+	}
+
+	io_sq->db_addr = (u32 *)((u8 *)ena_dev->reg_bar +
+		cmd_completion.sq_doorbell_offset);
+
+	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY)
+		io_sq->header_addr = (u8 *)((u8 *)ena_dev->mem_bar +
+				cmd_completion.llq_headers_offset);
+
+	if (io_sq->mem_queue_type != ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+		io_sq->desc_addr.pbuf_dev_addr =
+			(u8 *)((u8 *)ena_dev->mem_bar +
+			cmd_completion.llq_descriptors_offset);
+
+	ena_trc_dbg("created sq[%u], depth[%u]\n", io_sq->idx, io_sq->q_depth);
+
+	return ret;
+}
+
+int ena_com_create_io_cq(struct ena_com_dev *ena_dev,
+			 struct ena_com_io_cq *io_cq)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_admin_aq_create_cq_cmd create_cmd;
+	struct ena_admin_acq_create_cq_resp_desc cmd_completion;
+	int ret;
+
+	memset(&create_cmd, 0x0, sizeof(struct ena_admin_aq_create_cq_cmd));
+
+	create_cmd.aq_common_descriptor.opcode = ena_admin_create_cq;
+
+	create_cmd.cq_caps_1 |= ena_admin_eth &
+		ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_TYPE_MASK;
+	create_cmd.cq_caps_2 |= (io_cq->cdesc_entry_size_in_bytes / 4) &
+		ENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK;
+	create_cmd.cq_caps_1 |=
+		ENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_MASK;
+
+	create_cmd.msix_vector = io_cq->msix_vector;
+	create_cmd.cq_depth = io_cq->q_depth;
+
+	ena_com_mem_addr_set(&create_cmd.cq_ba, io_cq->cdesc_addr.phys_addr);
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&create_cmd,
+					    sizeof(create_cmd),
+					    (struct ena_admin_acq_entry *)&cmd_completion,
+					    sizeof(cmd_completion));
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to create IO CQ. error: %d\n", ret);
+		return ret;
+	}
+
+	io_cq->idx = cmd_completion.cq_idx;
+	io_cq->db_addr = (u32 *)((u8 *)ena_dev->reg_bar +
+		cmd_completion.cq_doorbell_offset);
+
+	if (io_cq->q_depth != cmd_completion.cq_actual_depth) {
+		ena_trc_err("completion actual queue size (%d) is differ from requested size (%d)\n",
+			    cmd_completion.cq_actual_depth, io_cq->q_depth);
+		return -ENOSPC;
+	}
+
+	io_cq->unmask_reg = (u32 *)((u8 *)ena_dev->reg_bar +
+		cmd_completion.cq_interrupt_unmask_register);
+	io_cq->unmask_val = cmd_completion.cq_interrupt_unmask_value;
+
+	ena_trc_dbg("created cq[%u], depth[%u]\n", io_cq->idx, io_cq->q_depth);
+
+	return ret;
+}
+
+int ena_com_get_io_handlers(struct ena_com_dev *ena_dev, u16 qid,
+			    struct ena_com_io_sq **io_sq,
+			    struct ena_com_io_cq **io_cq)
+{
+	if (qid >= ENA_TOTAL_NUM_QUEUES) {
+		ena_trc_err("Invalid queue number %d but the max is %d\n",
+			    qid, ENA_TOTAL_NUM_QUEUES);
+		return -EINVAL;
+	}
+
+	*io_sq = &ena_dev->io_sq_queues[qid];
+	*io_cq = &ena_dev->io_cq_queues[qid];
+
+	return 0;
+}
+
+void ena_com_abort_admin_commands(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_comp_ctx *comp_ctx;
+	u16 i;
+
+	for (i = 0; i < admin_queue->q_depth; i++) {
+		comp_ctx = get_comp_ctxt(admin_queue, i, false);
+		comp_ctx->status = ENA_CMD_ABORTED;
+
+		complete(&comp_ctx->wait_event);
+	}
+}
+
+void ena_com_wait_for_abort_completion(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	unsigned long flags;
+
+	spin_lock_irqsave(&admin_queue->q_lock, flags);
+	while (atomic_read(&admin_queue->outstanding_cmds) != 0) {
+		spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+		msleep(20);
+		spin_lock_irqsave(&admin_queue->q_lock, flags);
+	}
+	spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+}
+
+bool ena_get_admin_running_state(struct ena_com_dev *ena_dev)
+{
+	return ena_dev->admin_queue.running_state;
+}
+
+int ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,
+			  struct ena_com_io_cq *io_cq)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	struct ena_admin_aq_destroy_cq_cmd destroy_cmd;
+	struct ena_admin_acq_destroy_cq_resp_desc destroy_resp;
+	int ret;
+
+	memset(&destroy_cmd, 0x0, sizeof(struct ena_admin_aq_destroy_sq_cmd));
+
+	destroy_cmd.cq_idx = io_cq->idx;
+	destroy_cmd.aq_common_descriptor.opcode = ena_admin_destroy_cq;
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&destroy_cmd,
+					    sizeof(destroy_cmd),
+					    (struct ena_admin_acq_entry *)&destroy_resp,
+					    sizeof(destroy_resp));
+
+	if (unlikely(ret))
+		ena_trc_err("Failed to destroy IO CQ. error: %d\n", ret);
+
+	return ret;
+}
+
+void ena_com_set_admin_running_state(struct ena_com_dev *ena_dev, bool state)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	unsigned long flags;
+
+	spin_lock_irqsave(&admin_queue->q_lock, flags);
+	ena_dev->admin_queue.running_state = state;
+	spin_unlock_irqrestore(&admin_queue->q_lock, flags);
+}
+
+void ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev)
+{
+	ENA_ASSERT(ena_dev->aenq.head == ena_dev->aenq.q_depth,
+		   "Invliad AENQ state\n");
+
+	/* Init head_db to mark that all entries in the queue
+	 * are initially available
+	 */
+	writel(ena_dev->aenq.q_depth, &ena_dev->reg_bar->aenq_head_db);
+}
+
+int ena_com_get_dma_width(struct ena_com_dev *ena_dev)
+{
+	u32 caps = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);
+	int width;
+
+	if (unlikely(caps == 0xFFFFFFFF)) {
+		ena_trc_err("Reg read timeout occur\n");
+		return -ETIME;
+	}
+
+	width = (caps & ENA_REGS_CAPS_DMA_ADDR_WIDTH_MASK) >>
+		ENA_REGS_CAPS_DMA_ADDR_WIDTH_SHIFT;
+
+	ena_trc_dbg("ENA dma width: %d\n", width);
+
+	ENA_ASSERT(width > 0, "Invalid dma width: %d\n", width);
+
+	return width;
+}
+
+int ena_com_validate_version(struct ena_com_dev *ena_dev)
+{
+	u32 ver;
+	u32 ctrl_ver;
+	u32 ctrl_ver_masked;
+	/* Make sure the ENA version and the controller version are at least
+	 * as the driver expects
+	 */
+
+	ver = ena_com_reg_bar_read32(ena_dev, ENA_REGS_VERSION_OFF);
+	ctrl_ver = ena_com_reg_bar_read32(ena_dev,
+					  ENA_REGS_CONTROLLER_VERSION_OFF);
+
+	if (unlikely((ver == 0xFFFFFFFF) || (ctrl_ver == 0xFFFFFFFF))) {
+		ena_trc_err("Reg read timeout occur\n");
+		return -ETIME;
+	}
+
+	ena_trc_info("ena device version: %d.%d\n",
+		     (ver & ENA_REGS_VERSION_MAJOR_VERSION_MASK) >>
+		     ENA_REGS_VERSION_MAJOR_VERSION_SHIFT,
+		     ver & ENA_REGS_VERSION_MINOR_VERSION_MASK);
+
+	if (ver < MIN_ENA_VER) {
+		ena_trc_err("ENA version is lower than the minimal version the driver supports\n");
+		return -1;
+	}
+
+	ena_trc_info("ena controller version: %d.%d.%d implementation version %d\n",
+		     (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK)
+		     >> ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_SHIFT,
+		     (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK)
+		     >> ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_SHIFT,
+		     (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK),
+		     (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_IMPL_ID_MASK) >>
+		     ENA_REGS_CONTROLLER_VERSION_IMPL_ID_SHIFT);
+
+	ctrl_ver_masked =
+		(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK) |
+		(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK) |
+		(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK);
+
+	/* Validate the ctrl version without the implementation ID */
+	if (ctrl_ver_masked < MIN_ENA_CTRL_VER) {
+		ena_trc_err("ENA ctrl version is lower than the minimal ctrl version the driver supports\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+void ena_com_admin_destroy(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+
+	if (!admin_queue)
+		return;
+
+	if (admin_queue->comp_ctx)
+		devm_kfree(ena_dev->dmadev, admin_queue->comp_ctx);
+	admin_queue->comp_ctx = NULL;
+
+	if (admin_queue->sq.entries)
+		dma_free_coherent(ena_dev->dmadev,
+				  ADMIN_SQ_SIZE(admin_queue->q_depth),
+				  admin_queue->sq.entries,
+				  admin_queue->sq.dma_addr);
+	admin_queue->sq.entries = NULL;
+
+	if (admin_queue->cq.entries)
+		dma_free_coherent(ena_dev->dmadev,
+				  ADMIN_CQ_SIZE(admin_queue->q_depth),
+				  admin_queue->cq.entries,
+				  admin_queue->cq.dma_addr);
+	admin_queue->cq.entries = NULL;
+
+	if (ena_dev->aenq.entries)
+		dma_free_coherent(ena_dev->dmadev,
+				  ADMIN_AENQ_SIZE(ena_dev->aenq.q_depth),
+				  ena_dev->aenq.entries,
+				  ena_dev->aenq.dma_addr);
+	ena_dev->aenq.entries = NULL;
+}
+
+void ena_com_set_admin_polling_mode(struct ena_com_dev *ena_dev, bool polling)
+{
+	ena_dev->admin_queue.polling = polling;
+}
+
+bool ena_com_get_admin_polling_mode(struct ena_com_dev *ena_dev)
+{
+	return ena_dev->admin_queue.polling;
+}
+
+int ena_com_mmio_reg_read_request_init(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
+
+	spin_lock_init(&mmio_read->lock);
+	mmio_read->read_resp =
+		dma_alloc_coherent(ena_dev->dmadev,
+				   sizeof(*mmio_read->read_resp),
+				   &mmio_read->read_resp_dma_addr,
+				   GFP_KERNEL | __GFP_ZERO);
+	if (unlikely(!mmio_read->read_resp))
+		return -ENOMEM;
+
+	ena_com_mmio_reg_read_request_write_dev_addr(ena_dev);
+
+	mmio_read->read_resp->req_id = 0x0;
+	mmio_read->seq_num = 0x0;
+
+	return 0;
+}
+
+void ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
+
+	ena_dev->reg_bar->mmio_resp_lo = ENA_DMA_ADDR_TO_UINT32_LOW(0x0);
+	ena_dev->reg_bar->mmio_resp_hi = ENA_DMA_ADDR_TO_UINT32_HIGH(0x0);
+
+	dma_free_coherent(ena_dev->dmadev,
+			  sizeof(*mmio_read->read_resp),
+			  mmio_read->read_resp,
+			  mmio_read->read_resp_dma_addr);
+
+	mmio_read->read_resp = NULL;
+}
+
+void ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev)
+{
+	struct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;
+
+	ena_dev->reg_bar->mmio_resp_lo =
+		ENA_DMA_ADDR_TO_UINT32_LOW(mmio_read->read_resp_dma_addr);
+	ena_dev->reg_bar->mmio_resp_hi =
+		ENA_DMA_ADDR_TO_UINT32_HIGH(mmio_read->read_resp_dma_addr);
+}
+
+int ena_com_admin_init(struct ena_com_dev *ena_dev,
+		       struct ena_aenq_handlers *aenq_handlers,
+		       bool init_spinlock)
+{
+	struct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;
+	u32 aq_caps, acq_caps, dev_sts;
+	int ret;
+
+	ena_trc_info("ena_defs : Version:[%s] Build date [%s]",
+		     ENA_GEN_COMMIT, ENA_GEN_DATE);
+
+	dev_sts = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);
+
+	if (unlikely(dev_sts == 0xFFFFFFFF)) {
+		ena_trc_err("Reg read timeout occur\n");
+		return -ETIME;
+	}
+
+	if (!(dev_sts & ENA_REGS_DEV_STS_READY_MASK)) {
+		ena_trc_err("Device isn't ready, abort com init\n");
+		return -1;
+	}
+
+	admin_queue->q_depth = ENA_ADMIN_QUEUE_DEPTH;
+
+	admin_queue->q_dmadev = ena_dev->dmadev;
+	admin_queue->polling = false;
+	admin_queue->curr_cmd_id = 0;
+
+	atomic_set(&admin_queue->outstanding_cmds, 0);
+
+	if (init_spinlock)
+		spin_lock_init(&admin_queue->q_lock);
+
+	ret = ena_com_init_comp_ctxt(admin_queue);
+	if (ret)
+		goto error;
+
+	ret = ena_com_admin_init_sq(admin_queue);
+	if (ret)
+		goto error;
+
+	ret = ena_com_admin_init_cq(admin_queue);
+	if (ret)
+		goto error;
+
+	admin_queue->sq.db_addr = (void __iomem *)&ena_dev->reg_bar->aq_db;
+
+	ena_dev->reg_bar->aq_base_lo =
+		ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->sq.dma_addr);
+	ena_dev->reg_bar->aq_base_hi =
+		ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->sq.dma_addr);
+
+	ena_dev->reg_bar->acq_base_lo =
+		ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->cq.dma_addr);
+	ena_dev->reg_bar->acq_base_hi =
+		ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->cq.dma_addr);
+
+	aq_caps = 0;
+	aq_caps |= admin_queue->q_depth & ENA_REGS_AQ_CAPS_AQ_DEPTH_MASK;
+	aq_caps |= (sizeof(struct ena_admin_aq_entry) <<
+			ENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_SHIFT) &
+			ENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_MASK;
+
+	acq_caps = 0;
+	acq_caps |= admin_queue->q_depth & ENA_REGS_ACQ_CAPS_ACQ_DEPTH_MASK;
+	acq_caps |= (sizeof(struct ena_admin_acq_entry) <<
+		ENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_SHIFT) &
+		ENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_MASK;
+
+	writel(aq_caps, &ena_dev->reg_bar->aq_caps);
+	writel(acq_caps, &ena_dev->reg_bar->acq_caps);
+	ret = ena_com_admin_init_aenq(ena_dev, aenq_handlers);
+	if (ret)
+		goto error;
+
+	admin_queue->running_state = true;
+
+	return 0;
+error:
+	ena_com_admin_destroy(ena_dev);
+
+	return ret;
+}
+
+int ena_com_create_io_queue(struct ena_com_dev *ena_dev,
+			    u16 qid,
+			    enum queue_direction direction,
+			    enum ena_com_memory_queue_type mem_queue_type,
+			    u32 msix_vector,
+			    u16 queue_size)
+{
+	struct ena_com_io_sq *io_sq = &ena_dev->io_sq_queues[qid];
+	struct ena_com_io_cq *io_cq = &ena_dev->io_cq_queues[qid];
+	int ret = 0;
+
+	memset(io_sq, 0x0, sizeof(struct ena_com_io_sq));
+	memset(io_cq, 0x0, sizeof(struct ena_com_io_cq));
+
+	/* Init CQ */
+	io_cq->q_depth = queue_size;
+	io_cq->direction = direction;
+	io_cq->qid = qid;
+
+	io_cq->msix_vector = msix_vector;
+
+	io_sq->q_depth = queue_size;
+	io_sq->direction = direction;
+	io_sq->qid = qid;
+
+	io_sq->mem_queue_type = mem_queue_type;
+
+	ret = ena_com_init_io_sq(ena_dev, io_sq);
+	if (ret)
+		goto error;
+	ret = ena_com_init_io_cq(ena_dev, io_cq);
+	if (ret)
+		goto error;
+
+	ret = ena_com_create_io_cq(ena_dev, io_cq);
+	if (ret)
+		goto error;
+
+	ret = ena_com_create_io_sq(ena_dev, io_sq, io_cq->idx);
+	if (ret)
+		goto error;
+
+	return 0;
+error:
+	ena_com_io_queue_free(ena_dev, io_sq, io_cq);
+	return ret;
+}
+
+void ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid)
+{
+	struct ena_com_io_sq *io_sq = &ena_dev->io_sq_queues[qid];
+	struct ena_com_io_cq *io_cq = &ena_dev->io_cq_queues[qid];
+
+	ena_com_destroy_io_sq(ena_dev, io_sq);
+	ena_com_destroy_io_cq(ena_dev, io_cq);
+
+	ena_com_io_queue_free(ena_dev, io_sq, io_cq);
+}
+
+int ena_com_get_link_params(struct ena_com_dev *ena_dev,
+			    struct ena_admin_get_feat_resp *resp)
+{
+	return ena_com_get_feature(ena_dev, resp, ena_admin_link_config);
+}
+
+int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
+			      struct ena_com_dev_get_features_ctx *get_feat_ctx)
+{
+	struct ena_admin_get_feat_resp get_resp;
+	int rc;
+
+	rc = ena_com_get_feature(ena_dev, &get_resp,
+				 ena_admin_device_attributes);
+	if (rc)
+		return rc;
+
+	memcpy(&get_feat_ctx->dev_attr, &get_resp.u.dev_attr,
+	       sizeof(get_resp.u.dev_attr));
+
+	rc = ena_com_get_feature(ena_dev, &get_resp,
+				 ena_admin_max_queues_num);
+	if (rc)
+		return rc;
+
+	memcpy(&get_feat_ctx->max_queues, &get_resp.u.max_queue,
+	       sizeof(get_resp.u.max_queue));
+
+	rc = ena_com_get_feature(ena_dev, &get_resp,
+				 ena_admin_aenq_config);
+	if (rc)
+		return rc;
+
+	memcpy(&get_feat_ctx->aenq, &get_resp.u.aenq,
+	       sizeof(get_resp.u.aenq));
+
+	rc = ena_com_get_feature(ena_dev, &get_resp,
+				 ena_admin_stateless_offload_config);
+	if (rc)
+		return rc;
+
+	memcpy(&get_feat_ctx->offload, &get_resp.u.offload,
+	       sizeof(get_resp.u.offload));
+
+	return 0;
+}
+
+void ena_com_admin_q_comp_intr_handler(struct ena_com_dev *ena_dev)
+{
+	ena_com_handle_admin_completion(&ena_dev->admin_queue);
+}
+
+/* ena_handle_specific_aenq_event:
+ * return the handler that is relevant to the specific event group
+ */
+static ena_aenq_handler ena_com_get_specific_aenq_cb(struct ena_com_dev *dev,
+						     u16 group)
+{
+	struct ena_aenq_handlers *aenq_handlers = dev->aenq.aenq_handlers;
+
+	if ((group < ENA_MAX_HANDLERS) && aenq_handlers->handlers[group])
+		return aenq_handlers->handlers[group];
+
+	return aenq_handlers->unimplemented_handler;
+}
+
+/* ena_aenq_intr_handler:
+ * handles the aenq incoming events.
+ * pop events from the queue and apply the specific handler
+ */
+void ena_com_aenq_intr_handler(struct ena_com_dev *dev, void *data)
+{
+	struct ena_admin_aenq_entry *aenq_e;
+	struct ena_admin_aenq_common_desc *aenq_common;
+	struct ena_com_aenq *aenq  = &dev->aenq;
+	ena_aenq_handler handler_cb;
+	u16 masked_head, processed = 0;
+	u8 phase;
+
+	masked_head = aenq->head & (aenq->q_depth - 1);
+	phase = aenq->phase;
+	aenq_e = &aenq->entries[masked_head]; /* Get first entry */
+	aenq_common = &aenq_e->aenq_common_desc;
+
+	/* Go over all the events */
+	while ((aenq_common->flags & ENA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK) ==
+		phase) {
+		ena_trc_dbg("AENQ! Group[%x] Syndrom[%x] timestamp: [%llus]\n",
+			    aenq_common->group,
+			    aenq_common->syndrom,
+			    (u64)aenq_common->timestamp_low +
+			    ((u64)aenq_common->timestamp_high << 32));
+
+		/* Handle specific event*/
+		handler_cb = ena_com_get_specific_aenq_cb(dev,
+							  aenq_common->group);
+		handler_cb(data, aenq_e); /* call the actual event handler*/
+
+		/* Get next event entry */
+		masked_head++;
+		processed++;
+
+		if (unlikely(masked_head == aenq->q_depth)) {
+			masked_head = 0;
+			phase = !phase;
+		}
+		aenq_e = &aenq->entries[masked_head];
+		aenq_common = &aenq_e->aenq_common_desc;
+	}
+
+	aenq->head += processed;
+	aenq->phase = phase;
+	/* update ena-device for the last processed event */
+	if (processed)
+		writel((u32)aenq->head, &dev->reg_bar->aenq_head_db);
+}
+
+int ena_com_dev_reset(struct ena_com_dev *ena_dev)
+{
+	u32 stat, timeout, cap;
+	int rc;
+
+	stat = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);
+	cap = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);
+
+	if (unlikely((stat == 0xFFFFFFFF) || (cap == 0xFFFFFFFF))) {
+		ena_trc_err("Reg read32 timeout occur\n");
+		return -ETIME;
+	}
+
+	if ((stat & ENA_REGS_DEV_STS_READY_MASK) == 0) {
+		ena_trc_err("Device isn't ready, can't reset device\n");
+		return -EINVAL;
+	}
+
+	timeout = (cap & ENA_REGS_CAPS_RESET_TIMEOUT_MASK) >>
+			ENA_REGS_CAPS_RESET_TIMEOUT_SHIFT;
+	if (timeout == 0) {
+		ena_trc_err("Invalid timeout value\n");
+		return -EINVAL;
+	}
+
+	/* If the register read failed */
+	if (unlikely((stat == 0xFFFFFFFF) || (cap == 0xFFFFFFFF)))
+		return -ETIME;
+
+	/* start reset */
+	writel(ENA_REGS_DEV_CTL_DEV_RESET_MASK, &ena_dev->reg_bar->dev_ctl);
+
+	/* Write again the MMIO read request address */
+	ena_com_mmio_reg_read_request_write_dev_addr(ena_dev);
+
+	rc = wait_for_reset_state(ena_dev, timeout,
+				  ENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK);
+	if (rc != 0) {
+		ena_trc_err("Reset indication didn't turn on\n");
+		return rc;
+	}
+
+	/* reset done */
+	writel(0, &ena_dev->reg_bar->dev_ctl);
+	rc = wait_for_reset_state(ena_dev, timeout, 0);
+	if (rc != 0) {
+		ena_trc_err("Reset indication didn't turn off\n");
+		return rc;
+	}
+
+	return 0;
+}
+
+static int ena_get_dev_stats(struct ena_com_dev *ena_dev,
+			     struct ena_admin_aq_get_stats_cmd *get_cmd,
+			     struct ena_admin_acq_get_stats_resp *get_resp,
+			     enum ena_admin_get_stats_type type)
+{
+	struct ena_com_admin_queue *admin_queue;
+	int ret = 0;
+
+	if (!ena_dev) {
+		ena_trc_err("%s : ena_dev is NULL\n", __func__);
+		return -ENODEV;
+	}
+
+	admin_queue = &ena_dev->admin_queue;
+
+	get_cmd->aq_common_descriptor.opcode = ena_admin_get_stats;
+	get_cmd->aq_common_descriptor.flags = 0;
+	get_cmd->type = type;
+
+	ret =  ena_com_execute_admin_command(admin_queue,
+					     (struct ena_admin_aq_entry *)get_cmd,
+					     sizeof(*get_cmd),
+					     (struct ena_admin_acq_entry *)get_resp,
+					     sizeof(*get_resp));
+
+	if (unlikely(ret))
+		ena_trc_err("Failed to get stats. error: %d\n", ret);
+
+	return ret;
+}
+
+int ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,
+				struct ena_admin_basic_stats *stats)
+{
+	int ret = 0;
+	struct ena_admin_aq_get_stats_cmd get_cmd;
+	struct ena_admin_acq_get_stats_resp get_resp;
+
+	memset(&get_cmd, 0x0, sizeof(get_cmd));
+	ret = ena_get_dev_stats(ena_dev, &get_cmd, &get_resp,
+				ena_admin_get_stats_type_basic);
+	if (likely(ret == 0))
+		memcpy(stats, &get_resp.basic_stats,
+		       sizeof(get_resp.basic_stats));
+
+	return ret;
+}
+
+int ena_com_get_dev_extended_stats(struct ena_com_dev *ena_dev, char *buff,
+				   u32 len)
+{
+	int ret = 0;
+	struct ena_admin_aq_get_stats_cmd get_cmd;
+	struct ena_admin_acq_get_stats_resp get_resp;
+	u8 __iomem *virt_addr;
+	dma_addr_t phys_addr;
+
+	virt_addr = dma_alloc_coherent(ena_dev->dmadev,
+				       len,
+				       &phys_addr,
+				       GFP_KERNEL | __GFP_ZERO);
+	if (!virt_addr) {
+		ret = -ENOMEM;
+		goto done;
+	}
+	memset(&get_cmd, 0x0, sizeof(get_cmd));
+	ena_com_mem_addr_set(&get_cmd.u.control_buffer.address, phys_addr);
+	get_cmd.u.control_buffer.length = len;
+
+	get_cmd.device_id = ena_dev->stats_func;
+	get_cmd.queue_idx = ena_dev->stats_queue;
+
+	ret = ena_get_dev_stats(ena_dev, &get_cmd, &get_resp,
+				ena_admin_get_stats_type_extended);
+	if (ret < 0)
+		goto free_ext_stats_mem;
+
+	ret = snprintf(buff, len, "%s", (char *)virt_addr);
+
+free_ext_stats_mem:
+	dma_free_coherent(ena_dev->dmadev, len, virt_addr, phys_addr);
+done:
+	return ret;
+}
+
+int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu)
+{
+	struct ena_com_admin_queue *admin_queue;
+	struct ena_admin_set_feat_cmd cmd;
+	struct ena_admin_set_feat_resp resp;
+	int ret = 0;
+
+	if (unlikely(!ena_dev)) {
+		ena_trc_err("%s : ena_dev is NULL\n", __func__);
+		return -ENODEV;
+	}
+
+	memset(&cmd, 0x0, sizeof(cmd));
+	admin_queue = &ena_dev->admin_queue;
+
+	cmd.aq_common_descriptor.opcode = ena_admin_set_feature;
+	cmd.aq_common_descriptor.flags = 0;
+	cmd.feat_common.feature_id = ena_admin_mtu;
+	cmd.u.mtu.mtu = mtu;
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&cmd,
+					    sizeof(cmd),
+					    (struct ena_admin_acq_entry *)&resp,
+					    sizeof(resp));
+
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to set mtu %d. error: %d\n", mtu, ret);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+int ena_com_set_interrupt_moderation(struct ena_com_dev *ena_dev, int qid,
+				     bool enable, u16 count, u16 interval)
+{
+	struct ena_com_io_cq *io_cq = &ena_dev->io_cq_queues[qid];
+
+	struct ena_com_admin_queue *admin_queue;
+	struct ena_admin_set_feat_cmd cmd;
+	struct ena_admin_set_feat_resp resp;
+	u8 direction;
+	int ret = 0;
+
+	if (unlikely(!ena_dev)) {
+		ena_trc_err("%s : ena_dev is NULL\n", __func__);
+		return -ENODEV;
+	}
+
+	memset(&cmd, 0x0, sizeof(cmd));
+	admin_queue = &ena_dev->admin_queue;
+
+	cmd.aq_common_descriptor.opcode = ena_admin_set_feature;
+	cmd.aq_common_descriptor.flags = 0;
+	cmd.feat_common.feature_id = ena_admin_interrupt_moderation;
+	cmd.u.intr_moder.cq_idx = io_cq->idx;
+	if (io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)
+		direction = ena_admin_sq_direction_tx;
+	else
+		direction = ena_admin_sq_direction_rx;
+
+	cmd.u.intr_moder.queue_identity |= direction &
+		ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_SQ_DIRECTION_MASK;
+
+	if (enable) {
+		cmd.u.intr_moder.flags |=
+			ENA_ADMIN_SET_FEATURE_INTR_MODER_DESC_ENABLE_MASK;
+		cmd.u.intr_moder.intr_moder_metrics.count = count;
+		cmd.u.intr_moder.intr_moder_metrics.interval = interval;
+	}
+
+	ret = ena_com_execute_admin_command(admin_queue,
+					    (struct ena_admin_aq_entry *)&cmd,
+					    sizeof(cmd),
+					    (struct ena_admin_acq_entry *)&resp,
+					    sizeof(resp));
+
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to set interrupt moderation %d\n", ret);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+int ena_com_get_offload_settings(struct ena_com_dev *ena_dev,
+				 struct ena_admin_feature_offload_desc *offload)
+{
+	int ret;
+	struct ena_admin_get_feat_resp resp;
+
+	ret = ena_com_get_feature(ena_dev, &resp,
+				  ena_admin_stateless_offload_config);
+	if (unlikely(ret)) {
+		ena_trc_err("Failed to get offload capabilities %d\n", ret);
+		return -EINVAL;
+	}
+
+	memcpy(offload, &resp.u.offload, sizeof(resp.u.offload));
+
+	return 0;
+}
diff --git a/drivers/amazon/ena/ena_com.h b/drivers/amazon/ena/ena_com.h
new file mode 100644
index 0000000..07b1583
--- /dev/null
+++ b/drivers/amazon/ena/ena_com.h
@@ -0,0 +1,364 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef ENA_COM
+#define ENA_COM
+
+#include <linux/types.h>
+#include <linux/spinlock.h>
+#include <linux/wait.h>
+#include <linux/gfp.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+
+#include "ena_includes.h"
+
+#define ena_trc_dbg(format, arg...) \
+	pr_debug("[ENA_COM: %s] " format, __func__, ##arg)
+#define ena_trc_info(format, arg...) \
+	pr_info("[ENA_COM: %s] " format, __func__, ##arg)
+#define ena_trc_warn(format, arg...) \
+	pr_warn("[ENA_COM: %s] " format, __func__, ##arg)
+#define ena_trc_err(format, arg...) \
+	pr_err("[ENA_COM: %s] " format, __func__, ##arg)
+
+#define ENA_ASSERT(cond, format, arg...)				\
+	do {								\
+		if (unlikely(!(cond))) {				\
+			ena_trc_err(					\
+				"Assert failed on %s:%s:%d:" format,	\
+				__FILE__, __func__, __LINE__, ##arg);	\
+			WARN_ON(cond);					\
+		}							\
+	} while (0)
+
+#define ENA_MAX_NUM_IO_QUEUES		128U
+/* We need to queues for each IO (on for Tx and one for Rx) */
+#define ENA_TOTAL_NUM_QUEUES		(2 * (ENA_MAX_NUM_IO_QUEUES))
+
+#define ENA_MAX_HANDLERS 256
+
+#define ENA_MAX_PHYS_ADDR_SIZE_BITS 48
+
+#define ENA_MAC_LEN 6
+
+/* Unit in usec */
+#define ENA_REG_READ_TIMEOUT 5000
+
+#define ADMIN_SQ_SIZE(depth)	((depth) * sizeof(struct ena_admin_aq_entry))
+#define ADMIN_CQ_SIZE(depth)	((depth) * sizeof(struct ena_admin_acq_entry))
+#define ADMIN_AENQ_SIZE(depth)	((depth) * sizeof(struct ena_admin_aenq_entry))
+
+#define IO_TX_SQ_SIZE(depth)	((depth) * sizeof(struct ena_eth_io_tx_desc))
+#define IO_TX_CQ_SIZE(depth)	((depth) * sizeof(struct ena_eth_io_tx_cdesc))
+
+#define IO_RX_SQ_SIZE(depth) ((depth) * sizeof(struct ena_eth_io_rx_desc))
+#define IO_RX_CQ_SIZE(depth) ((depth) * sizeof(struct ena_eth_io_rx_cdesc_ext))
+
+/*****************************************************************************/
+/*****************************************************************************/
+
+enum queue_direction {
+	ENA_COM_IO_QUEUE_DIRECTION_TX,
+	ENA_COM_IO_QUEUE_DIRECTION_RX
+};
+
+enum ena_com_memory_queue_type {
+	/* descriptors and headers are located on the host OS memory
+	 */
+	ENA_MEM_QUEUE_TYPE_HOST_MEMORY = 0x1,
+	/* descriptors located on device memory
+	 * and headers located on host OS memory
+	 */
+	ENA_MEM_QUEUE_TYPE_MIXED_MEMORY = 0x2,
+	/* descriptors and headers are copied to the device memory */
+	ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY = 0x3,
+	ENA_MEM_QUEUE_TYPE_MAX_TYPES = ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY
+};
+
+struct ena_com_buf {
+	dma_addr_t paddr; /**< Buffer physical address */
+	u16 len; /**< Buffer length in bytes */
+};
+
+struct ena_com_io_desc_addr {
+	u8 __iomem *pbuf_dev_addr; /* LLQ address */
+	u8 __iomem *virt_addr;
+	dma_addr_t phys_addr;
+};
+
+struct ena_com_tx_meta {
+	u16 mss;
+	u16 l3_hdr_len;
+	u16 l3_hdr_offset;
+	u16 l3_outer_hdr_len; /* In words */
+	u16 l3_outer_hdr_offset;
+	u16 l4_hdr_len; /* In words */
+};
+
+struct ena_com_io_cq {
+	struct ena_com_io_desc_addr cdesc_addr;
+
+	u32 __iomem *db_addr;
+
+	/* The offset of the interrupt unmask register */
+	u32 __iomem *unmask_reg;
+
+	/* The value to write to the above register to unmask
+	 * the interrupt of this queue
+	 */
+	u32 unmask_val;
+	u32 msix_vector;
+
+	enum queue_direction direction;
+
+	/* holds the number of cdesc of the current packet */
+	u16 cur_rx_pkt_cdesc_count;
+	/* save the firt cdesc idx of the current packet */
+	u16 cur_rx_pkt_cdesc_start_idx;
+
+	u16 q_depth;
+	u16 qid;
+
+	u16 idx;
+	u16 head;
+	u8 phase;
+	u8 cdesc_entry_size_in_bytes;
+
+} ____cacheline_aligned;
+
+struct ena_com_io_sq {
+	struct ena_com_io_desc_addr desc_addr;
+
+	u32 __iomem *db_addr;
+	u8 __iomem *header_addr;
+
+	enum queue_direction direction;
+	enum ena_com_memory_queue_type mem_queue_type;
+
+	u32 msix_vector;
+	struct ena_com_tx_meta cached_tx_meta;
+
+	u16 q_depth;
+	u16 qid;
+
+	u16 idx;
+	u16 tail;
+	u16 next_to_comp;
+	u8 phase;
+	u8 desc_entry_size;
+} ____cacheline_aligned;
+
+struct ena_com_admin_cq {
+	struct ena_admin_acq_entry *entries;
+	dma_addr_t dma_addr;
+
+	u16 head;
+	u8 phase;
+};
+
+struct ena_com_admin_sq {
+	struct ena_admin_aq_entry *entries;
+	dma_addr_t dma_addr;
+
+	u32 __iomem *db_addr;
+
+	u16 head;
+	u16 tail;
+	u8 phase;
+
+};
+
+struct ena_com_admin_queue {
+	void *q_dmadev;
+	spinlock_t q_lock; /* spinlock for the admin queue */
+	struct ena_comp_ctx *comp_ctx;
+	u16 q_depth;
+	struct ena_com_admin_cq cq;
+	struct ena_com_admin_sq sq;
+
+	/* Indicate if the admin queue should poll for completion */
+	bool polling;
+
+	u16 curr_cmd_id;
+
+	/* Indicate that the ena was initialized and can
+	 * process new admin commands
+	 */
+	bool running_state;
+
+	/* Count the number of outstanding admin commands */
+	atomic_t outstanding_cmds;
+};
+
+struct ena_aenq_handlers;
+
+struct ena_com_aenq {
+	u16 head;
+	u8 phase;
+	struct ena_admin_aenq_entry *entries;
+	dma_addr_t dma_addr;
+	u16 q_depth;
+	struct ena_aenq_handlers *aenq_handlers;
+};
+
+struct ena_com_mmio_read {
+	struct ena_admin_ena_mmio_read_less_resp *read_resp;
+	dma_addr_t read_resp_dma_addr;
+	u16 seq_num;
+	/* spin lock to ensure a single outstanding read */
+	spinlock_t lock;
+};
+
+/* Each ena_dev is a PCI function. */
+struct ena_com_dev {
+	struct ena_com_admin_queue admin_queue;
+	struct ena_com_aenq aenq;
+	struct ena_com_io_cq io_cq_queues[ENA_TOTAL_NUM_QUEUES];
+	struct ena_com_io_sq io_sq_queues[ENA_TOTAL_NUM_QUEUES];
+	struct ena_regs_ena_registers __iomem *reg_bar;
+	void __iomem *mem_bar;
+	void *dmadev;
+
+	enum ena_com_memory_queue_type tx_mem_queue_type;
+
+	u16 stats_func; /* Selected function for extended statistic dump */
+	u16 stats_queue; /* Selected queue for extended statistic dump */
+
+	struct ena_com_mmio_read mmio_read;
+};
+
+struct ena_com_dev_get_features_ctx {
+	struct ena_admin_queue_feature_desc max_queues;
+	struct ena_admin_device_attr_feature_desc dev_attr;
+	struct ena_admin_feature_aenq_desc aenq;
+	struct ena_admin_feature_offload_desc offload;
+};
+
+/*****************************************************************************/
+/*****************************************************************************/
+
+int ena_com_get_link_params(struct ena_com_dev *ena_dev,
+			    struct ena_admin_get_feat_resp *resp);
+
+int ena_com_get_io_handlers(struct ena_com_dev *ena_dev, u16 qid,
+			    struct ena_com_io_sq **io_sq,
+			    struct ena_com_io_cq **io_cq);
+
+int ena_com_mmio_reg_read_request_init(struct ena_com_dev *ena_dev);
+
+void ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev);
+
+void ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev);
+
+int ena_com_get_dma_width(struct ena_com_dev *ena_dev);
+
+int ena_com_validate_version(struct ena_com_dev *ena_dev);
+
+void ena_com_set_admin_running_state(struct ena_com_dev *ena_dev, bool state);
+
+bool ena_com_get_admin_running_state(struct ena_com_dev *ena_dev);
+
+int ena_com_set_interrupt_moderation(struct ena_com_dev *ena_dev, int qid,
+				     bool enable, u16 count, u16 interval);
+
+void ena_com_set_admin_polling_mode(struct ena_com_dev *ena_dev, bool polling);
+
+bool ena_com_get_ena_admin_polling_mode(struct ena_com_dev *ena_dev);
+
+int ena_com_admin_init(struct ena_com_dev *ena_dev,
+		       struct ena_aenq_handlers *aenq_handlers,
+		       bool init_spinlock);
+
+void ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev);
+
+int ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,
+			      struct ena_com_dev_get_features_ctx
+			      *get_feat_ctx);
+
+int ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,
+				struct ena_admin_basic_stats *stats);
+
+int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu);
+
+int ena_com_create_io_queue(struct ena_com_dev *ena_dev, u16 qid,
+			    enum queue_direction direction,
+			    enum ena_com_memory_queue_type mem_queue_type,
+			    u32 msix_vector,
+			    u16 queue_size);
+
+void ena_com_admin_destroy(struct ena_com_dev *ena_dev);
+
+void ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid);
+
+void ena_com_admin_q_comp_intr_handler(struct ena_com_dev *ena_dev);
+
+void ena_com_admin_queue_completion_int_handler(struct ena_com_dev *ena_dev);
+
+int ena_com_create_io_cq(struct ena_com_dev *ena_dev,
+			 struct ena_com_io_cq *io_cq);
+
+int ena_com_create_io_sq(struct ena_com_dev *ena_dev,
+			 struct ena_com_io_sq *io_sq, u16 cq_idx);
+
+int ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,
+			  struct ena_com_io_cq *io_cq);
+
+int ena_com_execute_admin_command(struct ena_com_admin_queue *admin_queue,
+				  struct ena_admin_aq_entry *cmd,
+				  size_t cmd_size,
+				  struct ena_admin_acq_entry *cmd_comp,
+				  size_t cmd_comp_size);
+
+void ena_com_abort_admin_commands(struct ena_com_dev *ena_dev);
+
+void ena_com_wait_for_abort_completion(struct ena_com_dev *ena_dev);
+
+void ena_com_aenq_intr_handler(struct ena_com_dev *dev, void *data);
+
+typedef void (*ena_aenq_handler)(void *data,
+	struct ena_admin_aenq_entry *aenq_e);
+
+/* Holds all aenq handlers. Indexed by AENQ event group */
+struct ena_aenq_handlers {
+	ena_aenq_handler handlers[ENA_MAX_HANDLERS];
+	ena_aenq_handler unimplemented_handler;
+};
+
+int ena_com_dev_reset(struct ena_com_dev *ena_dev);
+
+int ena_com_get_offload_settings(struct ena_com_dev *ena_dev,
+				 struct ena_admin_feature_offload_desc
+				 *offload);
+
+#endif /* !(ENA_COM) */
diff --git a/drivers/amazon/ena/ena_common_defs.h b/drivers/amazon/ena/ena_common_defs.h
new file mode 100644
index 0000000..77bfc22
--- /dev/null
+++ b/drivers/amazon/ena/ena_common_defs.h
@@ -0,0 +1,52 @@
+/******************************************************************************
+Copyright (C) 2015 Annapurna Labs Ltd.
+
+This file may be licensed under the terms of the Annapurna Labs Commercial
+License Agreement.
+
+Alternatively, this file can be distributed under the terms of the GNU General
+Public License V2 as published by the Free Software Foundation and can be
+found at http://www.gnu.org/licenses/gpl-2.0.html
+
+Alternatively, redistribution and use in source and binary forms, with or
+without modification, are permitted provided that the following conditions are
+met:
+
+    *  Redistributions of source code must retain the above copyright notice,
+this list of conditions and the following disclaimer.
+
+    *  Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+******************************************************************************/
+
+#ifndef _ENA_COMMON_H_
+#define _ENA_COMMON_H_
+
+/* ENA operates with 48-bit memory addresses. ena_mem_addr_t */
+struct ena_common_mem_addr {
+	/* word 0 : low 32 bit of the memory address */
+	u32 mem_addr_low;
+
+	/* word 1 : */
+	/* high 16 bits of the memory address */
+	u16 mem_addr_high;
+
+	/* MBZ */
+	u16 reserved16;
+};
+
+#endif /*_ENA_COMMON_H_ */
diff --git a/drivers/amazon/ena/ena_eth_com.h b/drivers/amazon/ena/ena_eth_com.h
new file mode 100644
index 0000000..bcfe122
--- /dev/null
+++ b/drivers/amazon/ena/ena_eth_com.h
@@ -0,0 +1,576 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef ENA_ETH_COM_H_
+#define ENA_ETH_COM_H_
+
+#include "ena_com.h"
+
+#define ENA_MAX_PUSH_PKT_SIZE 128
+
+struct ena_com_tx_ctx {
+	struct ena_com_tx_meta ena_meta;
+	struct ena_com_buf *ena_bufs;
+	/* For LLQ, header buffer - pushed to the device mem space */
+	void *push_header;
+
+	enum ena_eth_io_l3_proto_index l3_proto;
+	enum ena_eth_io_l4_proto_index l4_proto;
+	u16 num_bufs;
+	u16 req_id;
+	/* For regular queue, indicate the size of the header
+	 * For LLQ, indicate the size of the pushed buffer
+	 */
+	u16 header_len;
+
+	bool meta_valid;
+	bool tso_enable;
+	bool l3_csum_enable;
+	bool l4_csum_enable;
+	bool l4_csum_partial;
+	bool tunnel_ctrl;
+};
+
+struct ena_com_rx_ctx {
+	struct ena_com_buf *ena_bufs;
+	enum ena_eth_io_l3_proto_index l3_proto;
+	enum ena_eth_io_l4_proto_index l4_proto;
+	bool l3_csum_err;
+	bool l4_csum_err;
+	/* fragmented packet */
+	bool frag;
+	u16 hash_frag_csum;
+	int max_bufs;
+};
+
+static inline struct ena_eth_io_rx_cdesc_base *ena_com_get_next_rx_cdesc(
+	struct ena_com_io_cq *io_cq)
+{
+	struct ena_eth_io_rx_cdesc_base *cdesc;
+	u16 expected_phase, head_masked;
+	u16 desc_phase;
+
+	head_masked = io_cq->head & (io_cq->q_depth - 1);
+	expected_phase = io_cq->phase;
+
+	cdesc = (struct ena_eth_io_rx_cdesc_base *)(io_cq->cdesc_addr.virt_addr
+			+ (head_masked * io_cq->cdesc_entry_size_in_bytes));
+
+	desc_phase = (cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_PHASE_MASK) >>
+			ENA_ETH_IO_RX_CDESC_BASE_PHASE_SHIFT;
+
+	if (desc_phase != expected_phase)
+		return NULL;
+
+	return cdesc;
+}
+
+static inline void ena_com_cq_inc_head(struct ena_com_io_cq *io_cq)
+{
+	io_cq->head++;
+
+	/* Switch phase bit in case of wrap around */
+	if (unlikely((io_cq->head & (io_cq->q_depth - 1)) == 0))
+		io_cq->phase = 1 - io_cq->phase;
+}
+
+static inline void *get_sq_desc(struct ena_com_io_sq *io_sq)
+{
+	u16 tail_masked;
+	u32 offset;
+
+	tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+
+	offset = tail_masked * io_sq->desc_entry_size;
+
+	return io_sq->desc_addr.virt_addr + offset;
+}
+
+static inline void ena_com_copy_curr_sq_desc_to_dev(struct ena_com_io_sq *io_sq)
+{
+	u16 tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+	u32 offset = tail_masked * io_sq->desc_entry_size;
+
+	/* In case this queue isn't a LLQ */
+	if (io_sq->mem_queue_type == ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+		return;
+
+	memcpy(io_sq->desc_addr.pbuf_dev_addr + offset,
+	       io_sq->desc_addr.virt_addr + offset,
+	       io_sq->desc_entry_size);
+}
+
+static inline void ena_com_sq_update_tail(struct ena_com_io_sq *io_sq)
+{
+	io_sq->tail++;
+
+	/* Switch phase bit in case of wrap around */
+	if (unlikely((io_sq->tail & (io_sq->q_depth - 1)) == 0))
+		io_sq->phase = 1 - io_sq->phase;
+}
+
+static inline int ena_com_write_header(struct ena_com_io_sq *io_sq,
+				       u8 *head_src, u16 header_len)
+{
+	u16 tail_masked = io_sq->tail & (io_sq->q_depth - 1);
+	u8 *dev_head_addr =
+		io_sq->header_addr + (tail_masked * ENA_MAX_PUSH_PKT_SIZE);
+
+	if (io_sq->mem_queue_type != ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY)
+		return 0;
+
+	ENA_ASSERT(io_sq->header_addr, "header address is NULL\n");
+
+	if (unlikely(header_len > ENA_MAX_PUSH_PKT_SIZE)) {
+		ena_trc_err("header size is too large\n");
+		return -EINVAL;
+	}
+
+	memcpy(dev_head_addr, head_src, header_len);
+
+	return 0;
+}
+
+static inline struct ena_eth_io_rx_cdesc_base *
+	ena_com_rx_cdesc_idx_to_ptr(struct ena_com_io_cq *io_cq, u16 idx)
+{
+	idx &= (io_cq->q_depth - 1);
+	return (struct ena_eth_io_rx_cdesc_base *)(io_cq->cdesc_addr.virt_addr +
+			idx * io_cq->cdesc_entry_size_in_bytes);
+}
+
+static inline int ena_com_cdesc_rx_pkt_get(struct ena_com_io_cq *io_cq,
+					   u16 *first_cdesc_idx,
+					   u16 *nb_hw_desc)
+{
+	struct ena_eth_io_rx_cdesc_base *cdesc;
+	u16 count = 0, head_masked;
+	u32 last = 0;
+
+	do {
+		cdesc = ena_com_get_next_rx_cdesc(io_cq);
+		if (!cdesc)
+			break;
+
+		ena_com_cq_inc_head(io_cq);
+		count++;
+		last = (cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_LAST_MASK) >>
+			ENA_ETH_IO_RX_CDESC_BASE_LAST_SHIFT;
+	} while (!last);
+
+	if (last) {
+		*first_cdesc_idx = io_cq->cur_rx_pkt_cdesc_start_idx;
+		count += io_cq->cur_rx_pkt_cdesc_count;
+
+		head_masked = io_cq->head & (io_cq->q_depth - 1);
+
+		io_cq->cur_rx_pkt_cdesc_count = 0;
+		io_cq->cur_rx_pkt_cdesc_start_idx = head_masked;
+
+		ena_trc_dbg("ena q_id: %d packets were completed. first desc idx %u descs# %d\n",
+			    io_cq->qid, *first_cdesc_idx, count);
+	} else {
+		io_cq->cur_rx_pkt_cdesc_count += count;
+		count = 0;
+	}
+
+	*nb_hw_desc = count;
+	return 0;
+}
+
+static inline bool ena_com_meta_desc_changed(struct ena_com_io_sq *io_sq,
+					     struct ena_com_tx_ctx *ena_tx_ctx)
+{
+	int rc;
+
+	if (ena_tx_ctx->meta_valid) {
+		rc = memcmp(&io_sq->cached_tx_meta,
+			    &ena_tx_ctx->ena_meta,
+			    sizeof(struct ena_com_tx_meta));
+
+		if (unlikely(rc != 0))
+			return true;
+	}
+
+	return false;
+}
+
+static inline void ena_com_create_and_store_tx_meta_desc(
+	struct ena_com_io_sq *io_sq,
+	struct ena_com_tx_ctx *ena_tx_ctx)
+{
+	struct ena_eth_io_tx_meta_desc *meta_desc = NULL;
+	struct ena_com_tx_meta *ena_meta = &ena_tx_ctx->ena_meta;
+
+	meta_desc = get_sq_desc(io_sq);
+	memset(meta_desc, 0x0, sizeof(struct ena_eth_io_tx_meta_desc));
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_DESC_MASK;
+
+	if (ena_tx_ctx->tunnel_ctrl) {
+		/* Bits 0-2 */
+		meta_desc->word3 |= (ena_meta->l3_outer_hdr_offset <<
+			ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_SHIFT) &
+			ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_MASK;
+		/* Bits 3-4 */
+		meta_desc->len_ctrl |= ((ena_meta->l3_outer_hdr_offset >> 0x3)
+			<< ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_SHIFT) &
+			ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_MASK;
+
+		meta_desc->word3 |= (ena_meta->l3_outer_hdr_len <<
+			ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_SHIFT) &
+			ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_MASK;
+	}
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_EXT_VALID_MASK;
+
+	/* bits 0-9 of the mss */
+	meta_desc->word2 |= (ena_meta->mss <<
+		ENA_ETH_IO_TX_META_DESC_MSS_LO_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_MSS_LO_MASK;
+	/* bits 10-13 of the mss */
+	meta_desc->len_ctrl |= ((ena_meta->mss >> 10) <<
+		ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_MASK;
+
+	/* Extended meta desc */
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_MASK;
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_STORE_MASK;
+	meta_desc->len_ctrl |= (io_sq->phase <<
+		ENA_ETH_IO_TX_META_DESC_PHASE_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_PHASE_MASK;
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_FIRST_MASK;
+	meta_desc->word2 |= ena_meta->l3_hdr_len &
+		ENA_ETH_IO_TX_META_DESC_L3_HDR_LEN_MASK;
+	meta_desc->word2 |= (ena_meta->l3_hdr_offset <<
+		ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_MASK;
+
+	meta_desc->word2 |= (ena_meta->l4_hdr_len <<
+		ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_SHIFT) &
+		ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_MASK;
+
+	meta_desc->len_ctrl |= ENA_ETH_IO_TX_META_DESC_META_STORE_MASK;
+
+	/* Cached the meta desc */
+	memcpy(&io_sq->cached_tx_meta, ena_meta,
+	       sizeof(struct ena_com_tx_meta));
+
+	ena_com_copy_curr_sq_desc_to_dev(io_sq);
+	ena_com_sq_update_tail(io_sq);
+}
+
+static inline void ena_com_rx_set_flags(struct ena_com_rx_ctx *ena_rx_ctx,
+					struct ena_eth_io_rx_cdesc_base *cdesc)
+{
+	ena_rx_ctx->l3_proto = cdesc->status &
+		ENA_ETH_IO_RX_CDESC_BASE_L3_PROTO_IDX_MASK;
+	ena_rx_ctx->l4_proto =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_SHIFT;
+	ena_rx_ctx->l3_csum_err =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_SHIFT;
+	ena_rx_ctx->l4_csum_err =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_SHIFT;
+	ena_rx_ctx->hash_frag_csum =
+		(cdesc->word2 & ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_SHIFT;
+	ena_rx_ctx->frag =
+		(cdesc->status & ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_MASK) >>
+		ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_SHIFT;
+
+	ena_trc_dbg("ena_rx_ctx->l3_proto %d ena_rx_ctx->l4_proto %d\nena_rx_ctx->l3_csum_err %d ena_rx_ctx->l4_csum_err %d\nhash frag %d frag: %d cdesc_status: %x\n",
+		    ena_rx_ctx->l3_proto,
+		    ena_rx_ctx->l4_proto,
+		    ena_rx_ctx->l3_csum_err,
+		    ena_rx_ctx->l4_csum_err,
+		    ena_rx_ctx->hash_frag_csum,
+		    ena_rx_ctx->frag,
+		    cdesc->status);
+}
+
+/*****************************************************************************/
+/*****************************     API      **********************************/
+/*****************************************************************************/
+
+static inline int ena_com_sq_empty_space(struct ena_com_io_sq *io_sq)
+{
+	u16 tail, next_to_comp, cnt;
+
+	next_to_comp = io_sq->next_to_comp;
+	tail = io_sq->tail;
+	cnt = tail - next_to_comp;
+
+	return io_sq->q_depth - 1 - cnt;
+}
+
+static inline int ena_com_write_sq_doorbell(struct ena_com_io_sq *io_sq)
+{
+	u16 tail;
+
+	tail = io_sq->tail;
+
+	ena_trc_dbg("write db for queue: %d tail: %d\n", io_sq->qid, tail);
+
+	writel(tail, io_sq->db_addr);
+
+	return 0;
+}
+
+static inline int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
+				     struct ena_com_tx_ctx *ena_tx_ctx,
+				     int *nb_hw_desc)
+{
+	struct ena_eth_io_tx_desc *desc = NULL;
+	struct ena_com_buf *ena_bufs = ena_tx_ctx->ena_bufs;
+	void *push_header = ena_tx_ctx->push_header;
+	u16 header_len = ena_tx_ctx->header_len;
+	u16 num_bufs = ena_tx_ctx->num_bufs;
+	int total_desc, i, rc;
+	bool have_meta;
+
+	ENA_ASSERT(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX,
+		   "wrong Q type");
+
+	/* num_bufs +1 for potential meta desc */
+	if (ena_com_sq_empty_space(io_sq) < (num_bufs + 1))
+		return -ENOMEM;
+
+	have_meta = ena_tx_ctx->meta_valid && ena_com_meta_desc_changed(io_sq,
+			ena_tx_ctx);
+	if (have_meta)
+		ena_com_create_and_store_tx_meta_desc(io_sq, ena_tx_ctx);
+
+	/* start with pushing the header (if needed) */
+	rc = ena_com_write_header(io_sq, push_header, header_len);
+	if (unlikely(rc))
+		return rc;
+
+	desc = get_sq_desc(io_sq);
+	memset(desc, 0x0, sizeof(struct ena_eth_io_tx_desc));
+
+	/* Set first desc when we don't have meta descriptor */
+	if (!have_meta)
+		desc->len_ctrl |= ENA_ETH_IO_TX_DESC_FIRST_MASK;
+
+	desc->buff_addr_hi_hdr_sz |= (header_len <<
+		ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_SHIFT) &
+		ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_MASK;
+	desc->len_ctrl |= (io_sq->phase << ENA_ETH_IO_TX_DESC_PHASE_SHIFT) &
+		ENA_ETH_IO_TX_DESC_PHASE_MASK;
+
+	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_COMP_REQ_MASK;
+
+	/* Bits 0-9 */
+	desc->meta_ctrl |= (ena_tx_ctx->req_id <<
+		ENA_ETH_IO_TX_DESC_REQ_ID_LO_SHIFT) &
+		ENA_ETH_IO_TX_DESC_REQ_ID_LO_MASK;
+
+	/* Bits 10-15 */
+	desc->len_ctrl |= ((ena_tx_ctx->req_id >> 10) <<
+		ENA_ETH_IO_TX_DESC_REQ_ID_HI_SHIFT) &
+		ENA_ETH_IO_TX_DESC_REQ_ID_HI_MASK;
+
+	if (ena_tx_ctx->meta_valid) {
+		desc->meta_ctrl |= (ena_tx_ctx->tso_enable <<
+			ENA_ETH_IO_TX_DESC_TSO_EN_SHIFT) &
+			ENA_ETH_IO_TX_DESC_TSO_EN_MASK;
+		desc->meta_ctrl |= ena_tx_ctx->l3_proto &
+			ENA_ETH_IO_TX_DESC_L3_PROTO_IDX_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l4_proto <<
+			ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l3_csum_enable <<
+			ENA_ETH_IO_TX_DESC_L3_CSUM_EN_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L3_CSUM_EN_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l4_csum_enable <<
+			ENA_ETH_IO_TX_DESC_L4_CSUM_EN_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L4_CSUM_EN_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->l4_csum_partial <<
+			ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_SHIFT) &
+			ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_MASK;
+		desc->meta_ctrl |= (ena_tx_ctx->tunnel_ctrl <<
+			ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_SHIFT) &
+			ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_MASK;
+	}
+
+	for (i = 0; i < num_bufs; i++) {
+		/* The first desc share the same desc as the header */
+		if (likely(i != 0)) {
+			ena_com_copy_curr_sq_desc_to_dev(io_sq);
+			ena_com_sq_update_tail(io_sq);
+
+			desc = get_sq_desc(io_sq);
+			memset(desc, 0x0, sizeof(struct ena_eth_io_tx_desc));
+
+			desc->len_ctrl |= (io_sq->phase <<
+				ENA_ETH_IO_TX_DESC_PHASE_SHIFT) &
+				ENA_ETH_IO_TX_DESC_PHASE_MASK;
+		}
+
+		desc->len_ctrl |= ena_bufs->len &
+			ENA_ETH_IO_TX_DESC_LENGTH_MASK;
+
+		desc->buff_addr_hi_hdr_sz |= (ena_bufs->paddr >> 32) &
+			ENA_ETH_IO_TX_DESC_ADDR_HI_MASK;
+		desc->buff_addr_lo = ena_bufs->paddr & 0xFFFFFFFF;
+		ena_bufs++;
+	}
+
+	/* set the last desc indicator */
+	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_LAST_MASK;
+
+	ena_com_copy_curr_sq_desc_to_dev(io_sq);
+
+	ena_com_sq_update_tail(io_sq);
+
+	total_desc = max_t(u16, num_bufs, 1);
+	total_desc += have_meta ? 1 : 0;
+
+	*nb_hw_desc = total_desc;
+	return 0;
+}
+
+static inline int ena_com_rx_pkt(struct ena_com_io_cq *io_cq,
+				 struct ena_com_io_sq *io_sq,
+				 struct ena_com_rx_ctx *ena_rx_ctx)
+{
+	struct ena_com_buf *ena_buf = &ena_rx_ctx->ena_bufs[0];
+	struct ena_eth_io_rx_cdesc_base *cdesc = NULL;
+	u16 cdesc_idx = 0;
+	u16 nb_hw_desc;
+	u16 i;
+	int rc;
+
+	ENA_ASSERT(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_RX,
+		   "wrong Q type");
+
+	rc = ena_com_cdesc_rx_pkt_get(io_cq, &cdesc_idx, &nb_hw_desc);
+	if (rc || (nb_hw_desc == 0))
+		return 0;
+
+	ena_trc_dbg(
+		"fetch rx packet: queue %d completed desc: %d\n",
+		io_cq->qid, nb_hw_desc);
+
+	ENA_ASSERT(nb_hw_desc <= ena_rx_ctx->max_bufs,
+		   "Too many RX cdescs (%d) > MAX(%d)\n",
+		   nb_hw_desc, ena_rx_ctx->max_bufs);
+
+	for (i = 0; i < nb_hw_desc; i++) {
+		cdesc = ena_com_rx_cdesc_idx_to_ptr(io_cq, cdesc_idx + i);
+
+		ena_buf->len = cdesc->length;
+		ena_buf++;
+	}
+
+	/* Update SQ head ptr */
+	io_sq->next_to_comp += nb_hw_desc;
+
+	ena_trc_dbg("[%s][QID#%d] Updating SQ head to: %d\n", __func__,
+		    io_sq->qid, io_sq->next_to_comp);
+
+	/* Get rx flags from the last pkt */
+	ena_com_rx_set_flags(ena_rx_ctx, cdesc);
+
+	return nb_hw_desc;
+}
+
+static inline int ena_com_add_single_rx_desc(struct ena_com_io_sq *io_sq,
+					     struct ena_com_buf *ena_buf)
+{
+	struct ena_eth_io_rx_desc *desc;
+
+	ENA_ASSERT(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_RX,
+		   "wrong Q type");
+
+	if (unlikely(ena_com_sq_empty_space(io_sq) == 0))
+		return -1;
+
+	desc = get_sq_desc(io_sq);
+	memset(desc, 0x0, sizeof(struct ena_eth_io_rx_desc));
+
+	desc->length = ena_buf->len;
+
+	desc->ctrl |= ENA_ETH_IO_RX_DESC_FIRST_MASK;
+	desc->ctrl |= ENA_ETH_IO_RX_DESC_LAST_MASK;
+	desc->ctrl |= io_sq->phase & ENA_ETH_IO_RX_DESC_PHASE_MASK;
+	desc->ctrl |= ENA_ETH_IO_RX_DESC_COMP_REQ_MASK;
+
+	desc->req_id = io_sq->tail;
+
+	desc->buff_addr_lo = ena_buf->paddr & 0xFFFFFFFF;
+	desc->buff_addr_hi = (ena_buf->paddr >> 32) & 0xFFFF;
+
+	ena_com_sq_update_tail(io_sq);
+
+	return 0;
+}
+
+static inline void ena_com_unmask_intr(struct ena_com_io_cq *io_cq)
+{
+	writel(io_cq->unmask_val, io_cq->unmask_reg);
+}
+
+static inline int ena_com_tx_comp_req_id_get(struct ena_com_io_cq *io_cq,
+					     u16 *req_id)
+{
+	u8 expected_phase, cdesc_phase;
+	struct ena_eth_io_tx_cdesc *cdesc;
+	u16 masked_head;
+
+	masked_head = io_cq->head & (io_cq->q_depth - 1);
+	expected_phase = io_cq->phase;
+
+	cdesc = (struct ena_eth_io_tx_cdesc *)(io_cq->cdesc_addr.virt_addr
+		+ (masked_head * io_cq->cdesc_entry_size_in_bytes));
+
+	cdesc_phase = cdesc->flags & ENA_ETH_IO_TX_CDESC_PHASE_MASK;
+	if (cdesc_phase != expected_phase)
+		return -1;
+
+	ena_com_cq_inc_head(io_cq);
+
+	*req_id = cdesc->req_id;
+
+	return 0;
+}
+
+static inline void ena_com_comp_ack(struct ena_com_io_sq *io_sq, u16 elem)
+{
+	io_sq->next_to_comp += elem;
+}
+
+#endif /* ENA_ETH_COM_H_ */
diff --git a/drivers/amazon/ena/ena_eth_io_defs.h b/drivers/amazon/ena/ena_eth_io_defs.h
new file mode 100644
index 0000000..823b1ec
--- /dev/null
+++ b/drivers/amazon/ena/ena_eth_io_defs.h
@@ -0,0 +1,512 @@
+/******************************************************************************
+Copyright (C) 2015 Annapurna Labs Ltd.
+
+This file may be licensed under the terms of the Annapurna Labs Commercial
+License Agreement.
+
+Alternatively, this file can be distributed under the terms of the GNU General
+Public License V2 as published by the Free Software Foundation and can be
+found at http://www.gnu.org/licenses/gpl-2.0.html
+
+Alternatively, redistribution and use in source and binary forms, with or
+without modification, are permitted provided that the following conditions are
+met:
+
+    *  Redistributions of source code must retain the above copyright notice,
+this list of conditions and the following disclaimer.
+
+    *  Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+******************************************************************************/
+
+#ifndef _ENA_ETH_IO_H_
+#define _ENA_ETH_IO_H_
+
+/* Layer 3 protocol index */
+enum ena_eth_io_l3_proto_index {
+	ena_eth_io_l3_proto_unknown = 0,
+
+	ena_eth_io_l3_proto_ipv4 = 8,
+
+	ena_eth_io_l3_proto_ipv6 = 11,
+
+	ena_eth_io_l3_proto_fcoe = 21,
+
+	ena_eth_io_l3_proto_roce = 22,
+};
+
+/* Layer 4 protocol index */
+enum ena_eth_io_l4_proto_index {
+	ena_eth_io_l4_proto_unknown = 0,
+
+	ena_eth_io_l4_proto_tcp = 12,
+
+	ena_eth_io_l4_proto_udp = 13,
+
+	ena_eth_io_l4_proto_routeable_roce = 23,
+};
+
+/* ENA IO Queue Tx descriptor */
+struct ena_eth_io_tx_desc {
+	/* word 0 : */
+	/*
+	 * length, request id and control flags
+	 * 15:0 : length - Buffer length in bytes, must
+	 *    include any packet trailers that the ENA supposed
+	 *    to update like End-to-End CRC, Authentication GMAC
+	 *    etc. This length must not include the
+	 *    'Push_Buffer' length. This length must not include
+	 *    the 4-byte added in the end for 802.3 Ethernet FCS
+	 * 21:16 : req_id_hi - Request ID[15:10]
+	 * 22 : reserved22 - MBZ
+	 * 23 : meta_desc - MBZ
+	 * 24 : phase
+	 * 25 : reserved1 - MBZ
+	 * 26 : first - Indicates first descriptor in
+	 *    transaction
+	 * 27 : last - Indicates last descriptor in
+	 *    transaction
+	 * 28 : comp_req - Indicates whether completion
+	 *    should be posted, after packet is transmitted.
+	 *    Valid only for first descriptor
+	 * 30:29 : reserved29 - MBZ
+	 * 31 : reserved31 - MBZ
+	 */
+	u32 len_ctrl;
+
+	/* word 1 : */
+	/*
+	 * ethernet control
+	 * 3:0 : l3_proto_idx - L3 protocol, if
+	 *    tunnel_ctrl[0] is set, then this is the inner
+	 *    packet L3. This field required when
+	 *    l3_csum_en,l3_csum or tso_en are set.
+	 * 4 : reserved4
+	 * 6:5 : reserved5
+	 * 7 : tso_en - Enable TSO, For TCP only. For packets
+	 *    with tunnel (tunnel_ctrl[0]=1), then the inner
+	 *    packet will be segmented while the outer tunnel is
+	 *    duplicated
+	 * 12:8 : l4_proto_idx - L4 protocol, if
+	 *    tunnel_ctrl[0] is set, then this is the inner
+	 *    packet L4. This field need to be set when
+	 *    l4_csum_en or tso_en are set.
+	 * 13 : l3_csum_en - enable IPv4 header checksum. if
+	 *    tunnel_ctrl[0] is set, then this will enable
+	 *    checksum for the inner packet IPv4
+	 * 14 : l4_csum_en - enable TCP/UDP checksum. if
+	 *    tunnel_ctrl[0] is set, then this will enable
+	 *    checksum on the inner packet TCP/UDP checksum
+	 * 15 : ethernet_fcs_dis - when set, the controller
+	 *    will not append the 802.3 Ethernet Frame Check
+	 *    Sequence to the packet
+	 * 16 : reserved16
+	 * 17 : l4_csum_partial - L4 partial checksum. when
+	 *    set to 0, the ENA calculates the L4 checksum,
+	 *    where the Destination Address required for the
+	 *    TCP/UDP pseudo-header is taken from the actual
+	 *    packet L3 header. when set to 1, the ENA doesn't
+	 *    calculate the sum of the pseudo-header, instead,
+	 *    the checksum field of the L3 is used instead. L4
+	 *    partial checksum should be used for IPv6 packet
+	 *    that contains Routing Headers.
+	 * 20:18 : tunnel_ctrl - Bit 0: tunneling exists, Bit
+	 *    1: tunnel packet actually uses UDP as L4, Bit 2:
+	 *    tunnel packet L3 protocol: 0: IPv4 1: IPv6
+	 * 21 : ts_req - Indicates that the packet is IEEE
+	 *    1588v2 packet requiring the timestamp
+	 * 31:22 : req_id_lo - Request ID[9:0]
+	 */
+	u32 meta_ctrl;
+
+	/* word 2 : Buffer address bits[31:0] */
+	u32 buff_addr_lo;
+
+	/* word 3 : */
+	/*
+	 * address high and header size
+	 * 15:0 : addr_hi - Buffer Pointer[47:32]
+	 * 23:16 : reserved16_w2
+	 * 31:24 : push_buffer_length - Push Buffer length,
+	 *    number of bytes pushed to the ENA memory. used
+	 *    only for low latency queues. Maximum allowed value
+	 *    is negotiated through Admin Aueue, Minimum allowed
+	 *    value: for IPv4/6, the pushed buffer must include
+	 *    the Layer 2 and layer 3 headers, for ARP packets,
+	 *    in must include the Layer 2 and ARP header
+	 */
+	u32 buff_addr_hi_hdr_sz;
+};
+
+/* ENA IO Queue Tx Meta descriptor */
+struct ena_eth_io_tx_meta_desc {
+	/* word 0 : */
+	/*
+	 * length, request id and control flags
+	 * 9:0 : req_id_lo - Request ID[9:0]
+	 * 11:10 : outr_l3_off_hi - valid if
+	 *    tunnel_ctrl[0]=1. bits[4:3] of outer packet L3
+	 *    offset
+	 * 12 : reserved12 - MBZ
+	 * 13 : reserved13 - MBZ
+	 * 14 : ext_valid - if set, offset fields in Word2
+	 *    are valid Also MSS High in Word 0 and Outer L3
+	 *    Offset High in WORD 0 and bits [31:24] in Word 3
+	 * 15 : word3_valid - If set Crypto Info[23:0] of
+	 *    Word 3 is valid
+	 * 19:16 : mss_hi_ptp
+	 * 20 : eth_meta_type - 0: Tx Metadata Descriptor, 1:
+	 *    Extended Metadata Descriptor
+	 * 21 : meta_store - Store extended metadata in queue
+	 *    cache
+	 * 22 : reserved22 - MBZ
+	 * 23 : meta_desc - MBO
+	 * 24 : phase
+	 * 25 : reserved25 - MBZ
+	 * 26 : first - Indicates first descriptor in
+	 *    transaction
+	 * 27 : last - Indicates last descriptor in
+	 *    transaction
+	 * 28 : comp_req - Indicates whether completion
+	 *    should be posted, after packet is transmitted.
+	 *    Valid only for first descriptor
+	 * 30:29 : reserved29 - MBZ
+	 * 31 : reserved31 - MBZ
+	 */
+	u32 len_ctrl;
+
+	/* word 1 : */
+	/*
+	 * word 1
+	 * 5:0 : req_id_hi
+	 * 31:6 : reserved6 - MBZ
+	 */
+	u32 word1;
+
+	/* word 2 : */
+	/*
+	 * word 2
+	 * 7:0 : l3_hdr_len - the header length L3 IP header.
+	 *    if tunnel_ctrl[0]=1, this is the IP header length
+	 *    of the inner packet.  FIXME - check if includes IP
+	 *    options hdr_len
+	 * 15:8 : l3_hdr_off - the offset of the first byte
+	 *    in the L3 header from the beginning of the to-be
+	 *    transmitted packet. if tunnel_ctrl[0]=1, this is
+	 *    the offset the L3 header of the inner packet
+	 * 21:16 : l4_hdr_len_in_words - counts the L4 header
+	 *    length in words. there is an explicit assumption
+	 *    that L4 header appears right after L3 header and
+	 *    L4 offset is based on l3_hdr_off+l3_hdr_len FIXME
+	 *    - pls confirm
+	 * 31:22 : mss_lo
+	 */
+	u32 word2;
+
+	/* word 3 : */
+	/*
+	 * word 3
+	 * 23:0 : crypto_info
+	 * 28:24 : outr_l3_hdr_len_words - valid if
+	 *    tunnel_ctrl[0]=1.  Counts in words
+	 * 31:29 : outr_l3_off_lo - valid if
+	 *    tunnel_ctrl[0]=1. bits[2:0] of outer packet L3
+	 *    offset. Counts the offset of the tunnel IP header
+	 *    from beginning of the packet. NOTE: if the tunnel
+	 *    header requires CRC or checksum, it is expected to
+	 *    be done by the driver as it is not done by the HW
+	 */
+	u32 word3;
+};
+
+/* ENA IO Queue Tx completions descriptor */
+struct ena_eth_io_tx_cdesc {
+	/* word 0 : */
+	/* Request ID[15:0] */
+	u16 req_id;
+
+	u8 status;
+
+	/*
+	 * flags
+	 * 0 : phase
+	 * 7:1 : reserved1
+	 */
+	u8 flags;
+
+	/* word 1 : */
+	u16 sub_qid;
+
+	/* indicates location of submission queue head */
+	u16 sq_head_idx;
+};
+
+/* ENA IO Queue Rx descriptor */
+struct ena_eth_io_rx_desc {
+	/* word 0 : */
+	/* In bytes. 0 means 64KB */
+	u16 length;
+
+	/* MBZ */
+	u8 reserved2;
+
+	/*
+	 * control flags
+	 * 0 : phase
+	 * 1 : reserved1 - MBZ
+	 * 2 : first - Indicates first descriptor in
+	 *    transaction
+	 * 3 : last - Indicates last descriptor in transaction
+	 * 4 : comp_req
+	 * 5 : reserved5 - MBO
+	 * 7:6 : reserved6 - MBZ
+	 */
+	u8 ctrl;
+
+	/* word 1 : */
+	u16 req_id;
+
+	/* MBZ */
+	u16 reserved6;
+
+	/* word 2 : Buffer address bits[31:0] */
+	u32 buff_addr_lo;
+
+	/* word 3 : */
+	/* Buffer Address bits[47:16] */
+	u16 buff_addr_hi;
+
+	/* MBZ */
+	u16 reserved16_w3;
+};
+
+/*
+ * ENA IO Queue Rx Completion Base Descriptor (4-word format). Note: all
+ * ethernet parsing information are valid only when last=1
+ */
+struct ena_eth_io_rx_cdesc_base {
+	/* word 0 : */
+	/*
+	 * 4:0 : l3_proto_idx - L3 protocol index
+	 * 6:5 : src_vlan_cnt - Source VLAN count
+	 * 7 : tunnel - Tunnel exists
+	 * 12:8 : l4_proto_idx - L4 protocol index
+	 * 13 : l3_csum_err - when set, L3 checksum error
+	 *    detected, If tunnel exists, this result is for the
+	 *    inner packet
+	 * 14 : l4_csum_err - when set, L4 checksum error
+	 *    detected, If tunnel exists, this result is for the
+	 *    inner packet
+	 * 15 : ipv4_frag - Indicates IPv4 fragmented packet
+	 * 17:16 : reserved16
+	 * 19:18 : reserved18
+	 * 20 : secured_pkt - Set if packet was handled by
+	 *    inline crypto engine
+	 * 22:21 : crypto_status -  bit 0 secured direction:
+	 *    0: decryption, 1: encryption. bit 1 reserved
+	 * 23 : reserved23
+	 * 24 : phase
+	 * 25 : l3_csum2 - second checksum engine result
+	 * 26 : first - Indicates first descriptor in
+	 *    transaction
+	 * 27 : last - Indicates last descriptor in
+	 *    transaction
+	 * 28 : inr_l4_csum - TCP/UDP checksum results for
+	 *    inner packet
+	 * 29 : reserved29
+	 * 30 : buffer - 0: Metadata descriptor. 1: Buffer
+	 *    Descriptor was used
+	 * 31 : reserved31
+	 */
+	u32 status;
+
+	/* word 1 : */
+	u16 length;
+
+	u16 req_id;
+
+	/* word 2 : */
+	/*
+	 * 8:0 : tunnel_off - inner packet offset
+	 * 15:9 : l3_off - Offset of first byte in the L3
+	 *    header from the beginning of the packet. if
+	 *    tunnel=1, this is of the inner packet
+	 * 31:16 : hash_frag_csum - 16-bit hash results for
+	 *    TCP/UDP packets (could be used by driver to
+	 *    accelerate flow lookup or LRO) OR partial checksum
+	 *    of IP packet was fragmented
+	 */
+	u32 word2;
+
+	/* word 3 : */
+	/* submission queue number */
+	u16 sub_qid;
+
+	u8 reserved;
+
+	/*
+	 * Offset of first byte in the L4 header from the beginning of the
+	 *    packet. if tunnel=1, this is of the inner packet
+	 */
+	u8 l4_off;
+};
+
+/* ENA IO Queue Rx Completion Descriptor (8-word format) */
+struct ena_eth_io_rx_cdesc_ext {
+	/* words 0:3 : Rx Completion Extended */
+	struct ena_eth_io_rx_cdesc_base base;
+
+	/* word 4 : Completed Buffer address bits[31:0] */
+	u32 buff_addr_lo;
+
+	/* word 5 : */
+	/* the buffer address used bits[47:32] */
+	u16 buff_addr_hi;
+
+	u16 reserved16;
+
+	/* word 6 : Reserved */
+	u32 reserved_w6;
+
+	/* word 7 : Reserved */
+	u32 reserved_w7;
+};
+
+/* tx_desc */
+#define ENA_ETH_IO_TX_DESC_LENGTH_MASK		GENMASK(16, 0)
+#define ENA_ETH_IO_TX_DESC_REQ_ID_HI_SHIFT		16
+#define ENA_ETH_IO_TX_DESC_REQ_ID_HI_MASK		GENMASK(22, 16)
+#define ENA_ETH_IO_TX_DESC_META_DESC_SHIFT		23
+#define ENA_ETH_IO_TX_DESC_META_DESC_MASK		BIT(23)
+#define ENA_ETH_IO_TX_DESC_PHASE_SHIFT		24
+#define ENA_ETH_IO_TX_DESC_PHASE_MASK		BIT(24)
+#define ENA_ETH_IO_TX_DESC_FIRST_SHIFT		26
+#define ENA_ETH_IO_TX_DESC_FIRST_MASK		BIT(26)
+#define ENA_ETH_IO_TX_DESC_LAST_SHIFT		27
+#define ENA_ETH_IO_TX_DESC_LAST_MASK		BIT(27)
+#define ENA_ETH_IO_TX_DESC_COMP_REQ_SHIFT		28
+#define ENA_ETH_IO_TX_DESC_COMP_REQ_MASK		BIT(28)
+#define ENA_ETH_IO_TX_DESC_L3_PROTO_IDX_MASK		GENMASK(4, 0)
+#define ENA_ETH_IO_TX_DESC_TSO_EN_SHIFT		7
+#define ENA_ETH_IO_TX_DESC_TSO_EN_MASK		BIT(7)
+#define ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_SHIFT		8
+#define ENA_ETH_IO_TX_DESC_L4_PROTO_IDX_MASK		GENMASK(13, 8)
+#define ENA_ETH_IO_TX_DESC_L3_CSUM_EN_SHIFT		13
+#define ENA_ETH_IO_TX_DESC_L3_CSUM_EN_MASK		BIT(13)
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_EN_SHIFT		14
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_EN_MASK		BIT(14)
+#define ENA_ETH_IO_TX_DESC_ETHERNET_FCS_DIS_SHIFT		15
+#define ENA_ETH_IO_TX_DESC_ETHERNET_FCS_DIS_MASK		BIT(15)
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_SHIFT		17
+#define ENA_ETH_IO_TX_DESC_L4_CSUM_PARTIAL_MASK		BIT(17)
+#define ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_SHIFT		18
+#define ENA_ETH_IO_TX_DESC_TUNNEL_CTRL_MASK		GENMASK(21, 18)
+#define ENA_ETH_IO_TX_DESC_TS_REQ_SHIFT		21
+#define ENA_ETH_IO_TX_DESC_TS_REQ_MASK		BIT(21)
+#define ENA_ETH_IO_TX_DESC_REQ_ID_LO_SHIFT		22
+#define ENA_ETH_IO_TX_DESC_REQ_ID_LO_MASK		GENMASK(32, 22)
+#define ENA_ETH_IO_TX_DESC_ADDR_HI_MASK		GENMASK(16, 0)
+#define ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_SHIFT		24
+#define ENA_ETH_IO_TX_DESC_PUSH_BUFFER_LENGTH_MASK		GENMASK(32, 24)
+
+/* tx_meta_desc */
+#define ENA_ETH_IO_TX_META_DESC_REQ_ID_LO_MASK		GENMASK(10, 0)
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_SHIFT		10
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_HI_MASK		GENMASK(12, 10)
+#define ENA_ETH_IO_TX_META_DESC_EXT_VALID_SHIFT		14
+#define ENA_ETH_IO_TX_META_DESC_EXT_VALID_MASK		BIT(14)
+#define ENA_ETH_IO_TX_META_DESC_WORD3_VALID_SHIFT		15
+#define ENA_ETH_IO_TX_META_DESC_WORD3_VALID_MASK		BIT(15)
+#define ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_SHIFT		16
+#define ENA_ETH_IO_TX_META_DESC_MSS_HI_PTP_MASK		GENMASK(20, 16)
+#define ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_SHIFT		20
+#define ENA_ETH_IO_TX_META_DESC_ETH_META_TYPE_MASK		BIT(20)
+#define ENA_ETH_IO_TX_META_DESC_META_STORE_SHIFT		21
+#define ENA_ETH_IO_TX_META_DESC_META_STORE_MASK		BIT(21)
+#define ENA_ETH_IO_TX_META_DESC_META_DESC_SHIFT		23
+#define ENA_ETH_IO_TX_META_DESC_META_DESC_MASK		BIT(23)
+#define ENA_ETH_IO_TX_META_DESC_PHASE_SHIFT		24
+#define ENA_ETH_IO_TX_META_DESC_PHASE_MASK		BIT(24)
+#define ENA_ETH_IO_TX_META_DESC_FIRST_SHIFT		26
+#define ENA_ETH_IO_TX_META_DESC_FIRST_MASK		BIT(26)
+#define ENA_ETH_IO_TX_META_DESC_LAST_SHIFT		27
+#define ENA_ETH_IO_TX_META_DESC_LAST_MASK		BIT(27)
+#define ENA_ETH_IO_TX_META_DESC_COMP_REQ_SHIFT		28
+#define ENA_ETH_IO_TX_META_DESC_COMP_REQ_MASK		BIT(28)
+#define ENA_ETH_IO_TX_META_DESC_REQ_ID_HI_MASK		GENMASK(6, 0)
+#define ENA_ETH_IO_TX_META_DESC_L3_HDR_LEN_MASK		GENMASK(8, 0)
+#define ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_SHIFT		8
+#define ENA_ETH_IO_TX_META_DESC_L3_HDR_OFF_MASK		GENMASK(16, 8)
+#define ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_SHIFT		16
+#define ENA_ETH_IO_TX_META_DESC_L4_HDR_LEN_IN_WORDS_MASK		GENMASK(22, 16)
+#define ENA_ETH_IO_TX_META_DESC_MSS_LO_SHIFT		22
+#define ENA_ETH_IO_TX_META_DESC_MSS_LO_MASK		GENMASK(32, 22)
+#define ENA_ETH_IO_TX_META_DESC_CRYPTO_INFO_MASK		GENMASK(24, 0)
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_SHIFT		24
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_HDR_LEN_WORDS_MASK		GENMASK(29, 24)
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_SHIFT		29
+#define ENA_ETH_IO_TX_META_DESC_OUTR_L3_OFF_LO_MASK		GENMASK(32, 29)
+
+/* tx_cdesc */
+#define ENA_ETH_IO_TX_CDESC_PHASE_MASK		BIT(0)
+
+/* rx_desc */
+#define ENA_ETH_IO_RX_DESC_PHASE_MASK		BIT(0)
+#define ENA_ETH_IO_RX_DESC_FIRST_SHIFT		2
+#define ENA_ETH_IO_RX_DESC_FIRST_MASK		BIT(2)
+#define ENA_ETH_IO_RX_DESC_LAST_SHIFT		3
+#define ENA_ETH_IO_RX_DESC_LAST_MASK		BIT(3)
+#define ENA_ETH_IO_RX_DESC_COMP_REQ_SHIFT		4
+#define ENA_ETH_IO_RX_DESC_COMP_REQ_MASK		BIT(4)
+
+/* rx_cdesc_base */
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_PROTO_IDX_MASK		GENMASK(5, 0)
+#define ENA_ETH_IO_RX_CDESC_BASE_SRC_VLAN_CNT_SHIFT		5
+#define ENA_ETH_IO_RX_CDESC_BASE_SRC_VLAN_CNT_MASK		GENMASK(7, 5)
+#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_SHIFT		7
+#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_MASK		BIT(7)
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_SHIFT		8
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_PROTO_IDX_MASK		GENMASK(13, 8)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_SHIFT		13
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM_ERR_MASK		BIT(13)
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_SHIFT		14
+#define ENA_ETH_IO_RX_CDESC_BASE_L4_CSUM_ERR_MASK		BIT(14)
+#define ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_SHIFT		15
+#define ENA_ETH_IO_RX_CDESC_BASE_IPV4_FRAG_MASK		BIT(15)
+#define ENA_ETH_IO_RX_CDESC_BASE_SECURED_PKT_SHIFT		20
+#define ENA_ETH_IO_RX_CDESC_BASE_SECURED_PKT_MASK		BIT(20)
+#define ENA_ETH_IO_RX_CDESC_BASE_CRYPTO_STATUS_SHIFT		21
+#define ENA_ETH_IO_RX_CDESC_BASE_CRYPTO_STATUS_MASK		GENMASK(23, 21)
+#define ENA_ETH_IO_RX_CDESC_BASE_PHASE_SHIFT		24
+#define ENA_ETH_IO_RX_CDESC_BASE_PHASE_MASK		BIT(24)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM2_SHIFT		25
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_CSUM2_MASK		BIT(25)
+#define ENA_ETH_IO_RX_CDESC_BASE_FIRST_SHIFT		26
+#define ENA_ETH_IO_RX_CDESC_BASE_FIRST_MASK		BIT(26)
+#define ENA_ETH_IO_RX_CDESC_BASE_LAST_SHIFT		27
+#define ENA_ETH_IO_RX_CDESC_BASE_LAST_MASK		BIT(27)
+#define ENA_ETH_IO_RX_CDESC_BASE_INR_L4_CSUM_SHIFT		28
+#define ENA_ETH_IO_RX_CDESC_BASE_INR_L4_CSUM_MASK		BIT(28)
+#define ENA_ETH_IO_RX_CDESC_BASE_BUFFER_SHIFT		30
+#define ENA_ETH_IO_RX_CDESC_BASE_BUFFER_MASK		BIT(30)
+#define ENA_ETH_IO_RX_CDESC_BASE_TUNNEL_OFF_MASK		GENMASK(9, 0)
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_OFF_SHIFT		9
+#define ENA_ETH_IO_RX_CDESC_BASE_L3_OFF_MASK		GENMASK(16, 9)
+#define ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_SHIFT		16
+#define ENA_ETH_IO_RX_CDESC_BASE_HASH_FRAG_CSUM_MASK		GENMASK(32, 16)
+
+#endif /*_ENA_ETH_IO_H_ */
diff --git a/drivers/amazon/ena/ena_gen_info.h b/drivers/amazon/ena/ena_gen_info.h
new file mode 100644
index 0000000..cacfa40
--- /dev/null
+++ b/drivers/amazon/ena/ena_gen_info.h
@@ -0,0 +1,37 @@
+/******************************************************************************
+Copyright (C) 2015 Annapurna Labs Ltd.
+
+This file may be licensed under the terms of the Annapurna Labs Commercial
+License Agreement.
+
+Alternatively, this file can be distributed under the terms of the GNU General
+Public License V2 as published by the Free Software Foundation and can be
+found at http://www.gnu.org/licenses/gpl-2.0.html
+
+Alternatively, redistribution and use in source and binary forms, with or
+without modification, are permitted provided that the following conditions are
+met:
+
+    *  Redistributions of source code must retain the above copyright notice,
+this list of conditions and the following disclaimer.
+
+    *  Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+******************************************************************************/
+
+#define	ENA_GEN_DATE	"Thu Nov 19 17:16:56 IST 2015"
+#define	ENA_GEN_COMMIT	"de30f60"
diff --git a/drivers/amazon/ena/ena_includes.h b/drivers/amazon/ena/ena_includes.h
new file mode 100644
index 0000000..5ea312f
--- /dev/null
+++ b/drivers/amazon/ena/ena_includes.h
@@ -0,0 +1,4 @@
+#include "ena_common_defs.h"
+#include "ena_regs_defs.h"
+#include "ena_admin_defs.h"
+#include "ena_eth_io_defs.h"
diff --git a/drivers/amazon/ena/ena_netdev.c b/drivers/amazon/ena/ena_netdev.c
new file mode 100644
index 0000000..07f4a0e
--- /dev/null
+++ b/drivers/amazon/ena/ena_netdev.c
@@ -0,0 +1,2956 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/ethtool.h>
+#include <linux/pci.h>
+#include <linux/if_vlan.h>
+#include <linux/cpu_rmap.h>
+#include <net/ip.h>
+
+#include "ena_netdev.h"
+#include "ena_sysfs.h"
+
+#include "ena_pci_id_tbl.h"
+
+#define DRV_MODULE_NAME		"ena"
+#ifndef DRV_MODULE_VERSION
+#define DRV_MODULE_VERSION      "0.2"
+#endif
+#define DRV_MODULE_RELDATE      "OCT 14, 2015"
+
+#define DEVICE_NAME	"Elastic Network Adapter (ENA)"
+
+static char version[] =
+		DEVICE_NAME DRV_MODULE_NAME " v"
+		DRV_MODULE_VERSION " (" DRV_MODULE_RELDATE ")\n";
+
+MODULE_AUTHOR("Amazon.com, Inc. or its affiliates");
+MODULE_DESCRIPTION(DEVICE_NAME);
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_MODULE_VERSION);
+
+/* Time in jiffies before concluding the transmitter is hung. */
+#define TX_TIMEOUT  (5 * HZ)
+
+#define ENA_NAPI_BUDGET 64
+
+#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK)
+static int debug = -1;
+module_param(debug, int, 0);
+MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
+
+static int push_mode = 3;
+module_param(push_mode, int, 0);
+MODULE_PARM_DESC(push_mode, "1 - Don't push anything to the device memory.\n2 - Push the header buffer to the dev memory.\n3 - Push descriptors and header buffer to the dev memory. (default)\n");
+
+static struct ena_aenq_handlers aenq_handlers;
+
+MODULE_DEVICE_TABLE(pci, ena_pci_tbl);
+
+static void ena_tx_timeout(struct net_device *dev)
+{
+	struct ena_adapter *adapter = netdev_priv(dev);
+
+	if (netif_msg_tx_err(adapter))
+		netdev_err(dev, "transmit timed out\n");
+}
+
+static void update_rx_ring_mtu(struct ena_adapter *adapter, int mtu)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		adapter->rx_ring[i].mtu = mtu;
+}
+
+static int ena_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct ena_adapter *adapter = netdev_priv(dev);
+	int ret;
+
+	if (new_mtu > adapter->max_mtu) {
+		netdev_err(dev,
+			   "Invalid MTU setting. new_mtu: %d\n", new_mtu);
+		return -EINVAL;
+	}
+
+	ret = ena_com_set_dev_mtu(adapter->ena_dev, new_mtu);
+	if (!ret) {
+		netdev_dbg(adapter->netdev, "set MTU to %d\n", new_mtu);
+		update_rx_ring_mtu(adapter, new_mtu);
+		dev->mtu = new_mtu;
+	} else {
+		netdev_err(adapter->netdev, "Failed to set MTU to %d\n",
+			   new_mtu);
+	}
+
+	return ret;
+}
+
+static int ena_init_rx_cpu_rmap(struct ena_adapter *adapter)
+{
+	u32 i;
+	int rc;
+
+	adapter->netdev->rx_cpu_rmap = alloc_irq_cpu_rmap(adapter->num_queues);
+	if (!adapter->netdev->rx_cpu_rmap)
+		return -ENOMEM;
+	for (i = 0; i < adapter->num_queues; i++) {
+		int irq_idx = ENA_IO_IRQ_IDX(i);
+
+		rc = irq_cpu_rmap_add(adapter->netdev->rx_cpu_rmap,
+				      adapter->msix_entries[irq_idx].vector);
+		if (rc) {
+			free_irq_cpu_rmap(adapter->netdev->rx_cpu_rmap);
+			adapter->netdev->rx_cpu_rmap = NULL;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static void ena_init_io_rings(struct ena_adapter *adapter)
+{
+	struct ena_com_dev *ena_dev;
+	struct ena_ring *ring;
+	int i;
+
+	ena_dev = adapter->ena_dev;
+
+	/* TX */
+	for (i = 0; i < adapter->num_queues; i++) {
+		ring = &adapter->tx_ring[i];
+
+		ring->pdev = adapter->pdev;
+		ring->dev = &adapter->pdev->dev;
+		ring->netdev = adapter->netdev;
+		ring->napi = &adapter->ena_napi[i].napi;
+		ring->ring_size = adapter->tx_ring_size;
+		ring->qid = i;
+
+		ring->tx_mem_queue_type = ena_dev->tx_mem_queue_type;
+	}
+
+	/* RX */
+	for (i = 0; i < adapter->num_queues; i++) {
+		ring = &adapter->rx_ring[i];
+
+		ring->pdev = adapter->pdev;
+		ring->dev = &adapter->pdev->dev;
+		ring->netdev = adapter->netdev;
+		ring->napi = &adapter->ena_napi[i].napi;
+		ring->ring_size = adapter->rx_ring_size;
+		ring->rx_small_copy_len = adapter->small_copy_len;
+		ring->qid = i;
+	}
+}
+
+/**
+ * ena_setup_tx_resources - allocate I/O Tx resources (Descriptors)
+ * @adapter: network interface device structure
+ * @qid: queue index
+ *
+ * Return 0 on success, negative on failure
+ **/
+static int ena_setup_tx_resources(struct ena_adapter *adapter, int qid)
+{
+	struct ena_ring *tx_ring = &adapter->tx_ring[qid];
+	struct device *dev;
+	int size, i;
+
+	dev = &adapter->pdev->dev;
+
+	if ((tx_ring->tx_buffer_info) || (tx_ring->rx_buffer_info))
+		return -EEXIST;
+
+	size = sizeof(struct ena_tx_buffer) * tx_ring->ring_size;
+
+	tx_ring->tx_buffer_info = devm_kzalloc(dev, size, GFP_KERNEL);
+	if (!tx_ring->tx_buffer_info)
+		return -ENOMEM;
+
+	size = sizeof(u16) * tx_ring->ring_size;
+	tx_ring->free_tx_ids = devm_kzalloc(dev, size, GFP_KERNEL);
+	if (!tx_ring->free_tx_ids) {
+		devm_kfree(dev, tx_ring->tx_buffer_info);
+		return -ENOMEM;
+	}
+
+	/* Req id ring for TX out of order completions */
+	for (i = 0; i < tx_ring->ring_size; i++)
+		tx_ring->free_tx_ids[i] = i;
+
+	tx_ring->next_to_use = 0;
+	tx_ring->next_to_clean = 0;
+	return 0;
+}
+
+/**
+ * ena_free_tx_resources - Free I/O Tx Resources per Queue
+ * @adapter: network interface device structure
+ * @qid: queue index
+ *
+ * Free all transmit software resources
+ **/
+static void ena_free_tx_resources(struct ena_adapter *adapter, int qid)
+{
+	struct ena_ring *tx_ring = &adapter->tx_ring[qid];
+	struct device *dev = &adapter->pdev->dev;
+
+	netdev_dbg(adapter->netdev, "%s qid %d\n", __func__, qid);
+
+	devm_kfree(dev, tx_ring->tx_buffer_info);
+	tx_ring->tx_buffer_info = NULL;
+
+	devm_kfree(dev, tx_ring->free_tx_ids);
+	tx_ring->free_tx_ids = NULL;
+}
+
+/**
+ * ena_setup_all_tx_resources - allocate I/O Tx queues resources for All queues
+ * @adapter: private structure
+ *
+ * Return 0 on success, negative on failure
+ **/
+static int ena_setup_all_tx_resources(struct ena_adapter *adapter)
+{
+	int i, rc = 0;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_setup_tx_resources(adapter, i);
+		if (rc)
+			goto err_setup_tx;
+	}
+
+	return 0;
+
+err_setup_tx:
+
+	netdev_err(adapter->netdev, "Allocation for Tx Queue %u failed\n", i);
+
+	/* rewind the index freeing the rings as we go */
+	while (i--)
+		ena_free_tx_resources(adapter, i);
+	return rc;
+}
+
+/**
+ * ena_free_all_io_tx_resources - Free I/O Tx Resources for All Queues
+ * @adapter: board private structure
+ *
+ * Free all transmit software resources
+ **/
+static void ena_free_all_io_tx_resources(struct ena_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		ena_free_tx_resources(adapter, i);
+}
+
+/**
+ * ena_setup_rx_resources - allocate I/O Rx resources (Descriptors)
+ * @adapter: network interface device structure
+ * @qid: queue index
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int ena_setup_rx_resources(struct ena_adapter *adapter,
+				  u32 qid)
+{
+	struct ena_ring *rx_ring = &adapter->rx_ring[qid];
+	int size;
+
+	if (rx_ring->rx_buffer_info)
+		return -EEXIST;
+
+	size = sizeof(struct ena_rx_buffer) * rx_ring->ring_size;
+
+	/* alloc extra element so in rx path
+	 * we can always prefetch rx_info + 1
+	 */
+	size += sizeof(struct ena_rx_buffer);
+
+	rx_ring->rx_buffer_info = devm_kzalloc(&adapter->pdev->dev,
+					       size, GFP_KERNEL);
+	if (!rx_ring->rx_buffer_info)
+		return -ENOMEM;
+
+	rx_ring->next_to_clean = 0;
+	rx_ring->next_to_use = 0;
+
+	return 0;
+}
+
+/**
+ * ena_free_rx_resources - Free I/O Rx Resources
+ * @adapter: network interface device structure
+ * @qid: queue index
+ *
+ * Free all receive software resources
+ */
+static void ena_free_rx_resources(struct ena_adapter *adapter,
+				  u32 qid)
+{
+	struct ena_ring *rx_ring = &adapter->rx_ring[qid];
+
+	devm_kfree(&adapter->pdev->dev, rx_ring->rx_buffer_info);
+	rx_ring->rx_buffer_info = NULL;
+}
+
+/**
+ * ena_setup_all_rx_resources - allocate I/O Rx queues resources for all queues
+ * @adapter: board private structure
+ *
+ * Return 0 on success, negative on failure
+ **/
+static int ena_setup_all_rx_resources(struct ena_adapter *adapter)
+{
+	int i, rc = 0;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_setup_rx_resources(adapter, i);
+		if (rc)
+			goto err_setup_rx;
+	}
+
+	return 0;
+
+err_setup_rx:
+
+	netdev_err(adapter->netdev, "Allocation for Rx Queue %u failed\n", i);
+
+	/* rewind the index freeing the rings as we go */
+	while (i--)
+		ena_free_rx_resources(adapter, i);
+	return rc;
+}
+
+/**
+ * ena_free_all_io_rx_resources - Free I/O Rx Resources for All Queues
+ * @adapter: board private structure
+ *
+ * Free all receive software resources
+ **/
+static void ena_free_all_rx_resources(struct ena_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		ena_free_rx_resources(adapter, i);
+}
+
+static inline int ena_alloc_rx_frag(struct ena_ring *rx_ring,
+				    struct ena_rx_buffer *rx_info)
+{
+	struct ena_com_buf *ena_buf;
+	dma_addr_t dma;
+	u8 *data;
+	u32 frame_size;
+
+	/* if previous allocated frag is not used */
+	if (rx_info->data)
+		return 0;
+
+	/* Limit the buffer to 1 page */
+	frame_size = (rx_ring->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);
+	rx_info->data_size = min_t(u32, frame_size, PAGE_SIZE);
+
+	rx_info->data_size = max_t(u32,
+				   rx_info->data_size,
+				   ENA_DEFAULT_MIN_RX_BUFF_ALLOC_SIZE);
+
+	rx_info->frag_size =
+		SKB_DATA_ALIGN(rx_info->data_size + NET_IP_ALIGN) +
+		SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+	data = netdev_alloc_frag(rx_info->frag_size);
+
+	if (unlikely(!data)) {
+		rx_ring->alloc_fail_cnt++;
+		return -ENOMEM;
+	}
+
+	dma = dma_map_single(rx_ring->dev, data + NET_IP_ALIGN,
+			     rx_info->data_size, DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(rx_ring->dev, dma))) {
+		put_page(virt_to_head_page(data));
+		return -EIO;
+	}
+	netdev_dbg(rx_ring->netdev,
+		   "alloc frag %p, rx_info %p len %x skb size %x\n", data,
+		   rx_info, rx_info->data_size, rx_info->frag_size);
+
+	rx_info->data = data;
+
+	rx_info->page = virt_to_head_page(rx_info->data);
+	rx_info->page_offset = (uintptr_t)rx_info->data
+		- (uintptr_t)page_address(rx_info->page);
+	ena_buf = &rx_info->ena_buf;
+	ena_buf->paddr = dma;
+	ena_buf->len = rx_info->data_size;
+	return 0;
+}
+
+static void ena_free_rx_frag(struct ena_ring *rx_ring,
+			     struct ena_rx_buffer *rx_info)
+{
+	u8 *data = rx_info->data;
+	struct ena_com_buf *ena_buf = &rx_info->ena_buf;
+
+	if (!data)
+		return;
+
+	dma_unmap_single(rx_ring->dev, ena_buf->paddr,
+			 rx_info->data_size, DMA_FROM_DEVICE);
+
+	put_page(virt_to_head_page(data));
+	rx_info->data = NULL;
+}
+
+static int ena_refill_rx_bufs(struct ena_ring *rx_ring, u32 num)
+{
+	u16 next_to_use;
+	u32 i;
+	int rc;
+
+	next_to_use = rx_ring->next_to_use;
+
+	for (i = 0; i < num; i++) {
+		struct ena_rx_buffer *rx_info =
+			&rx_ring->rx_buffer_info[next_to_use];
+
+		if (unlikely(
+			ena_alloc_rx_frag(rx_ring, rx_info) < 0)) {
+			netdev_warn(rx_ring->netdev,
+				    "failed to alloc buffer for rx queue %d\n",
+				    rx_ring->qid);
+			break;
+		}
+		rc = ena_com_add_single_rx_desc(rx_ring->ena_com_io_sq,
+						&rx_info->ena_buf);
+		if (unlikely(rc)) {
+			netdev_warn(rx_ring->netdev,
+				    "failed to add buffer for rx queue %d\n",
+				    rx_ring->qid);
+			break;
+		}
+		next_to_use = ENA_RX_RING_IDX_NEXT(next_to_use,
+						   rx_ring->ring_size);
+	}
+
+	if (unlikely(i < num)) {
+		netdev_warn(rx_ring->netdev,
+			    "refilled rx qid %d with only %d buffers (from %d)\n",
+			    rx_ring->qid, i, num);
+	}
+
+	if (likely(i)) {
+		/* Add memory barrier to make sure the desc were written before
+		 * issue a doorbell
+		 */
+		wmb();
+		ena_com_write_sq_doorbell(rx_ring->ena_com_io_sq);
+	}
+
+	rx_ring->next_to_use = next_to_use;
+
+	return i;
+}
+
+static void ena_free_rx_bufs(struct ena_adapter *adapter,
+			     u32 qid)
+{
+	struct ena_ring *rx_ring = &adapter->rx_ring[qid];
+	u32 i;
+
+	for (i = 0; i < rx_ring->ring_size; i++) {
+		struct ena_rx_buffer *rx_info = &rx_ring->rx_buffer_info[i];
+
+		if (rx_info->data)
+			ena_free_rx_frag(rx_ring, rx_info);
+	}
+}
+
+/**
+ * ena_refill_all_rx_bufs - allocate all queues Rx buffers
+ * @adapter: board private structure
+ *
+ **/
+static void ena_refill_all_rx_bufs(struct ena_adapter *adapter)
+{
+	struct ena_ring *rx_ring;
+	int i, rc, bufs_num;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		rx_ring = &adapter->rx_ring[i];
+		bufs_num = rx_ring->ring_size - 1;
+		rc = ena_refill_rx_bufs(rx_ring, bufs_num);
+
+		if (unlikely(rc != bufs_num))
+			netdev_warn(adapter->netdev,
+				    "refilling Queue %d failed. allocated %d buffers from: %d\n",
+				    i, rc, bufs_num);
+	}
+}
+
+static void ena_free_all_rx_bufs(struct ena_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		ena_free_rx_bufs(adapter, i);
+}
+
+/**
+ * ena_free_tx_bufs - Free Tx Buffers per Queue
+ * @adapter: network interface device structure
+ * @qid: queue index
+ **/
+static void ena_free_tx_bufs(struct ena_ring *tx_ring)
+{
+	u32 i;
+
+	for (i = 0; i < tx_ring->ring_size; i++) {
+		struct ena_tx_buffer *tx_info = &tx_ring->tx_buffer_info[i];
+		struct ena_com_buf *ena_buf;
+		int nr_frags;
+		int j;
+
+		if (!tx_info->skb)
+			continue;
+
+		netdev_notice(tx_ring->netdev,
+			      "free uncompleted tx skb qid %d idx 0x%x\n",
+			      tx_ring->qid, i);
+
+		ena_buf = tx_info->bufs;
+		dma_unmap_single(tx_ring->dev,
+				 ena_buf->paddr,
+				 ena_buf->len,
+				 DMA_TO_DEVICE);
+
+		/* unmap remaining mapped pages */
+		nr_frags = tx_info->num_of_bufs - 1;
+		for (j = 0; j < nr_frags; j++) {
+			ena_buf++;
+			dma_unmap_page(tx_ring->dev,
+				       ena_buf->paddr,
+				       ena_buf->len,
+				       DMA_TO_DEVICE);
+		}
+
+		dev_kfree_skb_any(tx_info->skb);
+	}
+	netdev_tx_reset_queue(netdev_get_tx_queue(tx_ring->netdev,
+						  tx_ring->qid));
+}
+
+static void ena_free_all_tx_bufs(struct ena_adapter *adapter)
+{
+	struct ena_ring *tx_ring;
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		tx_ring = &adapter->tx_ring[i];
+		ena_free_tx_bufs(tx_ring);
+	}
+}
+
+static void ena_destroy_all_tx_queues(struct ena_adapter *adapter)
+{
+	u16 ena_qid;
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		ena_qid = ENA_IO_TXQ_IDX(i);
+		ena_com_destroy_io_queue(adapter->ena_dev, ena_qid);
+	}
+}
+
+static void ena_destroy_all_rx_queues(struct ena_adapter *adapter)
+{
+	u16 ena_qid;
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		ena_qid = ENA_IO_RXQ_IDX(i);
+		ena_com_destroy_io_queue(adapter->ena_dev, ena_qid);
+	}
+}
+
+static void ena_destroy_all_io_queues(struct ena_adapter *adapter)
+{
+	ena_destroy_all_tx_queues(adapter);
+	ena_destroy_all_rx_queues(adapter);
+}
+
+static int ena_clean_tx_irq(struct ena_ring *tx_ring,
+			    u32 budget)
+{
+	struct netdev_queue *txq;
+	bool above_thresh;
+	u32 tx_bytes = 0;
+	u32 total_done = 0;
+	u16 next_to_clean;
+	u16 req_id;
+	int tx_pkts = 0;
+	int rc;
+
+	next_to_clean = tx_ring->next_to_clean;
+	txq = netdev_get_tx_queue(tx_ring->netdev, tx_ring->qid);
+
+	while (tx_pkts < budget) {
+		struct ena_tx_buffer *tx_info;
+		struct sk_buff *skb;
+		struct ena_com_buf *ena_buf;
+		int i, nr_frags;
+
+		rc = ena_com_tx_comp_req_id_get(tx_ring->ena_com_io_cq,
+						&req_id);
+		if (rc)
+			break;
+
+		tx_info = &tx_ring->tx_buffer_info[req_id];
+		skb = tx_info->skb;
+
+		ENA_ASSERT(skb, "SKB is NULL\n");
+
+		/* prefetch skb_end_pointer() to speedup skb_shinfo(skb) */
+		prefetch(&skb->end);
+
+		tx_info->skb = NULL;
+
+		if (likely(tx_info->num_of_bufs != 0)) {
+			ena_buf = tx_info->bufs;
+
+			dma_unmap_single(tx_ring->dev,
+					 dma_unmap_addr(ena_buf, paddr),
+					 dma_unmap_len(ena_buf, len),
+					 DMA_TO_DEVICE);
+
+			/* unmap remaining mapped pages */
+			nr_frags = tx_info->num_of_bufs - 1;
+			for (i = 0; i < nr_frags; i++) {
+				ena_buf++;
+				dma_unmap_page(tx_ring->dev,
+					       dma_unmap_addr(ena_buf, paddr),
+					       dma_unmap_len(ena_buf, len),
+					       DMA_TO_DEVICE);
+			}
+		}
+
+		dev_dbg(tx_ring->dev, "tx_poll: q %d skb %p completed\n",
+			tx_ring->qid, skb);
+
+		tx_bytes += skb->len;
+		dev_kfree_skb(skb);
+		tx_pkts++;
+		total_done += tx_info->tx_descs;
+
+		tx_ring->free_tx_ids[next_to_clean] = req_id;
+		next_to_clean = ENA_TX_RING_IDX_NEXT(next_to_clean,
+						     tx_ring->ring_size);
+	}
+
+	tx_ring->next_to_clean = next_to_clean;
+	ena_com_comp_ack(tx_ring->ena_com_io_sq, total_done);
+
+	netdev_tx_completed_queue(txq, tx_pkts, tx_bytes);
+
+	dev_dbg(tx_ring->dev, "tx_poll: q %d done. total pkts: %d\n",
+		tx_ring->qid, tx_pkts);
+
+	/* need to make the rings circular update visible to
+	 * ena_start_xmit() before checking for netif_queue_stopped().
+	 */
+	smp_mb();
+
+	above_thresh = ena_com_sq_empty_space(tx_ring->ena_com_io_sq) >
+		ENA_TX_WAKEUP_THRESH;
+	if (unlikely(netif_tx_queue_stopped(txq) && above_thresh)) {
+		/* TODO I am not sure if I need the tx queue lock.
+		 * Some drivers have this lock (like tg3) but others
+		 * (like mlx4) don't have it.
+		 * It will best if I can ask someone from the network
+		 * community. For now leave the lock
+		 */
+		__netif_tx_lock(txq, smp_processor_id());
+		above_thresh = ena_com_sq_empty_space(tx_ring->ena_com_io_sq) >
+			ENA_TX_WAKEUP_THRESH;
+		if (netif_tx_queue_stopped(txq) && above_thresh)
+			netif_tx_wake_queue(txq);
+		__netif_tx_unlock(txq);
+	}
+
+	return tx_pkts;
+}
+
+static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
+				  struct ena_com_buf *ena_bufs,
+				  u32 descs,
+				  u16 *next_to_clean)
+{
+	struct sk_buff *skb;
+	struct ena_rx_buffer *rx_info =
+		&rx_ring->rx_buffer_info[*next_to_clean];
+	u32 len;
+	u32 buf = 0;
+
+	ENA_ASSERT(rx_info->data, "Invalid alloc frag buffer\n");
+
+	len = ena_bufs[0].len;
+	netdev_dbg(rx_ring->netdev, "rx_info %p data %p\n", rx_info,
+		   rx_info->data);
+
+	ENA_ASSERT(len > 0, "pkt length is 0\n");
+
+	prefetch(rx_info->data + NET_IP_ALIGN);
+
+	if (len <= rx_ring->rx_small_copy_len) {
+		netdev_dbg(rx_ring->netdev, "rx small packet. len %d\n", len);
+
+		skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
+						rx_ring->rx_small_copy_len);
+		if (unlikely(!skb))
+			return NULL;
+
+		pci_dma_sync_single_for_cpu(rx_ring->pdev,
+					    dma_unmap_addr(&rx_info->ena_buf, paddr),
+					    len,
+					    DMA_FROM_DEVICE);
+		skb_copy_to_linear_data(skb, rx_info->data + NET_IP_ALIGN, len);
+		pci_dma_sync_single_for_device(rx_ring->pdev,
+					       dma_unmap_addr(&rx_info->ena_buf, paddr),
+					       len,
+					       DMA_FROM_DEVICE);
+		skb_put(skb, len);
+		skb->protocol = eth_type_trans(skb, rx_ring->netdev);
+		*next_to_clean = ENA_RX_RING_IDX_NEXT(*next_to_clean,
+						      rx_ring->ring_size);
+		return skb;
+	}
+
+	dma_unmap_single(rx_ring->dev, dma_unmap_addr(&rx_info->ena_buf, paddr),
+			 rx_info->data_size, DMA_FROM_DEVICE);
+
+	skb = napi_get_frags(rx_ring->napi);
+	if (unlikely(!skb))
+		return NULL;
+
+	skb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags, rx_info->page,
+			   rx_info->page_offset + NET_IP_ALIGN, len);
+
+	skb->len += len;
+	skb->data_len += len;
+	skb->truesize += len;
+
+	netdev_dbg(rx_ring->netdev, "rx skb updated. len %d. data_len %d\n",
+		   skb->len, skb->data_len);
+
+	rx_info->data = NULL;
+	*next_to_clean = ENA_RX_RING_IDX_NEXT(*next_to_clean,
+					      rx_ring->ring_size);
+
+	while (--descs) {
+		rx_info = &rx_ring->rx_buffer_info[*next_to_clean];
+		len = ena_bufs[++buf].len;
+
+		dma_unmap_single(rx_ring->dev,
+				 dma_unmap_addr(&rx_info->ena_buf, paddr),
+				 rx_info->data_size, DMA_FROM_DEVICE);
+
+		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_info->page,
+				rx_info->page_offset + NET_IP_ALIGN, len,
+				rx_info->data_size);
+
+		netdev_dbg(rx_ring->netdev, "rx skb updated. len %d. data_len %d\n",
+			   skb->len, skb->data_len);
+
+		rx_info->data = NULL;
+
+		*next_to_clean = ENA_RX_RING_IDX_NEXT(*next_to_clean,
+						      rx_ring->ring_size);
+	}
+
+	return skb;
+}
+
+/**
+ * ena_rx_checksum - indicate in skb if hw indicated a good cksum
+ * @adapter: structure containing adapter specific data
+ * @hal_pkt: HAL structure for the packet
+ * @skb: skb currently being received and modified
+ **/
+static inline void ena_rx_checksum(struct ena_ring *rx_ring,
+				   struct ena_com_rx_ctx *ena_rx_ctx,
+				   struct sk_buff *skb)
+{
+	/* Rx csum disabled */
+	if (unlikely(!(rx_ring->netdev->features & NETIF_F_RXCSUM))) {
+		skb->ip_summed = CHECKSUM_NONE;
+		return;
+	}
+
+	/* For fragmented packets the checksum isn't valid */
+	if (ena_rx_ctx->frag) {
+		skb->ip_summed = CHECKSUM_NONE;
+		return;
+	}
+
+	/* if IP and error */
+	if (unlikely((ena_rx_ctx->l3_proto == ena_eth_io_l3_proto_ipv4) &&
+		     (ena_rx_ctx->l3_csum_err))) {
+		/* ipv4 checksum error */
+		skb->ip_summed = CHECKSUM_NONE;
+		rx_ring->bad_checksum++;
+		netdev_err(rx_ring->netdev, "rx ipv4 header checksum error\n");
+		return;
+	}
+
+	/* if TCP/UDP */
+	if (likely((ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_tcp) ||
+		   (ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_udp))) {
+		if (unlikely(ena_rx_ctx->l4_csum_err)) {
+			/* TCP/UDP checksum error */
+			rx_ring->bad_checksum++;
+			netdev_err(rx_ring->netdev, "rx L4 checksum error\n");
+			skb->ip_summed = CHECKSUM_NONE;
+			return;
+		}
+
+		if (!ena_rx_ctx->frag)
+			skb->ip_summed = CHECKSUM_NONE;
+		else
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+	}
+}
+
+static void ena_set_rx_hash(struct ena_ring *rx_ring,
+			    struct ena_com_rx_ctx *ena_rx_ctx,
+			    struct sk_buff *skb)
+{
+	enum pkt_hash_types hash_type;
+
+	if (likely(rx_ring->netdev->features & NETIF_F_RXHASH)) {
+		if (likely((ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_tcp) ||
+			   (ena_rx_ctx->l4_proto == ena_eth_io_l4_proto_udp)))
+
+			hash_type = PKT_HASH_TYPE_L4;
+		else
+			hash_type = PKT_HASH_TYPE_NONE;
+
+		/* Override hash type if the packet is fragmented */
+		if (ena_rx_ctx->frag)
+			hash_type = PKT_HASH_TYPE_NONE;
+
+		skb_set_hash(skb, ena_rx_ctx->hash_frag_csum, hash_type);
+	}
+}
+
+/**
+ * ena_clean_rx_irq - Cleanup RX irq
+ * @napi: napi handler
+ * @rx_refill_needed: return if refill is required.
+ * @budget: how many packets driver is allowed to clean
+ *
+ * This function the number of cleaned buffers.
+ **/
+static int ena_clean_rx_irq(struct ena_ring *rx_ring,
+			    struct napi_struct *napi,
+			    u32 budget)
+{
+	u16 next_to_clean = rx_ring->next_to_clean;
+	u32 descs, res_budget, work_done;
+
+	struct ena_com_rx_ctx ena_rx_ctx;
+	struct sk_buff *skb;
+	int refill_required;
+	int refill_threshold;
+
+	netdev_dbg(rx_ring->netdev, "%s qid %d\n", __func__, rx_ring->qid);
+	res_budget = budget;
+
+	do {
+		descs = 0;
+		ena_rx_ctx.ena_bufs = rx_ring->ena_bufs;
+		ena_rx_ctx.max_bufs = ENA_PKT_MAX_BUFS;
+		descs = ena_com_rx_pkt(rx_ring->ena_com_io_cq,
+				       rx_ring->ena_com_io_sq,
+				       &ena_rx_ctx);
+		if (unlikely(descs == 0))
+			break;
+
+		netdev_dbg(rx_ring->netdev,
+			   "rx_poll: q %d got packet from ena. descs #: %d l3 proto %d l4 proto %d frag_pkt: %d\n",
+			   rx_ring->qid, descs, ena_rx_ctx.l3_proto,
+			   ena_rx_ctx.l4_proto, ena_rx_ctx.hash_frag_csum);
+
+		/* allocate skb and fill it */
+		skb = ena_rx_skb(rx_ring, rx_ring->ena_bufs, descs,
+				 &next_to_clean);
+
+		/* exit if we failed to retrieve a buffer */
+		if (unlikely(!skb)) {
+			next_to_clean = ENA_RX_RING_IDX_ADD(next_to_clean,
+							    descs,
+							    rx_ring->ring_size);
+			rx_ring->alloc_fail_cnt++;
+			break;
+		}
+
+		ena_rx_checksum(rx_ring, &ena_rx_ctx, skb);
+
+		ena_set_rx_hash(rx_ring, &ena_rx_ctx, skb);
+
+		skb_record_rx_queue(skb, rx_ring->qid);
+
+		if (rx_ring->ena_bufs[0].len <= rx_ring->rx_small_copy_len)
+			napi_gro_receive(napi, skb);
+		else
+			napi_gro_frags(napi);
+
+		res_budget--;
+	} while (likely(res_budget));
+
+	work_done = budget - res_budget;
+
+	rx_ring->next_to_clean = next_to_clean;
+
+	refill_required = ena_com_sq_empty_space(rx_ring->ena_com_io_sq);
+	refill_threshold = rx_ring->ring_size / ENA_RX_REFILL_THRESH_DEVIDER;
+
+	/* Optimization, try to batch new rx buffers */
+	if (refill_required > refill_threshold)
+		ena_refill_rx_bufs(rx_ring, refill_required);
+
+	return work_done;
+}
+
+static int ena_io_poll(struct napi_struct *napi, int budget)
+{
+	struct ena_napi *ena_napi = container_of(napi, struct ena_napi, napi);
+	struct ena_ring *tx_ring, *rx_ring;
+	u32 tx_work_done;
+	u32 rx_work_done;
+	int tx_budget;
+
+	tx_ring = ena_napi->tx_ring;
+	rx_ring = ena_napi->rx_ring;
+
+	tx_budget = tx_ring->ring_size / ENA_TX_POLL_BUDGET_DEVIDER;
+
+	tx_work_done = ena_clean_tx_irq(tx_ring, tx_budget);
+	rx_work_done = ena_clean_rx_irq(rx_ring, napi, budget);
+
+	if ((budget > rx_work_done) && (tx_budget > tx_work_done)) {
+		napi_complete(napi);
+
+		/* Tx and Rx share the same interrupt vector so the driver
+		 *  can unmask either of the interrupts
+		 */
+		ena_com_unmask_intr(rx_ring->ena_com_io_cq);
+
+		return rx_work_done;
+	}
+
+	return budget;
+}
+
+static irqreturn_t ena_intr_msix_mgmnt(int irq, void *data)
+{
+	struct ena_adapter *adapter = (struct ena_adapter *)data;
+
+	ena_com_admin_q_comp_intr_handler(adapter->ena_dev);
+	ena_com_aenq_intr_handler(adapter->ena_dev, data);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * ena_intr_msix_io - MSIX Interrupt Handler for Tx/Rx
+ * @irq: interrupt number
+ * @data: pointer to a network interface private napi device structure
+ **/
+static irqreturn_t ena_intr_msix_io(int irq, void *data)
+{
+	struct ena_napi *ena_napi = data;
+
+	napi_schedule(&ena_napi->napi);
+
+	return IRQ_HANDLED;
+}
+
+static int ena_enable_msix(struct ena_adapter *adapter,
+			   int num_queues)
+{
+	int i, msix_vecs, rc;
+
+	if (adapter->msix_enabled) {
+		dev_err(&adapter->pdev->dev,
+			"Error, MSI-X is already enabled\n");
+		return -EPERM;
+	}
+
+	/* Reserved the max msix vectors we might need */
+	msix_vecs = ENA_MAX_MSIX_VEC(num_queues);
+
+	dev_dbg(&adapter->pdev->dev, "trying to enable MSIX, vectors %d\n",
+		msix_vecs);
+
+	adapter->msix_entries = devm_kzalloc(&adapter->pdev->dev,
+					     msix_vecs * sizeof(struct msix_entry),
+					     GFP_KERNEL);
+
+	if (!adapter->msix_entries) {
+		dev_err(&adapter->pdev->dev,
+			"failed to allocate msix_entries, vectors %d\n",
+			msix_vecs);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < msix_vecs; i++)
+		adapter->msix_entries[i].entry = i;
+
+	rc = pci_enable_msix(adapter->pdev, adapter->msix_entries, msix_vecs);
+	if (rc != 0) {
+		dev_err(&adapter->pdev->dev,
+			"failed to enable MSIX, vectors %d rc %d\n",
+			msix_vecs, rc);
+		return -ENOSPC;
+	}
+
+	dev_dbg(&adapter->pdev->dev,
+		"enable MSIX, vectors %d\n", msix_vecs);
+
+	if (msix_vecs >= 1) {
+		if (ena_init_rx_cpu_rmap(adapter))
+			dev_warn(&adapter->pdev->dev,
+				 "failed to map irqs to cpus\n");
+	}
+
+	adapter->msix_vecs = msix_vecs;
+	adapter->msix_enabled = true;
+
+	return 0;
+}
+
+static void ena_setup_mgmnt_intr(struct ena_adapter *adapter)
+{
+	u32 cpu;
+
+	snprintf(adapter->irq_tbl[ENA_MGMNT_IRQ_IDX].name,
+		 ENA_IRQNAME_SIZE, "ena-mgmnt@pci:%s",
+		 pci_name(adapter->pdev));
+	adapter->irq_tbl[ENA_MGMNT_IRQ_IDX].handler =
+		ena_intr_msix_mgmnt;
+	adapter->irq_tbl[ENA_MGMNT_IRQ_IDX].data = adapter;
+	adapter->irq_tbl[ENA_MGMNT_IRQ_IDX].vector =
+		adapter->msix_entries[ENA_MGMNT_IRQ_IDX].vector;
+	cpu = cpumask_first(cpu_online_mask);
+	cpumask_set_cpu(cpu,
+			&adapter->irq_tbl[ENA_MGMNT_IRQ_IDX].affinity_hint_mask);
+}
+
+static void ena_setup_io_intr(struct ena_adapter *adapter)
+{
+	struct net_device *netdev;
+	int irq_idx, i;
+
+	netdev = adapter->netdev;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		irq_idx = ENA_IO_IRQ_IDX(i);
+
+		snprintf(adapter->irq_tbl[irq_idx].name, ENA_IRQNAME_SIZE,
+			 "%s-Tx-Rx-%d", netdev->name, i);
+		adapter->irq_tbl[irq_idx].handler = ena_intr_msix_io;
+		adapter->irq_tbl[irq_idx].data = &adapter->ena_napi[i];
+		adapter->irq_tbl[irq_idx].vector =
+			adapter->msix_entries[irq_idx].vector;
+
+		cpumask_set_cpu(i % num_online_cpus(),
+				&adapter->irq_tbl[irq_idx].affinity_hint_mask);
+	}
+}
+
+static int ena_request_mgmnt_irq(struct ena_adapter *adapter)
+{
+	unsigned long flags = 0;
+	struct ena_irq *irq;
+	int rc;
+
+	irq = &adapter->irq_tbl[ENA_MGMNT_IRQ_IDX];
+	rc = request_irq(irq->vector, irq->handler, flags, irq->name,
+			 irq->data);
+	if (rc) {
+		netdev_err(adapter->netdev, "failed to request admin irq\n");
+		return rc;
+	}
+
+		netdev_dbg(adapter->netdev,
+			   "set affinity hint of mgmnt irq.to 0x%lx (irq vector: %d)\n",
+			   irq->affinity_hint_mask.bits[0], irq->vector);
+
+		irq_set_affinity_hint(irq->vector, &irq->affinity_hint_mask);
+
+	return rc;
+}
+
+static int ena_request_io_irq(struct ena_adapter *adapter)
+{
+	unsigned long flags = 0;
+	struct ena_irq *irq;
+	int rc = 0, i, k;
+
+	if (!adapter->msix_enabled) {
+		netdev_err(adapter->netdev, "failed to request irq\n");
+		return -EINVAL;
+	}
+
+	for (i = ENA_IO_IRQ_FIRST_IDX; i < adapter->msix_vecs; i++) {
+		irq = &adapter->irq_tbl[i];
+		rc = request_irq(irq->vector, irq->handler, flags, irq->name,
+				 irq->data);
+		if (rc) {
+			netdev_err(adapter->netdev,
+				   "failed to request irq. index %d rc %d\n",
+				   i, rc);
+			goto err;
+		}
+
+		netdev_dbg(adapter->netdev,
+			   "set affinity hint of irq. index %d to 0x%lx (irq vector: %d)\n",
+			   i, irq->affinity_hint_mask.bits[0], irq->vector);
+
+		irq_set_affinity_hint(irq->vector, &irq->affinity_hint_mask);
+	}
+
+	return rc;
+
+err:
+	for (k = ENA_IO_IRQ_FIRST_IDX; k < i; k++) {
+		irq = &adapter->irq_tbl[k];
+		free_irq(irq->vector, irq->data);
+	}
+
+	return rc;
+}
+
+static void ena_free_mgmnt_irq(struct ena_adapter *adapter)
+{
+	struct ena_irq *irq;
+
+	irq = &adapter->irq_tbl[ENA_MGMNT_IRQ_IDX];
+	synchronize_irq(irq->vector);
+		irq_set_affinity_hint(irq->vector, NULL);
+	free_irq(irq->vector, irq->data);
+}
+
+static void ena_free_io_irq(struct ena_adapter *adapter)
+{
+	struct ena_irq *irq;
+	int i;
+
+	if (adapter->msix_vecs >= 1) {
+		free_irq_cpu_rmap(adapter->netdev->rx_cpu_rmap);
+		adapter->netdev->rx_cpu_rmap = NULL;
+	}
+
+	for (i = ENA_IO_IRQ_FIRST_IDX; i < adapter->msix_vecs; i++) {
+		irq = &adapter->irq_tbl[i];
+		irq_set_affinity_hint(irq->vector, NULL);
+		free_irq(irq->vector, irq->data);
+	}
+}
+
+static void ena_disable_msix(struct ena_adapter *adapter)
+{
+	if (adapter->msix_enabled)
+		pci_disable_msix(adapter->pdev);
+
+	adapter->msix_enabled = false;
+
+	if (adapter->msix_entries)
+		devm_kfree(&adapter->pdev->dev, adapter->msix_entries);
+	adapter->msix_entries = NULL;
+}
+
+static void ena_disable_io_intr_sync(struct ena_adapter *adapter)
+{
+	int i;
+
+	if (!netif_running(adapter->netdev))
+		return;
+
+	for (i = ENA_IO_IRQ_FIRST_IDX; i < adapter->msix_vecs; i++)
+		synchronize_irq(adapter->irq_tbl[i].vector);
+}
+
+static void ena_del_napi(struct ena_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		netif_napi_del(&adapter->ena_napi[i].napi);
+}
+
+static void ena_init_napi(struct ena_adapter *adapter)
+{
+	struct ena_napi *napi;
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		napi = &adapter->ena_napi[i];
+
+		netif_napi_add(adapter->netdev,
+			       &adapter->ena_napi[i].napi,
+			       ena_io_poll,
+			       ENA_NAPI_BUDGET);
+		napi->rx_ring = &adapter->rx_ring[i];
+		napi->tx_ring = &adapter->tx_ring[i];
+		napi->qid = i;
+	}
+}
+
+static void ena_napi_disable_all(struct ena_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		napi_disable(&adapter->ena_napi[i].napi);
+}
+
+static void ena_napi_enable_all(struct ena_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		napi_enable(&adapter->ena_napi[i].napi);
+}
+
+static void ena_restore_ethtool_params(struct ena_adapter *adapter)
+{
+	adapter->tx_usecs = 0;
+	adapter->rx_usecs = 0;
+	adapter->tx_frames = 1;
+	adapter->rx_frames = 1;
+}
+
+static void ena_up_complete(struct ena_adapter *adapter)
+{
+	int i;
+
+	ena_init_napi(adapter);
+
+	ena_change_mtu(adapter->netdev, adapter->netdev->mtu);
+
+	ena_refill_all_rx_bufs(adapter);
+
+	/* enable transmits */
+	netif_tx_start_all_queues(adapter->netdev);
+
+	ena_restore_ethtool_params(adapter);
+
+	ena_napi_enable_all(adapter);
+
+	/* schedule napi in case we had pending packets
+	 * from the last time we disable napi
+	 */
+	for (i = 0; i < adapter->num_queues; i++)
+		napi_schedule(&adapter->ena_napi[i].napi);
+}
+
+static int ena_create_io_tx_queue(struct ena_adapter *adapter, int qid)
+{
+	struct ena_com_dev *ena_dev;
+	struct ena_ring *tx_ring;
+	u32 msix_vector;
+	u16 ena_qid;
+	int rc;
+
+	ena_dev = adapter->ena_dev;
+
+	tx_ring = &adapter->tx_ring[qid];
+	msix_vector = ENA_IO_IRQ_IDX(qid);
+	ena_qid = ENA_IO_TXQ_IDX(qid);
+
+	rc = ena_com_create_io_queue(ena_dev, ena_qid,
+				     ENA_COM_IO_QUEUE_DIRECTION_TX,
+				     ena_dev->tx_mem_queue_type, msix_vector,
+				     adapter->tx_ring_size);
+	if (rc) {
+		netdev_err(adapter->netdev,
+			   "failed to create io TX queue num %d rc: %d\n",
+			   qid, rc);
+		return rc;
+	}
+
+	rc = ena_com_get_io_handlers(ena_dev, ena_qid,
+				     &tx_ring->ena_com_io_sq,
+				     &tx_ring->ena_com_io_cq);
+	if (rc) {
+		netdev_err(adapter->netdev,
+			   "failed to get tx queue handlers. TX queue num %d rc: %d\n",
+			   qid, rc);
+		ena_com_destroy_io_queue(ena_dev, ena_qid);
+	}
+
+	return rc;
+}
+
+static int ena_create_all_io_tx_queues(struct ena_adapter *adapter)
+{
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	int rc, i;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_create_io_tx_queue(adapter, i);
+		if (rc)
+			goto create_err;
+	}
+
+	return 0;
+
+create_err:
+	while (i--)
+		ena_com_destroy_io_queue(ena_dev, ENA_IO_TXQ_IDX(i));
+
+	return rc;
+}
+
+static int ena_create_io_rx_queue(struct ena_adapter *adapter, int qid)
+{
+	struct ena_com_dev *ena_dev;
+	struct ena_ring *rx_ring;
+	u32 msix_vector;
+	u16 ena_qid;
+	int rc;
+
+	rx_ring = &adapter->rx_ring[qid];
+	msix_vector = ENA_IO_IRQ_IDX(qid);
+	ena_qid = ENA_IO_RXQ_IDX(qid);
+
+	ena_dev = adapter->ena_dev;
+
+	rc = ena_com_create_io_queue(adapter->ena_dev, ena_qid,
+				     ENA_COM_IO_QUEUE_DIRECTION_RX,
+				     ENA_MEM_QUEUE_TYPE_HOST_MEMORY,
+				     msix_vector,
+				     adapter->rx_ring_size);
+	if (rc) {
+		pr_err("failed to create io RX queue  num %d rc: %d\n",
+		       qid, rc);
+		return rc;
+	}
+
+	rc = ena_com_get_io_handlers(ena_dev, ena_qid,
+				     &rx_ring->ena_com_io_sq,
+				     &rx_ring->ena_com_io_cq);
+	if (rc) {
+		netdev_err(adapter->netdev,
+			   "failed to get rx queue handlers. RX queue  num %d rc: %d\n",
+			   qid, rc);
+		ena_com_destroy_io_queue(ena_dev, ena_qid);
+		return rc;
+	}
+
+	return 0;
+}
+
+static int ena_create_all_io_rx_queues(struct ena_adapter *adapter)
+{
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	int rc, i;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_create_io_rx_queue(adapter, i);
+		if (rc)
+			goto create_err;
+	}
+
+	return 0;
+
+create_err:
+
+	while (i--)
+		ena_com_destroy_io_queue(ena_dev, ENA_IO_RXQ_IDX(i));
+
+	return rc;
+}
+
+static int ena_up(struct ena_adapter *adapter)
+{
+	int rc;
+
+	netdev_dbg(adapter->netdev, "%s\n", __func__);
+
+	ena_setup_io_intr(adapter);
+
+	rc = ena_request_io_irq(adapter);
+	if (rc)
+		goto err_req_irq;
+
+	/* allocate transmit descriptors */
+	rc = ena_setup_all_tx_resources(adapter);
+	if (rc)
+		goto err_setup_tx;
+
+	/* allocate receive descriptors */
+	rc = ena_setup_all_rx_resources(adapter);
+	if (rc)
+		goto err_setup_rx;
+
+	/* Create TX queues */
+	rc = ena_create_all_io_tx_queues(adapter);
+	if (rc)
+		goto err_create_tx_queues;
+
+	/* Create RX queues */
+	rc = ena_create_all_io_rx_queues(adapter);
+	if (rc)
+		goto err_create_rx_queues;
+
+	if (adapter->link_status)
+		netif_carrier_on(adapter->netdev);
+
+	ena_up_complete(adapter);
+
+	adapter->up = true;
+
+	return rc;
+
+err_create_rx_queues:
+	ena_destroy_all_tx_queues(adapter);
+err_create_tx_queues:
+	ena_free_all_rx_resources(adapter);
+err_setup_rx:
+	ena_free_all_io_tx_resources(adapter);
+err_setup_tx:
+	ena_free_io_irq(adapter);
+err_req_irq:
+
+	return rc;
+}
+
+#if 0
+static int
+ena_flow_steer(struct net_device *netdev, const struct sk_buff *skb,
+	       u16 rxq_index, u32 flow_id)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	int rc = 0;
+
+	if ((skb->protocol != htons(ETH_P_IP)) &&
+	    (skb->protocol != htons(ETH_P_IPV6)))
+		return -EPROTONOSUPPORT;
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (ip_is_fragment(ip_hdr(skb)))
+			return -EPROTONOSUPPORT;
+		if ((ip_hdr(skb)->protocol != IPPROTO_TCP) &&
+		    (ip_hdr(skb)->protocol != IPPROTO_UDP))
+			return -EPROTONOSUPPORT;
+	}
+
+	if (skb->protocol == htons(ETH_P_IPV6)) {
+		/* ipv6 with extension not supported yet */
+		if ((ipv6_hdr(skb)->nexthdr != IPPROTO_TCP) &&
+		    (ipv6_hdr(skb)->nexthdr != IPPROTO_UDP))
+			return -EPROTONOSUPPORT;
+	}
+	rc = flow_id & (ENA_RX_THASH_TABLE_SIZE - 1);
+
+	adapter->rss_ind_tbl[rc] = rxq_index;
+	/* return the below func
+	 * ena_thash_table_set(&adapter->hal_adapter, rc,
+	 * adapter->udma_num, rxq_index);
+	 */
+	if (skb->protocol == htons(ETH_P_IP)) {
+		int nhoff = skb_network_offset(skb);
+		const struct iphdr *ip =
+			(const struct iphdr *)(skb->data + nhoff);
+		const __be16 *ports =
+			(const __be16 *)(skb->data + nhoff + 4 * ip->ihl);
+
+		netdev_info(adapter->netdev,
+			    "steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d]\n",
+			    (ip->protocol == IPPROTO_TCP) ? "TCP" : "UDP",
+			    &ip->saddr, ntohs(ports[0]),
+			    &ip->daddr, ntohs(ports[1]),
+			    rxq_index, flow_id, rc);
+	} else {
+		struct ipv6hdr *ip6h = ipv6_hdr(skb);
+		const __be16 *ports = (const __be16 *)skb_transport_header(skb);
+
+		netdev_info(adapter->netdev,
+			    "steering %s %pI6c:%u:%pI6c:%u to queue %u [flow %u filter %d]\n",
+			    (ipv6_hdr(skb)->nexthdr == IPPROTO_TCP) ? "TCP" : "UDP",
+			    &ip6h->saddr,
+			    ntohs(ports[0]),
+			    &ip6h->daddr,
+			    ntohs(ports[1]),
+			    rxq_index, flow_id, rc);
+	}
+
+	return rc;
+}
+#endif /* if 0 */
+static void ena_down(struct ena_adapter *adapter)
+{
+	netdev_info(adapter->netdev, "%s\n", __func__);
+
+	adapter->up = false;
+
+	netif_carrier_off(adapter->netdev);
+	netif_tx_disable(adapter->netdev);
+
+	ena_disable_io_intr_sync(adapter);
+	ena_napi_disable_all(adapter);
+	ena_free_io_irq(adapter);
+	ena_del_napi(adapter);
+
+	ena_destroy_all_io_queues(adapter);
+
+	ena_free_all_tx_bufs(adapter);
+	ena_free_all_rx_bufs(adapter);
+	ena_free_all_io_tx_resources(adapter);
+	ena_free_all_rx_resources(adapter);
+}
+
+/**
+ * ena_open - Called when a network interface is made active
+ * @netdev: network interface device structure
+ *
+ * Returns 0 on success, negative value on failure
+ *
+ * The open entry point is called when a network interface is made
+ * active by the system (IFF_UP).  At this point all resources needed
+ * for transmit and receive operations are allocated, the interrupt
+ * handler is registered with the OS, the watchdog timer is started,
+ * and the stack is notified that the interface is ready.
+ **/
+static int ena_open(struct net_device *netdev)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	int rc;
+
+	/* Notify the stack of the actual queue counts. */
+	rc = netif_set_real_num_tx_queues(netdev, adapter->num_queues);
+	if (rc)
+		return rc;
+
+	rc = netif_set_real_num_rx_queues(netdev, adapter->num_queues);
+	if (rc)
+		return rc;
+
+	rc = ena_up(adapter);
+	if (rc)
+		return rc;
+
+	return rc;
+}
+
+/**
+ * ena_close - Disables a network interface
+ * @netdev: network interface device structure
+ *
+ * Returns 0, this is not allowed to fail
+ *
+ * The close entry point is called when an interface is de-activated
+ * by the OS.  The hardware is still under the drivers control, but
+ * needs to be disabled.  A global MAC reset is issued to stop the
+ * hardware, and all transmit and receive resources are freed.
+ */
+static int ena_close(struct net_device *netdev)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	netdev_dbg(adapter->netdev, "%s\n", __func__);
+
+	if (adapter->up)
+		ena_down(adapter);
+
+	/*ena_release_hw_control(adapter);*/
+
+	return 0;
+}
+
+static int ena_get_settings(struct net_device *netdev,
+			    struct ethtool_cmd *ecmd)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	struct ena_admin_get_feature_link_desc *link;
+	struct ena_admin_get_feat_resp feat_resp;
+	int rc;
+
+	rc = ena_com_get_link_params(ena_dev, &feat_resp);
+	if (rc)
+		return rc;
+
+	link = &feat_resp.u.link;
+
+	ethtool_cmd_speed_set(ecmd, link->speed);
+
+	if (link->flags & ENA_ADMIN_GET_FEATURE_LINK_DESC_DUPLEX_MASK)
+		ecmd->duplex = DUPLEX_FULL;
+	else
+		ecmd->duplex = DUPLEX_HALF;
+
+	if (link->flags & ENA_ADMIN_GET_FEATURE_LINK_DESC_AUTONEG_MASK)
+		ecmd->autoneg = AUTONEG_ENABLE;
+	else
+		ecmd->autoneg = AUTONEG_DISABLE;
+
+	return 0;
+}
+
+static int ena_get_coalesce(struct net_device *net_dev,
+			    struct ethtool_coalesce *coalesce)
+{
+	struct ena_adapter *adapter = netdev_priv(net_dev);
+
+	coalesce->tx_coalesce_usecs = adapter->tx_usecs;
+	coalesce->rx_coalesce_usecs = adapter->rx_usecs;
+	coalesce->rx_max_coalesced_frames = adapter->rx_frames;
+	coalesce->tx_max_coalesced_frames = adapter->tx_frames;
+	coalesce->use_adaptive_rx_coalesce = false;
+
+	return 0;
+}
+
+static int ena_ethtool_set_coalesce(struct net_device *net_dev,
+				    struct ethtool_coalesce *coalesce)
+{
+	struct ena_adapter *adapter = netdev_priv(net_dev);
+	bool tx_enable = true;
+	bool rx_enable = true;
+	int i, rc;
+
+	/* The above params are unsupported by our driver */
+	if (coalesce->rx_coalesce_usecs_irq ||
+	    coalesce->rx_max_coalesced_frames_irq ||
+	    coalesce->tx_coalesce_usecs_irq ||
+	    coalesce->tx_max_coalesced_frames_irq ||
+	    coalesce->stats_block_coalesce_usecs ||
+	    coalesce->use_adaptive_rx_coalesce ||
+	    coalesce->use_adaptive_tx_coalesce ||
+	    coalesce->pkt_rate_low ||
+	    coalesce->rx_coalesce_usecs_low ||
+	    coalesce->rx_max_coalesced_frames_low ||
+	    coalesce->tx_coalesce_usecs_low ||
+	    coalesce->tx_max_coalesced_frames_low ||
+	    coalesce->pkt_rate_high ||
+	    coalesce->rx_coalesce_usecs_high ||
+	    coalesce->rx_max_coalesced_frames_high ||
+	    coalesce->tx_coalesce_usecs_high ||
+	    coalesce->tx_max_coalesced_frames_high ||
+	    coalesce->rate_sample_interval)
+		return -EINVAL;
+
+	if ((coalesce->rx_coalesce_usecs == 0) &&
+	    (coalesce->rx_max_coalesced_frames == 0))
+		return -EINVAL;
+
+	if ((coalesce->tx_coalesce_usecs == 0) &&
+	    (coalesce->tx_max_coalesced_frames == 0))
+		return -EINVAL;
+
+	if ((coalesce->rx_coalesce_usecs == 0) &&
+	    (coalesce->rx_max_coalesced_frames == 1))
+		rx_enable = false;
+
+	if ((coalesce->tx_coalesce_usecs == 0) &&
+	    (coalesce->tx_max_coalesced_frames == 1))
+		tx_enable = false;
+
+	/* Set tx interrupt moderation */
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_com_set_interrupt_moderation(adapter->ena_dev,
+						      ENA_IO_TXQ_IDX(i),
+						      tx_enable,
+						      coalesce->tx_max_coalesced_frames,
+						      coalesce->tx_coalesce_usecs);
+		if (rc) {
+			netdev_info(adapter->netdev,
+				    "setting interrupt moderation for TX queue %d failed. err: %d\n",
+				    i, rc);
+			goto err;
+		}
+	}
+
+	/* Set rx interrupt moderation */
+	for (i = 0; i < adapter->num_queues; i++) {
+		rc = ena_com_set_interrupt_moderation(adapter->ena_dev,
+						      ENA_IO_RXQ_IDX(i),
+						      rx_enable,
+						      coalesce->tx_max_coalesced_frames,
+						      coalesce->rx_coalesce_usecs);
+
+		if (rc) {
+			netdev_info(adapter->netdev,
+				    "setting interrupt moderation for RX queue %d failed. err: %d\n",
+				    i, rc);
+			goto err;
+		}
+	}
+
+	adapter->tx_usecs = coalesce->tx_coalesce_usecs;
+	adapter->rx_usecs = coalesce->rx_coalesce_usecs;
+
+	adapter->rx_frames = coalesce->rx_max_coalesced_frames_high;
+	adapter->tx_frames = coalesce->tx_max_coalesced_frames_high;
+
+	return 0;
+
+err:
+	return rc;
+}
+
+static int ena_nway_reset(struct net_device *netdev)
+{
+	return -ENODEV;
+}
+
+static u32 ena_get_msglevel(struct net_device *netdev)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	return adapter->msg_enable;
+}
+
+static void ena_set_msglevel(struct net_device *netdev, u32 value)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	adapter->msg_enable = value;
+}
+
+static struct rtnl_link_stats64 *ena_get_stats64(struct net_device *netdev,
+						 struct rtnl_link_stats64 *stats)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_admin_basic_stats ena_stats;
+	int rc;
+
+	if (!adapter->up)
+		return NULL;
+
+	rc = ena_com_get_dev_basic_stats(adapter->ena_dev, &ena_stats);
+	if (rc)
+		return NULL;
+
+	stats->tx_bytes = ((u64)ena_stats.tx_bytes_high << 32) |
+		ena_stats.tx_bytes_low;
+	stats->rx_bytes = ((u64)ena_stats.rx_bytes_high << 32) |
+		ena_stats.rx_bytes_low;
+
+	stats->rx_packets = ((u64)ena_stats.rx_pkts_high << 32) |
+		ena_stats.rx_pkts_low;
+	stats->tx_packets = ((u64)ena_stats.tx_pkts_high << 32) |
+		ena_stats.tx_pkts_low;
+
+	stats->rx_dropped = ((u64)ena_stats.rx_drops_high << 32) |
+		ena_stats.rx_drops_low;
+
+	stats->multicast = 0;
+	stats->collisions = 0;
+
+	stats->rx_length_errors = 0;
+	stats->rx_crc_errors = 0;
+	stats->rx_frame_errors = 0;
+	stats->rx_fifo_errors = 0;
+	stats->rx_missed_errors = 0;
+	stats->tx_window_errors = 0;
+
+	stats->rx_errors = 0;
+	stats->tx_errors = 0;
+
+	return stats;
+}
+
+static void ena_get_drvinfo(struct net_device *dev,
+			    struct ethtool_drvinfo *info)
+{
+	struct ena_adapter *adapter = netdev_priv(dev);
+
+	strlcpy(info->driver, DRV_MODULE_NAME, sizeof(info->driver));
+	strlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));
+	strlcpy(info->bus_info, pci_name(adapter->pdev),
+		sizeof(info->bus_info));
+}
+
+static void ena_get_ringparam(struct net_device *netdev,
+			      struct ethtool_ringparam *ring)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	struct ena_ring *tx_ring = &adapter->tx_ring[0];
+	struct ena_ring *rx_ring = &adapter->rx_ring[0];
+
+	ring->rx_max_pending = ENA_DEFAULT_RX_DESCS;
+	ring->tx_max_pending = ENA_DEFAULT_TX_DESCS;
+	ring->rx_pending = rx_ring->ring_size;
+	ring->tx_pending = tx_ring->ring_size;
+}
+
+static int ena_get_rxnfc(struct net_device *netdev,
+			 struct ethtool_rxnfc *info, u32 *rules __always_unused)
+{
+	/*struct ena_adapter *adapter = netdev_priv(netdev);*/
+
+	switch (info->cmd) {
+	case ETHTOOL_GRXRINGS:
+		info->data = ENA_MAX_NUM_IO_QUEUES;
+		return 0;
+	default:
+		netdev_err(netdev, "Command parameters not supported\n");
+		return -EOPNOTSUPP;
+	}
+}
+
+static u32 ena_get_rxfh_indir_size(struct net_device *netdev)
+{
+	/* TODO pending on rxfh defenition */
+	return ENA_RX_RSS_TABLE_SIZE;
+}
+
+static int ena_get_rxfh_indir(struct net_device *netdev,
+			      u32 *indir, u8 *key, u8 *hfunc)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	int i;
+
+	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++)
+		indir[i] = adapter->rss_ind_tbl[i];
+
+	return 0;
+}
+
+static int ena_set_rxfh_indir(struct net_device *netdev,
+			      const u32 *indir, const u8 *key,
+			      const u8 hfunc)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	size_t i;
+
+	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++) {
+		adapter->rss_ind_tbl[i] = indir[i];
+
+		/* TODO FSS table
+		 * al_eth_thash_table_set(&adapter->hal_adapter, i,
+		 * adapter->udma_num, indir[i]);
+		 */
+	}
+
+	return 0;
+}
+
+static void ena_get_channels(struct net_device *netdev,
+			     struct ethtool_channels *channels)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	channels->max_rx = ENA_MAX_NUM_IO_QUEUES;
+	channels->max_tx = ENA_MAX_NUM_IO_QUEUES;
+	channels->max_other = 0;
+	channels->max_combined = 0;
+	channels->rx_count = adapter->num_queues;
+	channels->tx_count = adapter->num_queues;
+	channels->other_count = 0;
+	channels->combined_count = 0;
+}
+
+static const struct ethtool_ops ena_ethtool_ops = {
+	.get_settings		= ena_get_settings,
+	.set_settings		= NULL,
+	.get_drvinfo		= ena_get_drvinfo,
+	.get_msglevel		= ena_get_msglevel,
+	.set_msglevel		= ena_set_msglevel,
+	.nway_reset		= ena_nway_reset,
+	.get_link		= ethtool_op_get_link,
+	.get_coalesce		= ena_get_coalesce,
+	.set_coalesce		= ena_ethtool_set_coalesce,
+	.get_ringparam		= ena_get_ringparam,
+	.get_rxnfc		= ena_get_rxnfc,
+	.get_rxfh_indir_size    = ena_get_rxfh_indir_size,
+	.get_rxfh		= ena_get_rxfh_indir,
+	.set_rxfh		= ena_set_rxfh_indir,
+	.get_channels		= ena_get_channels,
+};
+
+static void ena_tx_csum(struct ena_com_tx_ctx *ena_tx_ctx, struct sk_buff *skb)
+{
+	u32 mss = skb_shinfo(skb)->gso_size;
+	struct ena_com_tx_meta *ena_meta = &ena_tx_ctx->ena_meta;
+
+	if ((skb->ip_summed == CHECKSUM_PARTIAL) || mss) {
+		ena_tx_ctx->l4_csum_enable = true;
+		if (mss) {
+			ena_tx_ctx->tso_enable = true;
+			ena_meta->l4_hdr_len = tcp_hdr(skb)->doff;
+			ena_tx_ctx->l4_csum_partial = false;
+		} else {
+			ena_tx_ctx->tso_enable = false;
+			ena_meta->l4_hdr_len = 0;
+			ena_tx_ctx->l4_csum_partial = true;
+		}
+
+		switch (skb->protocol) {
+		case htons(ETH_P_IP):
+			ena_tx_ctx->l3_proto = ena_eth_io_l3_proto_ipv4;
+			if (mss)
+				ena_tx_ctx->l3_csum_enable = true;
+			if (ip_hdr(skb)->protocol == IPPROTO_TCP)
+				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_tcp;
+			else
+				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_udp;
+			break;
+		case htons(ETH_P_IPV6):
+			ena_tx_ctx->l3_proto = ena_eth_io_l3_proto_ipv6;
+			if (ip_hdr(skb)->protocol == IPPROTO_TCP)
+				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_tcp;
+			else
+				ena_tx_ctx->l4_proto = ena_eth_io_l4_proto_udp;
+			break;
+		default:
+			break;
+		}
+
+		ena_meta->mss = mss;
+		ena_meta->l3_hdr_len = skb_network_header_len(skb);
+		ena_meta->l3_hdr_offset = skb_network_offset(skb);
+		/* this param needed only for TSO with tunneling */
+		ena_meta->l3_outer_hdr_len = 0;
+		ena_meta->l3_outer_hdr_offset = 0;
+		ena_tx_ctx->meta_valid = true;
+
+	} else {
+		ena_tx_ctx->meta_valid = false;
+	}
+}
+
+/* Called with netif_tx_lock.
+ */
+static netdev_tx_t ena_start_xmit(struct sk_buff *skb,
+				  struct net_device *dev)
+{
+	struct ena_adapter *adapter = netdev_priv(dev);
+	struct ena_tx_buffer *tx_info;
+	struct ena_com_tx_ctx ena_tx_ctx;
+	struct ena_ring *tx_ring;
+	struct netdev_queue *txq;
+	struct ena_com_buf *ena_buf;
+	void *push_hdr;
+	u32 len, last_frag;
+	u16 next_to_use;
+	u16 req_id;
+	u16 push_len;
+	u16 header_len;
+	dma_addr_t dma;
+	int qid, rc, nb_hw_desc;
+	int i = 0;
+
+	netdev_dbg(adapter->netdev, "%s skb %p\n", __func__, skb);
+	/*  Determine which tx ring we will be placed on */
+	qid = skb_get_queue_mapping(skb);
+	tx_ring = &adapter->tx_ring[qid];
+	txq = netdev_get_tx_queue(dev, qid);
+
+	skb_tx_timestamp(skb);
+
+	len = skb_headlen(skb);
+
+	next_to_use = tx_ring->next_to_use;
+	req_id = tx_ring->free_tx_ids[next_to_use];
+	tx_info = &tx_ring->tx_buffer_info[req_id];
+	tx_info->num_of_bufs = 0;
+
+	ENA_ASSERT(!tx_info->skb, "SKB isn't NULL req_id %d\n", req_id);
+	ena_buf = tx_info->bufs;
+	tx_info->skb = skb;
+
+	if (tx_ring->tx_mem_queue_type == ENA_MEM_QUEUE_TYPE_DEVICE_MEMORY) {
+		/* prepared the push buffer */
+		push_len = min_t(u32, len, ENA_MAX_PUSH_PKT_SIZE);
+		header_len = push_len;
+		push_hdr = skb->data;
+	} else {
+		push_len = 0;
+		push_hdr = NULL;
+		header_len = min_t(u32, len, ENA_MAX_PUSH_PKT_SIZE);
+	}
+
+	pr_debug("skb: %p header_buf->vaddr: %p push_len: %d\n", skb,
+		 push_hdr, push_len);
+
+	if (len > push_len) {
+		dma = dma_map_single(tx_ring->dev, skb->data + push_len,
+				     len - push_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(tx_ring->dev, dma)) {
+			dev_kfree_skb(skb);
+			netdev_warn(adapter->netdev, "failed to map skb\n");
+			return NETDEV_TX_OK;
+		}
+		ena_buf->paddr = dma;
+		ena_buf->len = len - push_len;
+
+		ena_buf++;
+		tx_info->num_of_bufs++;
+	}
+
+	last_frag = skb_shinfo(skb)->nr_frags;
+	if (unlikely(last_frag > (ENA_PKT_MAX_BUFS - 2))) {
+		netdev_err(adapter->netdev,
+			   "too many descriptors. last_frag %d!\n", last_frag);
+		for (i = 0; i <= last_frag; i++)
+			netdev_err(adapter->netdev,
+				   "frag[%d]: addr:0x%llx, len 0x%x\n", i,
+				   (unsigned long long)tx_info->bufs[i].paddr,
+				   tx_info->bufs[i].len);
+		goto dma_error;
+	}
+
+	for (i = 0; i < last_frag; i++) {
+		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+
+		len = skb_frag_size(frag);
+		dma = skb_frag_dma_map(tx_ring->dev, frag, 0, len,
+				       DMA_TO_DEVICE);
+		if (dma_mapping_error(tx_ring->dev, dma))
+			goto dma_error;
+		ena_buf->paddr = dma;
+		ena_buf->len = len;
+		ena_buf++;
+	}
+
+	tx_info->num_of_bufs += last_frag;
+
+	netdev_tx_sent_queue(txq, skb->len);
+
+	memset(&ena_tx_ctx, 0x0, sizeof(struct ena_com_tx_ctx));
+	ena_tx_ctx.ena_bufs = tx_info->bufs;
+	ena_tx_ctx.push_header = push_hdr;
+	ena_tx_ctx.num_bufs = tx_info->num_of_bufs;
+	ena_tx_ctx.req_id = req_id;
+	ena_tx_ctx.header_len = header_len;
+
+	/* set flags and meta data */
+	ena_tx_csum(&ena_tx_ctx, skb);
+
+	/* prepare the packet's descriptors to dma engine */
+	rc = ena_com_prepare_tx(tx_ring->ena_com_io_sq, &ena_tx_ctx,
+				&nb_hw_desc);
+
+	if (unlikely(rc)) {
+		netdev_err(adapter->netdev, "failed to prepare tx bufs\n");
+		netif_tx_stop_queue(txq);
+		goto dma_error;
+	}
+	tx_info->tx_descs = nb_hw_desc;
+
+	tx_ring->next_to_use = ENA_TX_RING_IDX_NEXT(next_to_use,
+		tx_ring->ring_size);
+
+	/* This WMB is aimed to:
+	 * 1 - perform smp barrier before reading next_to_completion
+	 * 2 - make sure the desc were written before trigger DB
+	 */
+	wmb();
+
+	/* stop the queue when no more space available, the packet can have up
+	 * to MAX_SKB_FRAGS + 1 buffers and a meta descriptor
+	 */
+	if (unlikely(ena_com_sq_empty_space(tx_ring->ena_com_io_sq)
+		< (MAX_SKB_FRAGS + 2))) {
+		dev_dbg(&adapter->pdev->dev, "%s stop queue %d\n",
+			__func__, qid);
+
+		netif_tx_stop_queue(txq);
+
+		/* There is a rare condition where this function decide to
+		 * stop the queue but meanwhile clean_tx_irq updates
+		 * next_to_completion and terminates.
+		 * The queue will remine close forever.
+		 * To solve this issue this function perform rmb, check
+		 * the wakeup condition and wake up the queue if needed.
+		 */
+		smp_rmb();
+
+		if (ena_com_sq_empty_space(tx_ring->ena_com_io_sq)
+				> ENA_TX_WAKEUP_THRESH)
+			netif_tx_wake_queue(txq);
+	}
+
+	if (netif_xmit_stopped(txq) || !skb->xmit_more)
+		/* trigger the dma engine */
+		ena_com_write_sq_doorbell(tx_ring->ena_com_io_sq);
+
+	return NETDEV_TX_OK;
+
+dma_error:
+	/* save value of frag that failed */
+	last_frag = i;
+
+	/* start back at beginning and unmap skb */
+	tx_info->skb = NULL;
+	ena_buf = tx_info->bufs;
+	dma_unmap_single(tx_ring->dev, dma_unmap_addr(ena_buf, paddr),
+			 dma_unmap_len(ena_buf, len), DMA_TO_DEVICE);
+
+	/* unmap remaining mapped pages */
+	for (i = 0; i < last_frag; i++) {
+		ena_buf++;
+		dma_unmap_page(tx_ring->dev, dma_unmap_addr(ena_buf, paddr),
+			       dma_unmap_len(ena_buf, len), DMA_TO_DEVICE);
+	}
+
+	dev_kfree_skb(skb);
+	return NETDEV_TX_BUSY;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void ena_netpoll(struct net_device *netdev)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	int i;
+
+	for (i = 0; i < adapter->num_queues; i++)
+		napi_schedule(&adapter->ena_napi[i].napi);
+}
+#endif /* CONFIG_NET_POLL_CONTROLLER */
+
+/* Return subqueue id on this core (one per core). */
+static u16 ena_select_queue(struct net_device *dev, struct sk_buff *skb
+	, void *accel_priv
+	, select_queue_fallback_t fallback
+	)
+{
+	u16 qid;
+	/* we suspect that this is good for in--kernel network services that
+	 * want to loop incoming skb rx to tx in normal user generated traffic,
+	 * most probably we will not get to this
+	 */
+	if (skb_rx_queue_recorded(skb))
+		qid = skb_get_rx_queue(skb);
+	else
+		qid = fallback(dev, skb);
+
+	return qid;
+}
+
+static const struct net_device_ops ena_netdev_ops = {
+	.ndo_open		= ena_open,
+	.ndo_stop		= ena_close,
+	.ndo_start_xmit		= ena_start_xmit,
+	.ndo_select_queue	= ena_select_queue,
+	.ndo_get_stats64	= ena_get_stats64,
+	.ndo_tx_timeout		= ena_tx_timeout,
+	.ndo_change_mtu		= ena_change_mtu,
+	.ndo_set_mac_address	= NULL,
+	.ndo_validate_addr	= eth_validate_addr,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= ena_netpoll,
+#endif /* CONFIG_NET_POLL_CONTROLLER */
+#if 0
+	.ndo_rx_flow_steer      = ena_flow_steer,
+#endif
+};
+
+static void ena_device_io_suspend(struct work_struct *work)
+{
+	struct ena_adapter *adapter =
+		container_of(work, struct ena_adapter, suspend_io_task);
+
+	struct net_device *netdev = adapter->netdev;
+
+	/* ena_napi_disable_all disable only the IO handeling.
+	 * We are still subject to AENQ keep alive watchdog.
+	 */
+	ena_napi_disable_all(adapter);
+	netif_tx_lock(netdev);
+	netif_device_detach(netdev);
+	netif_tx_unlock(netdev);
+}
+
+static void ena_device_io_resume(struct work_struct *work)
+{
+	struct ena_adapter *adapter =
+		container_of(work, struct ena_adapter, resume_io_task);
+
+	struct net_device *netdev = adapter->netdev;
+
+	netif_device_attach(netdev);
+	ena_napi_enable_all(adapter);
+}
+
+static int ena_device_validate_params(struct ena_adapter *adapter,
+				      struct ena_com_dev_get_features_ctx
+				      *get_feat_ctx)
+{
+	int rc;
+	struct device *dev = &adapter->pdev->dev;
+	struct net_device *netdev = adapter->netdev;
+
+	rc = ether_addr_equal(get_feat_ctx->dev_attr.mac_addr,
+			      adapter->mac_addr);
+	if (!rc) {
+		dev_err(dev, "Error, mac address are different\n");
+		return -1;
+	}
+
+	if ((get_feat_ctx->max_queues.max_cq_num < adapter->num_queues) ||
+	    (get_feat_ctx->max_queues.max_cq_num < adapter->num_queues)) {
+		dev_err(dev, "Error, device doesn't support enough queues\n");
+		return -1;
+	}
+
+	if (get_feat_ctx->dev_attr.max_mtu < netdev->mtu) {
+		dev_err(dev,
+			"Error, device max mtu is smaller than netdev MTU\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+static int ena_device_init(struct ena_com_dev *ena_dev, struct pci_dev *pdev,
+			   struct ena_com_dev_get_features_ctx *get_feat_ctx)
+{
+	struct device *dev = &pdev->dev;
+	int dma_width;
+	int rc;
+
+	rc = ena_com_mmio_reg_read_request_init(ena_dev);
+	if (rc) {
+		dev_err(dev, "failed to init mmio read less\n");
+		return rc;
+	}
+
+	rc = ena_com_dev_reset(ena_dev);
+	if (rc) {
+		dev_err(dev, "Can not reset device\n");
+		goto err_mmio_read_less;
+	}
+
+	rc = ena_com_validate_version(ena_dev);
+	if (rc) {
+		dev_err(dev, "device version is too low\n");
+		goto err_mmio_read_less;
+	}
+
+	dma_width = ena_com_get_dma_width(ena_dev);
+
+	rc = pci_set_dma_mask(pdev, DMA_BIT_MASK(dma_width));
+	if (rc) {
+		dev_err(dev, "pci_set_dma_mask failed 0x%x\n", rc);
+		goto err_mmio_read_less;
+	}
+
+	rc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(dma_width));
+	if (rc) {
+		dev_err(dev, "err_pci_set_consistent_dma_mask failed 0x%x\n",
+			rc);
+		goto err_mmio_read_less;
+	}
+
+	/* ENA admin level init */
+	rc = ena_com_admin_init(ena_dev, &aenq_handlers, true);
+	if (rc) {
+		dev_err(dev,
+			"Can not initialize ena admin queue with device\n");
+		goto err_mmio_read_less;
+	}
+
+	/* To enable the msix interrupts the driver needs to know the number
+	 * of queues. So the driver uses polling mode to retrieve this
+	 * information
+	 */
+	ena_com_set_admin_polling_mode(ena_dev, true);
+
+	/* Get Device Attributes*/
+	rc = ena_com_get_dev_attr_feat(ena_dev, get_feat_ctx);
+	if (rc) {
+		dev_err(dev,
+			"Cannot get attribute for ena device rc= %d\n", rc);
+		goto err_admin_init;
+	}
+
+	return 0;
+
+err_admin_init:
+	ena_com_admin_destroy(ena_dev);
+err_mmio_read_less:
+	ena_com_mmio_reg_read_request_destroy(ena_dev);
+
+	return rc;
+}
+
+static int ena_enable_msix_and_set_admin_interrupts(struct ena_adapter *adapter,
+						    int io_vectors)
+{
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	struct device *dev = &adapter->pdev->dev;
+	int rc;
+
+	rc = ena_enable_msix(adapter, io_vectors);
+	if (rc) {
+		dev_err(dev, "Can not reserve msix vectors\n");
+		return rc;
+	}
+
+	ena_setup_mgmnt_intr(adapter);
+
+	rc = ena_request_mgmnt_irq(adapter);
+	if (rc) {
+		dev_err(dev, "Can not setup management interrupts\n");
+		goto err_disable_msix;
+	}
+
+	ena_com_set_admin_polling_mode(ena_dev, false);
+
+	ena_com_admin_aenq_enable(ena_dev);
+
+	return 0;
+
+err_disable_msix:
+	ena_disable_msix(adapter);
+
+	return rc;
+}
+
+static void ena_fw_reset_device(struct work_struct *work)
+{
+	struct ena_com_dev_get_features_ctx get_feat_ctx;
+	struct ena_adapter *adapter =
+		container_of(work, struct ena_adapter, reset_task);
+	struct net_device *netdev = adapter->netdev;
+	struct ena_com_dev *ena_dev = adapter->ena_dev;
+	struct pci_dev *pdev = adapter->pdev;
+	bool dev_up;
+	int rc;
+
+	del_timer_sync(&adapter->watchdog_timer);
+
+	rtnl_lock();
+
+	dev_up = adapter->up;
+
+	ena_sysfs_terminate(&pdev->dev);
+
+	ena_com_set_admin_running_state(ena_dev, false);
+
+	/* After calling ena_close the tx queues and the napi
+	 * are disabled so no one can interfere or touch the
+	 * data structures
+	 */
+	ena_close(netdev);
+
+	rc = ena_com_dev_reset(ena_dev);
+	if (rc) {
+		dev_err(&pdev->dev, "Device reset failed\n");
+		goto err;
+	}
+
+	ena_free_mgmnt_irq(adapter);
+
+	ena_disable_msix(adapter);
+
+	ena_com_abort_admin_commands(ena_dev);
+
+	ena_com_wait_for_abort_completion(ena_dev);
+
+	ena_com_admin_destroy(ena_dev);
+
+	ena_com_mmio_reg_read_request_destroy(ena_dev);
+
+	/* Finish with the destroy part. Start the init part */
+
+	rc = ena_device_init(ena_dev, adapter->pdev, &get_feat_ctx);
+	if (rc) {
+		dev_err(&pdev->dev, "Can not init device\n");
+		goto err;
+	}
+
+	rc = ena_device_validate_params(adapter, &get_feat_ctx);
+	if (rc) {
+		dev_err(&pdev->dev, "Can not reserve msix vectors\n");
+		goto err_device_destroy;
+	}
+
+	rc = ena_enable_msix_and_set_admin_interrupts(adapter,
+						      adapter->num_queues);
+	if (rc) {
+		dev_err(&pdev->dev, "enable misx failed\n");
+		goto err_device_destroy;
+	}
+
+	rc = ena_sysfs_init(&pdev->dev);
+	if (rc) {
+		dev_err(&pdev->dev, "Cannot init sysfs\n");
+		goto err_disable_msix;
+	}
+
+	/* If the interface was up before the reset bring it up */
+	if (dev_up) {
+		rc = ena_up(adapter);
+		if (rc) {
+			dev_err(&pdev->dev, "Failed to create io queues\n");
+			goto err_sysfs_terminate;
+		}
+	}
+
+	mod_timer(&adapter->watchdog_timer,
+		  round_jiffies(jiffies + ENA_DEVICE_KALIVE_TIMEOUT));
+
+	rtnl_unlock();
+
+	dev_err(&pdev->dev, "Device reset completed successfully\n");
+
+	return;
+
+err_sysfs_terminate:
+	ena_sysfs_terminate(&pdev->dev);
+err_disable_msix:
+	ena_free_mgmnt_irq(adapter);
+	ena_disable_msix(adapter);
+err_device_destroy:
+	ena_com_admin_destroy(ena_dev);
+err:
+	rtnl_unlock();
+
+	dev_err(&pdev->dev,
+		"Reset attempt failed. Can not reset the device\n");
+}
+
+static void ena_watchdog_expire(unsigned long data)
+{
+	struct ena_adapter *adapter = (struct ena_adapter *)data;
+
+	netdev_err(adapter->netdev, "[%s] ERROR!!! WD expired\n",
+		   __func__);
+
+	INIT_WORK(&adapter->reset_task, ena_fw_reset_device);
+	schedule_work(&adapter->reset_task);
+}
+
+static int ena_calc_io_queue_num(struct pci_dev *pdev,
+				 struct ena_com_dev_get_features_ctx
+				 *get_feat_ctx)
+{
+	int io_queue_num;
+
+	io_queue_num = min_t(int, num_possible_cpus(), ENA_MAX_NUM_IO_QUEUES);
+	io_queue_num = min_t(int, io_queue_num,
+			     get_feat_ctx->max_queues.max_sq_num);
+	io_queue_num = min_t(int, io_queue_num,
+			     get_feat_ctx->max_queues.max_cq_num);
+	/* 1 IRQ for for mgmnt and 1 IRQs for each IO direction */
+	io_queue_num = min_t(int, io_queue_num, pci_msix_vec_count(pdev) - 1);
+
+	ENA_ASSERT(io_queue_num > 0, "Invalid queue number: %d\n",
+		   io_queue_num);
+
+	return io_queue_num;
+}
+
+static int ena_set_push_mode(struct ena_com_dev *ena_dev)
+{
+	if ((push_mode == 0) || (push_mode > ENA_MEM_QUEUE_TYPE_MAX_TYPES))
+		return -1;
+
+	ena_dev->tx_mem_queue_type =
+		(enum ena_com_memory_queue_type)push_mode;
+
+	return 0;
+}
+
+void ena_set_dev_offloads(struct ena_com_dev_get_features_ctx *feat,
+			  struct net_device *netdev)
+{
+	netdev_features_t dev_features = 0;
+
+	/* Set offload features */
+	if (feat->offload.tx &
+		ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV4_CSUM_PART_MASK)
+		dev_features |= NETIF_F_IP_CSUM;
+
+	if (feat->offload.tx &
+		ENA_ADMIN_FEATURE_OFFLOAD_DESC_TX_L4_IPV6_CSUM_PART_MASK)
+		dev_features |= NETIF_F_IPV6_CSUM;
+
+	if (feat->offload.tx & ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV4_MASK)
+		dev_features |= NETIF_F_TSO;
+
+	if (feat->offload.tx & ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_IPV6_MASK)
+		dev_features |= NETIF_F_TSO6;
+
+	if (feat->offload.tx & ENA_ADMIN_FEATURE_OFFLOAD_DESC_TSO_ECN_MASK)
+		dev_features |= NETIF_F_TSO_ECN;
+
+	if (feat->offload.tx &
+		ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV4_CSUM_MASK)
+		dev_features |= NETIF_F_RXCSUM;
+
+	if (feat->offload.tx &
+		ENA_ADMIN_FEATURE_OFFLOAD_DESC_RX_L4_IPV6_CSUM_MASK)
+		dev_features |= NETIF_F_RXCSUM;
+
+	netdev->features =
+		dev_features |
+		NETIF_F_SG |
+		NETIF_F_NTUPLE |
+		NETIF_F_RXHASH |
+		NETIF_F_HIGHDMA;
+
+	netdev->hw_features |= netdev->features;
+}
+
+static void ena_set_conf_feat_params(struct ena_adapter *adapter,
+				     struct ena_com_dev_get_features_ctx *feat)
+{
+	struct net_device *netdev = adapter->netdev;
+
+	/* Copy mac address */
+	if (!is_valid_ether_addr(feat->dev_attr.mac_addr)) {
+		eth_hw_addr_random(netdev);
+		ether_addr_copy(adapter->mac_addr, netdev->dev_addr);
+	} else {
+		ether_addr_copy(adapter->mac_addr, feat->dev_attr.mac_addr);
+		ether_addr_copy(netdev->dev_addr, adapter->mac_addr);
+	}
+
+	/* Set offload features */
+	ena_set_dev_offloads(feat, netdev);
+
+	adapter->max_mtu = feat->dev_attr.max_mtu;
+}
+
+/**
+ * ena_probe - Device Initialization Routine
+ * @pdev: PCI device information struct
+ * @ent: entry in ena_pci_tbl
+ *
+ * Returns 0 on success, negative on failure
+ *
+ * ena_probe initializes an adapter identified by a pci_dev structure.
+ * The OS initialization, configuring of the adapter private structure,
+ * and a hardware reset occur.
+ **/
+static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct ena_com_dev_get_features_ctx get_feat_ctx;
+	static int version_printed;
+	struct net_device *netdev;
+	struct ena_adapter *adapter;
+	struct ena_com_dev *ena_dev = NULL;
+	static int adapters_found;
+	int io_queue_num;
+	int rc, i;
+
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+
+	if (version_printed++ == 0)
+		pr_info("%s", version);
+
+	rc = pci_enable_device_mem(pdev);
+	if (rc) {
+		dev_err(&pdev->dev, "pcim_enable_device failed!\n");
+		return rc;
+	}
+
+	rc = pci_request_selected_regions(pdev, (1 << 0), DRV_MODULE_NAME);
+	if (rc) {
+		dev_err(&pdev->dev, "pci_request_selected_regions failed %d\n",
+			rc);
+		goto err_disable_device;
+	}
+
+	pci_set_master(pdev);
+	pci_save_state(pdev);
+
+	ena_dev = devm_kzalloc(&pdev->dev, sizeof(struct ena_com_dev),
+			       GFP_KERNEL);
+	if (!ena_dev) {
+		rc = -ENOMEM;
+		goto err_free_region;
+	}
+
+	rc = ena_set_push_mode(ena_dev);
+	if (rc) {
+		dev_err(&pdev->dev, "Invalid module param(push_mode)\n");
+		goto err_free_ena_dev;
+	}
+
+	ena_dev->reg_bar = ioremap(pci_resource_start(pdev, ENA_REG_BAR),
+		pci_resource_len(pdev, ENA_REG_BAR));
+	if (!ena_dev->reg_bar) {
+		dev_err(&pdev->dev, "failed to remap regs bar\n");
+		rc = -EFAULT;
+		goto err_free_ena_dev;
+	}
+
+	if (ena_dev->tx_mem_queue_type != ENA_MEM_QUEUE_TYPE_HOST_MEMORY) {
+		ena_dev->mem_bar =
+			ioremap_wc(pci_resource_start(pdev, ENA_MEM_BAR),
+				   pci_resource_len(pdev, ENA_MEM_BAR));
+		if (!ena_dev->mem_bar) {
+			dev_err(&pdev->dev,
+				"failed to remap mem bar %d disable push mode\n",
+				ENA_MEM_BAR);
+			ena_dev->tx_mem_queue_type =
+				ENA_MEM_QUEUE_TYPE_HOST_MEMORY;
+		}
+	}
+
+	dev_info(&pdev->dev, "mapped bars to %p %p", ena_dev->reg_bar,
+		 ena_dev->mem_bar);
+
+	ena_dev->dmadev = &pdev->dev;
+
+	rc = ena_device_init(ena_dev, pdev, &get_feat_ctx);
+	if (rc) {
+		dev_err(&pdev->dev, "ena device init failed\n");
+		goto err_free_ena_dev;
+	}
+
+	io_queue_num = ena_calc_io_queue_num(pdev, &get_feat_ctx);
+	dev_info(&pdev->dev, "create %d io queues\n", io_queue_num);
+
+	/* dev zeroed in init_etherdev */
+	netdev = alloc_etherdev_mq(sizeof(struct ena_adapter), io_queue_num);
+	if (!netdev) {
+		dev_err(&pdev->dev, "alloc_etherdev_mq failed\n");
+		rc = -ENOMEM;
+		goto err_device_destroy;
+	}
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+
+	adapter = netdev_priv(netdev);
+	pci_set_drvdata(pdev, adapter);
+
+	adapter->ena_dev = ena_dev;
+	adapter->netdev = netdev;
+	adapter->pdev = pdev;
+
+	ena_set_conf_feat_params(adapter, &get_feat_ctx);
+
+	adapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);
+
+	/* set default ring sizes */
+	adapter->tx_ring_size = ENA_DEFAULT_TX_DESCS;
+	adapter->rx_ring_size = ENA_DEFAULT_RX_DESCS;
+
+	adapter->num_queues = io_queue_num;
+
+	adapter->small_copy_len =
+		ENA_DEFAULT_SMALL_PACKET_LEN;
+
+	snprintf(adapter->name, ENA_NAME_MAX_LEN, "ena_%d",
+		 adapters_found);
+
+	ena_init_io_rings(adapter);
+
+	netdev->netdev_ops = &ena_netdev_ops;
+	netdev->watchdog_timeo = TX_TIMEOUT;
+	netdev->ethtool_ops = &ena_ethtool_ops;
+
+#if defined(NETIF_F_MQ_TX_LOCK_OPT)
+	netdev->features &= ~NETIF_F_MQ_TX_LOCK_OPT;
+#endif /* defined(NETIF_F_MQ_TX_LOCK_OPT) */
+#ifdef IFF_UNICAST_FLT
+	netdev->priv_flags |= IFF_UNICAST_FLT;
+#endif /* IFF_UNICAST_FLT */
+
+	/* TODO RSS table size should be retrieved from the device */
+	for (i = 0; i < ENA_RX_RSS_TABLE_SIZE; i++)
+		adapter->rss_ind_tbl[i] =
+			ethtool_rxfh_indir_default(i, io_queue_num);
+
+	init_timer(&adapter->watchdog_timer);
+	adapter->watchdog_timer.expires =
+		round_jiffies(jiffies + ENA_DEVICE_KALIVE_TIMEOUT);
+	adapter->watchdog_timer.function = ena_watchdog_expire;
+	adapter->watchdog_timer.data = (unsigned long)adapter;
+
+	add_timer(&adapter->watchdog_timer);
+
+	INIT_WORK(&adapter->suspend_io_task, ena_device_io_suspend);
+	INIT_WORK(&adapter->resume_io_task, ena_device_io_resume);
+
+	rc = ena_enable_msix_and_set_admin_interrupts(adapter, io_queue_num);
+	if (rc) {
+		dev_err(&pdev->dev,
+			"Failed to enable and set the admin interrupts\n");
+		goto err_worker_destroy;
+	}
+
+	rc = ena_sysfs_init(&adapter->pdev->dev);
+	if (rc) {
+		dev_err(&pdev->dev, "Cannot init sysfs\n");
+		goto err_free_msix;
+	}
+
+	memcpy(adapter->netdev->perm_addr, adapter->mac_addr, netdev->addr_len);
+
+	rc = register_netdev(netdev);
+	if (rc) {
+		dev_err(&pdev->dev, "Cannot register net device\n");
+		goto err_terminate_sysfs;
+	}
+
+	netdev_info(netdev, "%s found at mem %lx, mac addr %pM Queues %d\n",
+		    DEVICE_NAME, (long)pci_resource_start(pdev, 0),
+		netdev->dev_addr,
+		io_queue_num);
+
+	adapters_found++;
+
+	return 0;
+
+err_terminate_sysfs:
+	ena_sysfs_terminate(&pdev->dev);
+err_free_msix:
+	ena_disable_msix(adapter);
+	ena_free_mgmnt_irq(adapter);
+err_worker_destroy:
+	del_timer(&adapter->watchdog_timer);
+	cancel_work_sync(&adapter->suspend_io_task);
+	cancel_work_sync(&adapter->resume_io_task);
+	free_netdev(netdev);
+err_device_destroy:
+	ena_com_admin_destroy(ena_dev);
+err_free_ena_dev:
+	pci_set_drvdata(pdev, NULL);
+	devm_kfree(&pdev->dev, ena_dev);
+err_free_region:
+	pci_release_regions(pdev);
+err_disable_device:
+	pci_disable_device(pdev);
+	return rc;
+}
+
+/*****************************************************************************/
+static int ena_sriov_configure(struct pci_dev *dev, int numvfs)
+{
+	int rc;
+
+	if (numvfs > 0) {
+		rc = pci_enable_sriov(dev, numvfs);
+		if (rc != 0) {
+			pr_err("pci_enable_sriov failed to enable: %d vfs with the error: %d\n",
+			       numvfs, rc);
+			return rc;
+		}
+
+		return numvfs;
+	}
+
+	if (numvfs == 0) {
+		pci_disable_sriov(dev);
+		return 0;
+	}
+
+	return -1;
+}
+
+/*****************************************************************************/
+/*****************************************************************************/
+
+/* ena_remove - Device Removal Routine
+ * @pdev: PCI device information struct
+ *
+ * ena_remove is called by the PCI subsystem to alert the driver
+ * that it should release a PCI device.
+ */
+static void ena_remove(struct pci_dev *pdev)
+{
+	struct ena_adapter *adapter = pci_get_drvdata(pdev);
+	struct ena_com_dev *ena_dev;
+	struct net_device *dev;
+	int release_bars;
+
+	if (!adapter)
+		/* This device didn't load properly and it's resources
+		 * already released, nothing to do
+		 */
+		return;
+
+	ena_dev = adapter->ena_dev;
+	dev = adapter->netdev;
+
+	release_bars = (1 << ENA_REG_BAR);
+	if (ena_dev->tx_mem_queue_type != ENA_MEM_QUEUE_TYPE_HOST_MEMORY)
+		release_bars |= (1 << ENA_MEM_BAR);
+
+	pci_release_selected_regions(pdev, release_bars);
+
+	unregister_netdev(dev);
+
+	ena_sysfs_terminate(&pdev->dev);
+
+	del_timer_sync(&adapter->watchdog_timer);
+
+	cancel_work_sync(&adapter->reset_task);
+
+	cancel_work_sync(&adapter->suspend_io_task);
+
+	cancel_work_sync(&adapter->resume_io_task);
+
+	free_netdev(dev);
+
+	ena_com_dev_reset(ena_dev);
+
+	ena_free_mgmnt_irq(adapter);
+
+	ena_disable_msix(adapter);
+
+	ena_com_mmio_reg_read_request_destroy(ena_dev);
+
+	ena_com_abort_admin_commands(ena_dev);
+
+	ena_com_wait_for_abort_completion(ena_dev);
+
+	pci_set_drvdata(pdev, NULL);
+
+	pci_disable_device(pdev);
+
+	devm_kfree(&pdev->dev, ena_dev);
+}
+
+static struct pci_driver ena_pci_driver = {
+	.name		= DRV_MODULE_NAME,
+	.id_table	= ena_pci_tbl,
+	.probe		= ena_probe,
+	.remove		= ena_remove,
+	.sriov_configure = ena_sriov_configure,
+};
+
+static int __init ena_init(void)
+{
+	return pci_register_driver(&ena_pci_driver);
+}
+
+static void __exit ena_cleanup(void)
+{
+	pci_unregister_driver(&ena_pci_driver);
+}
+
+/******************************************************************************
+ ******************************** AENQ Handlers *******************************
+ *****************************************************************************/
+/* ena_update_on_link_change:
+ * Notify the network interface about the change in link status
+ */
+static void ena_update_on_link_change(void *adapter_data,
+				      struct ena_admin_aenq_entry *aenq_e)
+{
+	struct ena_adapter *adapter = (struct ena_adapter *)adapter_data;
+	struct ena_admin_aenq_link_change_desc *aenq_desc =
+		(struct ena_admin_aenq_link_change_desc *)aenq_e;
+	int status = aenq_desc->flags &
+		ENA_ADMIN_AENQ_LINK_CHANGE_DESC_LINK_STATUS_MASK;
+
+	if (status) {
+		netdev_dbg(adapter->netdev, "%s\n", __func__);
+		netif_carrier_on(adapter->netdev);
+	} else {
+		netif_carrier_off(adapter->netdev);
+	}
+	adapter->link_status = status;
+}
+
+static void ena_keep_alive_wd(void *adapter_data,
+			      struct ena_admin_aenq_entry *aenq_e)
+{
+	struct ena_adapter *adapter = (struct ena_adapter *)adapter_data;
+
+	mod_timer(&adapter->watchdog_timer,
+		  round_jiffies(jiffies + ENA_DEVICE_KALIVE_TIMEOUT));
+}
+
+static void ena_notification(void *adapter_data,
+			     struct ena_admin_aenq_entry *aenq_e)
+{
+	struct ena_adapter *adapter = (struct ena_adapter *)adapter_data;
+
+	ENA_ASSERT(aenq_e->aenq_common_desc.group == ena_admin_notification,
+		   "Invalid group(%x) expected %x\n",
+		   aenq_e->aenq_common_desc.group,
+		   ena_admin_notification);
+
+	switch (aenq_e->aenq_common_desc.syndrom) {
+	case ena_admin_suspend:
+		/* Suspend just the IO queues.
+		 * We deliberately don't suspend admin so the timer and
+		 * the keep_alive events should remain.
+		 */
+		schedule_work(&adapter->suspend_io_task);
+		break;
+	case ena_admin_resume:
+		schedule_work(&adapter->resume_io_task);
+		break;
+	default:
+		netdev_err(adapter->netdev,
+			   "[%s] Invalid aenq notification link state\n",
+			   __func__);
+	}
+}
+
+/* This handler will called for unknown event group or unimplemented handlers*/
+static void unimplemented_aenq_handler(void *data,
+				       struct ena_admin_aenq_entry *aenq_e)
+{
+	struct ena_adapter *adapter = (struct ena_adapter *)data;
+
+	netdev_err(adapter->netdev,
+		   "Unknown event was received or event with unimplemented handler\n");
+}
+
+static struct ena_aenq_handlers aenq_handlers = {
+	.handlers = {
+		[ena_admin_link_change] = ena_update_on_link_change,
+		[ena_admin_notification] = ena_notification,
+		[ena_admin_keep_alive] = ena_keep_alive_wd,
+	},
+	.unimplemented_handler = unimplemented_aenq_handler
+};
+
+module_init(ena_init);
+module_exit(ena_cleanup);
diff --git a/drivers/amazon/ena/ena_netdev.h b/drivers/amazon/ena/ena_netdev.h
new file mode 100644
index 0000000..3af77c9
--- /dev/null
+++ b/drivers/amazon/ena/ena_netdev.h
@@ -0,0 +1,233 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef ENA_H
+#define ENA_H
+
+#include <linux/interrupt.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/inetdevice.h>
+
+#include "ena_com.h"
+#include "ena_eth_com.h"
+
+/* 1 for AENQ + ADMIN */
+#define ENA_MAX_MSIX_VEC(io_queues)	(1 + (io_queues))
+
+#define ENA_REG_BAR			0
+#define ENA_MEM_BAR			2
+
+#define ENA_DEFAULT_TX_DESCS	(1024)
+#define ENA_DEFAULT_RX_DESCS	(1024)
+
+#if ((ENA_DEFAULT_TX_DESCS / 4) < (MAX_SKB_FRAGS + 2))
+#define ENA_TX_WAKEUP_THRESH		(ENA_DEFAULT_TX_SW_DESCS / 4)
+#else
+#define ENA_TX_WAKEUP_THRESH		(MAX_SKB_FRAGS + 2)
+#endif
+#define ENA_DEFAULT_SMALL_PACKET_LEN		(128 - NET_IP_ALIGN)
+
+/* minimum the buffer size to 600 to avoid situation the mtu will be changed
+ * from too little buffer to very big one and then the number of buffer per
+ * packet could reach the maximum ENA_PKT_MAX_BUFS
+ */
+#define ENA_DEFAULT_MIN_RX_BUFF_ALLOC_SIZE 600
+
+#define ENA_NAME_MAX_LEN	20
+#define ENA_IRQNAME_SIZE	40
+
+#define ENA_PKT_MAX_BUFS	19
+
+#define ENA_RX_THASH_TABLE_SIZE	256
+
+/* The number of tx packet completions that will be handled each napi poll
+ * cycle is ring_size / ENA_TX_POLL_BUDGET_DEVIDER.
+ */
+#define ENA_TX_POLL_BUDGET_DEVIDER	4
+
+/* Refill Rx queue when number of available descriptors is below
+ * QUEUE_SIZE / ENA_RX_REFILL_THRESH_DEVIDER
+ */
+#define ENA_RX_REFILL_THRESH_DEVIDER	8
+
+#define ENA_TX_RING_IDX_NEXT(idx, ring_size) (((idx) + 1) & ((ring_size) - 1))
+
+#define ENA_RX_RING_IDX_NEXT(idx, ring_size) (((idx) + 1) & ((ring_size) - 1))
+#define ENA_RX_RING_IDX_ADD(idx, n, ring_size) \
+	(((idx) + (n)) & ((ring_size) - 1))
+
+#define ENA_IO_TXQ_IDX(q)	(2 * (q))
+#define ENA_IO_RXQ_IDX(q)	(2 * (q) + 1)
+
+#define ENA_MGMNT_IRQ_IDX		0
+#define ENA_IO_IRQ_FIRST_IDX		1
+#define ENA_IO_IRQ_IDX(q)		(ENA_IO_IRQ_FIRST_IDX + (q))
+
+/* ENA device should send keep alive msg every 1 sec.
+ * We wait for 3 sec just to be on the safe side.
+ */
+#define ENA_DEVICE_KALIVE_TIMEOUT	(3 * HZ)
+
+#define ENA_RX_RSS_TABLE_SIZE	ENA_RX_THASH_TABLE_SIZE
+
+struct ena_irq {
+	irq_handler_t handler;
+	void *data;
+	u32 vector;
+	cpumask_t affinity_hint_mask;
+	char name[ENA_IRQNAME_SIZE];
+};
+
+struct ena_napi {
+	struct napi_struct napi ____cacheline_aligned;
+	struct ena_ring *tx_ring;
+	struct ena_ring *rx_ring;
+#ifndef HAVE_NETDEV_NAPI_LIST
+	struct net_device poll_dev;
+#endif /* HAVE_NETDEV_NAPI_LIST */
+	u32 qid;
+};
+
+struct ena_tx_buffer {
+	struct sk_buff *skb;
+	/* num of ena desc for this specific skb
+	 * (includes data desc and metadata desc)
+	 */
+	u32 tx_descs;
+	/* num of buffers used by this skb */
+	u32 num_of_bufs;
+	struct ena_com_buf bufs[ENA_PKT_MAX_BUFS];
+} ____cacheline_aligned;
+
+struct ena_rx_buffer {
+	struct sk_buff *skb;
+	struct page *page;
+	u8 *data;
+	u32 data_size;
+	u32 frag_size; /* used in rx skb allocation */
+	u32 page_offset;
+	struct ena_com_buf ena_buf;
+} ____cacheline_aligned;
+
+struct ena_ring {
+	/* Holds the empty requests for TX out of order completions */
+	u16 *free_tx_ids;
+	union {
+		struct ena_tx_buffer *tx_buffer_info; /* contex of tx packet */
+		struct ena_rx_buffer *rx_buffer_info; /* contex of rx packet */
+	};
+
+	/* cache ptr to avoid using the adapter */
+	struct device *dev;
+	struct pci_dev *pdev;
+	struct napi_struct *napi;
+	struct net_device *netdev;
+	struct ena_com_io_cq *ena_com_io_cq;
+	struct ena_com_io_sq *ena_com_io_sq;
+
+	u16 next_to_use;
+	u16 next_to_clean;
+	u16 rx_small_copy_len;
+	u16 qid;
+	u16 mtu;
+
+	int ring_size; /* number of tx/rx_buffer_info's entries */
+
+	/* Count how many times the driver wasn't able to allocate new pages */
+	u32 alloc_fail_cnt;
+	u32 bad_checksum;
+
+	enum ena_com_memory_queue_type tx_mem_queue_type;
+
+	struct ena_com_buf ena_bufs[ENA_PKT_MAX_BUFS];
+} ____cacheline_aligned;
+
+/* adapter specific private data structure */
+struct ena_adapter {
+	struct ena_com_dev *ena_dev;
+	/* OS defined structs */
+	struct net_device *netdev;
+	struct pci_dev *pdev;
+
+	u32 msix_enabled;
+
+	/* rx packets that shorter that this len will be copied to the skb
+	 * header
+	 */
+	u32 small_copy_len;
+	u32 max_mtu;
+
+	int num_queues;
+
+	struct msix_entry *msix_entries;
+	int msix_vecs;
+
+	u32 tx_usecs, rx_usecs; /* interrupt moderation */
+	u32 tx_frames, rx_frames; /* interrupt moderation */
+
+	u32 tx_ring_size;
+	u32 rx_ring_size;
+
+	/* RSS*/
+	u8 rss_ind_tbl[ENA_RX_RSS_TABLE_SIZE];
+
+	u32 msg_enable;
+
+	u8 mac_addr[ETH_ALEN];
+
+	char name[ENA_NAME_MAX_LEN];
+	bool link_status;
+
+	bool up;
+
+	/* TX */
+	struct ena_ring tx_ring[ENA_MAX_NUM_IO_QUEUES]
+		____cacheline_aligned_in_smp;
+
+	/* RX */
+	struct ena_ring rx_ring[ENA_MAX_NUM_IO_QUEUES]
+		____cacheline_aligned_in_smp;
+
+	struct ena_napi ena_napi[ENA_MAX_NUM_IO_QUEUES];
+
+	struct ena_irq irq_tbl[ENA_MAX_MSIX_VEC(ENA_MAX_NUM_IO_QUEUES)];
+
+	/* watchdog timer */
+	struct work_struct reset_task;
+	struct work_struct suspend_io_task;
+	struct work_struct resume_io_task;
+	struct timer_list watchdog_timer;
+};
+
+#endif /* !(ENA_H) */
diff --git a/drivers/amazon/ena/ena_pci_id_tbl.h b/drivers/amazon/ena/ena_pci_id_tbl.h
new file mode 100644
index 0000000..2251bd1
--- /dev/null
+++ b/drivers/amazon/ena/ena_pci_id_tbl.h
@@ -0,0 +1,77 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef ENA_PCI_ID_TBL_H_
+#define ENA_PCI_ID_TBL_H_
+
+#ifndef PCI_VENDOR_ID_AMAZON
+#define PCI_VENDOR_ID_AMAZON 0x1d0f
+#endif
+
+#ifndef PCI_DEV_ID_ENA_PF
+#define PCI_DEV_ID_ENA_PF	0x0ec2
+#endif
+
+#ifndef PCI_DEV_ID_ENA_LLQ_PF
+#define PCI_DEV_ID_ENA_LLQ_PF	0x1ec2
+#endif
+
+#ifndef PCI_DEV_ID_ENA_VF
+#define PCI_DEV_ID_ENA_VF	0xec20
+#endif
+
+#ifndef PCI_DEV_ID_ENA_LLQ_VF
+#define PCI_DEV_ID_ENA_LLQ_VF	0xec21
+#endif
+
+#ifndef PCI_DEV_ID_ENA_EFA_PF
+#define PCI_DEV_ID_ENA_EFA_PF	0x0efa
+#endif
+
+#ifndef PCI_DEV_ID_ENA_EFA_VF
+#define PCI_DEV_ID_ENA_EFA_VF	0xefa0
+#endif
+
+#define ENA_PCI_ID_TABLE_ENTRY(devid) \
+	{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, devid)},
+
+static const struct pci_device_id ena_pci_tbl[] = {
+	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_PF)
+	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_LLQ_PF)
+	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_VF)
+	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_LLQ_VF)
+	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_EFA_PF)
+	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_EFA_VF)
+	{ }
+};
+
+#endif /* ENA_PCI_ID_TBL_H_ */
diff --git a/drivers/amazon/ena/ena_regs_defs.h b/drivers/amazon/ena/ena_regs_defs.h
new file mode 100644
index 0000000..febade7
--- /dev/null
+++ b/drivers/amazon/ena/ena_regs_defs.h
@@ -0,0 +1,328 @@
+/******************************************************************************
+Copyright (C) 2015 Annapurna Labs Ltd.
+
+This file may be licensed under the terms of the Annapurna Labs Commercial
+License Agreement.
+
+Alternatively, this file can be distributed under the terms of the GNU General
+Public License V2 as published by the Free Software Foundation and can be
+found at http://www.gnu.org/licenses/gpl-2.0.html
+
+Alternatively, redistribution and use in source and binary forms, with or
+without modification, are permitted provided that the following conditions are
+met:
+
+    *  Redistributions of source code must retain the above copyright notice,
+this list of conditions and the following disclaimer.
+
+    *  Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+******************************************************************************/
+
+#ifndef _ENA_REGS_H_
+#define _ENA_REGS_H_
+
+/* ENA Global Registers, for the entire PCIe function */
+struct ena_regs_ena_registers {
+	/* word 0 : */
+	/*
+	 * ENA specification version
+	 * 7:0 : minor_version - Minor version
+	 * 15:8 : major_version - Major version.
+	 * 31:16 : reserved16
+	 */
+	u32 version;
+
+	/* word 1 : */
+	/*
+	 * ENA controller version
+	 * 7:0 : subminor_version - Sub Minor version
+	 * 15:8 : minor_version - Minor version
+	 * 23:16 : major_version - Major version.
+	 * 31:24 : impl_id - implementation
+	 */
+	u32 controller_version;
+
+	/* word 2 : */
+	/*
+	 * capabilities register
+	 * 0 : contiguous_queue_required - If set, requires
+	 *    that each queue ring occupies a contiguous
+	 *    physical memory space.
+	 * 5:1 : reset_timeout - Max amount of time the
+	 *    driver should wait for ENA Reset to finish in
+	 *    resultion of 100ms.
+	 * 7:6 : reserved6
+	 * 15:8 : dma_addr_width - DMA address width. Number
+	 *    of bits of the DMA address supported by the
+	 *    Controller.
+	 * 31:16 : reserved16
+	 */
+	u32 caps;
+
+	/* word 3 : capabilities extended register */
+	u32 caps_ext;
+
+	/* word 4 : admin queue base address bits [31:0] */
+	u32 aq_base_lo;
+
+	/* word 5 : admin queue base address bits [63:32] */
+	u32 aq_base_hi;
+
+	/* word 6 : */
+	/*
+	 * admin queue capabilities register
+	 * 15:0 : aq_depth - admin queue depth in entries
+	 * 31:16 : aq_entry_size - admin queue entry size in
+	 *    32-bit words
+	 */
+	u32 aq_caps;
+
+	/* word 7 :  */
+	u32 reserved;
+
+	/* word 8 : admin completion queue base address bits [31:0]. */
+	u32 acq_base_lo;
+
+	/* word 9 : admin completion queue base address bits [63:32]. */
+	u32 acq_base_hi;
+
+	/* word 10 : */
+	/*
+	 * admin completion queue capabilities register
+	 * 15:0 : acq_depth - admin completion queue depth in
+	 *    entries
+	 * 31:16 : acq_entry_size - admin completion queue
+	 *    entry size in 32-bit words
+	 */
+	u32 acq_caps;
+
+	/* word 11 : AQ Doorbell */
+	u32 aq_db;
+
+	/*
+	 * word 12 : ACQ tail pointer, indicates where new completions will
+	 * be placed
+	 */
+	u32 acq_tail;
+
+	/* word 13 : */
+	/*
+	 * Asynchronous Event Notification Queue capabilities register
+	 * 15:0 : aenq_depth - queue depth in entries
+	 * 31:16 : aenq_entry_size - queue entry size in
+	 *    32-bit words
+	 */
+	u32 aenq_caps;
+
+	/*
+	 * word 14 : Asynchronous Event Notification Queue base address
+	 * bits [31:0]
+	 */
+	u32 aenq_base_lo;
+
+	/*
+	 * word 15 : Asynchronous Event Notification Queue base address
+	 * bits [63:32]
+	 */
+	u32 aenq_base_hi;
+
+	/*
+	 * word 16 : AENQ Head Doorbell, indicates the entries that have
+	 * been processed by the host
+	 */
+	u32 aenq_head_db;
+
+	/*
+	 * word 17 : AENQ tail pointer, indicates where new entries will be
+	 * placed
+	 */
+	u32 aenq_tail;
+
+	/* word 18 :  */
+	u32 intr_cause;
+
+	/* word 19 :  */
+	u32 intr_mask;
+
+	/* word 20 :  */
+	u32 intr_clear;
+
+	/* word 21 : */
+	/*
+	 * Device Control Register, some of these features may not be
+	 *    implemented or supported for a given client
+	 * 0 : dev_reset - If set, indicates request for a
+	 *    reset, this bit will only be cleared when the
+	 *    reset operation finished and can not be cleared by
+	 *    writing 0 to it.
+	 * 1 : aq_restart - Used in case AQ is not
+	 *    responsive: Once set, the it will be auto-cleared
+	 *    once process is done. The status of the restart is
+	 *    indicated in the status register.
+	 * 2 : quiescent - If set, indicates a request for
+	 *    suspending of I/O, Admin and Async Events
+	 *    handling, this bit will only be cleared when the
+	 *    quiescen process is finished.
+	 * 3 : io_resume - If set, indicates request to
+	 *    resume traffic processing.
+	 * 31:4 : reserved4
+	 */
+	u32 dev_ctl;
+
+	/* word 22 : */
+	/*
+	 * Device Status Register
+	 * 0 : ready - device ready to received admin commands
+	 * 1 : aq_restart_in_progress - this bit is set while
+	 *    aq_restart in process
+	 * 2 : aq_restart_finished - this bit is set only
+	 *    after aq_restart process finished, and will be
+	 *    auto-cleared one aq_restart in control register is
+	 *    invoked
+	 * 3 : reset_in_progress - this bit is set while ENA
+	 *    reset is going
+	 * 4 : reset_finished - this bit is set when ENA
+	 *    reset is finished. It is auto-cleared one reset is
+	 *    invoked in control register
+	 * 5 : fatal_error
+	 * 6 : quiescent_state_in_progress - A process to
+	 *    quiescent ENA is in progress
+	 * 7 : quiescent_state_achieved - This bit is set
+	 *    once the quiescent state is achieved, and it is
+	 *    auto-cleared once the quiescent_start
+	 * 31:8 : reserved8
+	 */
+	u32 dev_sts;
+
+	/* word 23 : */
+	/*
+	 * MMIO Read Less Register
+	 * 15:0 : req_id - request id
+	 * 31:16 : reg_off - register offset
+	 */
+	u32 mmio_read;
+
+	/* word 24 : read response will be sent to this address. bits [31:0] */
+	u32 mmio_resp_lo;
+
+	/*
+	 * word 25 : read response will be sent to this address. bits
+	 * [63:32]
+	 */
+	u32 mmio_resp_hi;
+};
+
+/* admin interrupt register */
+#define ENA_REGS_ADMIN_INTERRUPT_ACQ	0x1 /* Admin Completion queue */
+#define ENA_REGS_ADMIN_INTERRUPT_AENQ	0x2 /* Asynchronous Event Notification Queue */
+
+/* ena_registers offsets */
+#define ENA_REGS_VERSION_OFF		0x0
+#define ENA_REGS_CONTROLLER_VERSION_OFF		0x4
+#define ENA_REGS_CAPS_OFF		0x8
+#define ENA_REGS_CAPS_EXT_OFF		0xc
+#define ENA_REGS_AQ_BASE_LO_OFF		0x10
+#define ENA_REGS_AQ_BASE_HI_OFF		0x14
+#define ENA_REGS_AQ_CAPS_OFF		0x18
+#define ENA_REGS_ACQ_BASE_LO_OFF		0x20
+#define ENA_REGS_ACQ_BASE_HI_OFF		0x24
+#define ENA_REGS_ACQ_CAPS_OFF		0x28
+#define ENA_REGS_AQ_DB_OFF		0x2c
+#define ENA_REGS_ACQ_TAIL_OFF		0x30
+#define ENA_REGS_AENQ_CAPS_OFF		0x34
+#define ENA_REGS_AENQ_BASE_LO_OFF		0x38
+#define ENA_REGS_AENQ_BASE_HI_OFF		0x3c
+#define ENA_REGS_AENQ_HEAD_DB_OFF		0x40
+#define ENA_REGS_AENQ_TAIL_OFF		0x44
+#define ENA_REGS_INTR_CAUSE_OFF		0x48
+#define ENA_REGS_INTR_MASK_OFF		0x4c
+#define ENA_REGS_INTR_CLEAR_OFF		0x50
+#define ENA_REGS_DEV_CTL_OFF		0x54
+#define ENA_REGS_DEV_STS_OFF		0x58
+#define ENA_REGS_MMIO_READ_OFF		0x5c
+#define ENA_REGS_MMIO_RESP_LO_OFF		0x60
+#define ENA_REGS_MMIO_RESP_HI_OFF		0x64
+
+/* version register */
+#define ENA_REGS_VERSION_MINOR_VERSION_MASK		0xff
+#define ENA_REGS_VERSION_MAJOR_VERSION_SHIFT		8
+#define ENA_REGS_VERSION_MAJOR_VERSION_MASK		0xff00
+
+/* controller_version register */
+#define ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK		0xff
+#define ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_SHIFT		8
+#define ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK		0xff00
+#define ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_SHIFT		16
+#define ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK		0xff0000
+#define ENA_REGS_CONTROLLER_VERSION_IMPL_ID_SHIFT		24
+#define ENA_REGS_CONTROLLER_VERSION_IMPL_ID_MASK		0xff000000
+
+/* caps register */
+#define ENA_REGS_CAPS_CONTIGUOUS_QUEUE_REQUIRED_MASK		0x1
+#define ENA_REGS_CAPS_RESET_TIMEOUT_SHIFT		1
+#define ENA_REGS_CAPS_RESET_TIMEOUT_MASK		0x3e
+#define ENA_REGS_CAPS_DMA_ADDR_WIDTH_SHIFT		8
+#define ENA_REGS_CAPS_DMA_ADDR_WIDTH_MASK		0xff00
+
+/* aq_caps register */
+#define ENA_REGS_AQ_CAPS_AQ_DEPTH_MASK		0xffff
+#define ENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_SHIFT		16
+#define ENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_MASK		0xffff0000
+
+/* acq_caps register */
+#define ENA_REGS_ACQ_CAPS_ACQ_DEPTH_MASK		0xffff
+#define ENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_SHIFT		16
+#define ENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_MASK		0xffff0000
+
+/* aenq_caps register */
+#define ENA_REGS_AENQ_CAPS_AENQ_DEPTH_MASK		0xffff
+#define ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_SHIFT		16
+#define ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_MASK		0xffff0000
+
+/* dev_ctl register */
+#define ENA_REGS_DEV_CTL_DEV_RESET_MASK		0x1
+#define ENA_REGS_DEV_CTL_AQ_RESTART_SHIFT		1
+#define ENA_REGS_DEV_CTL_AQ_RESTART_MASK		0x2
+#define ENA_REGS_DEV_CTL_QUIESCENT_SHIFT		2
+#define ENA_REGS_DEV_CTL_QUIESCENT_MASK		0x4
+#define ENA_REGS_DEV_CTL_IO_RESUME_SHIFT		3
+#define ENA_REGS_DEV_CTL_IO_RESUME_MASK		0x8
+
+/* dev_sts register */
+#define ENA_REGS_DEV_STS_READY_MASK		0x1
+#define ENA_REGS_DEV_STS_AQ_RESTART_IN_PROGRESS_SHIFT		1
+#define ENA_REGS_DEV_STS_AQ_RESTART_IN_PROGRESS_MASK		0x2
+#define ENA_REGS_DEV_STS_AQ_RESTART_FINISHED_SHIFT		2
+#define ENA_REGS_DEV_STS_AQ_RESTART_FINISHED_MASK		0x4
+#define ENA_REGS_DEV_STS_RESET_IN_PROGRESS_SHIFT		3
+#define ENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK		0x8
+#define ENA_REGS_DEV_STS_RESET_FINISHED_SHIFT		4
+#define ENA_REGS_DEV_STS_RESET_FINISHED_MASK		0x10
+#define ENA_REGS_DEV_STS_FATAL_ERROR_SHIFT		5
+#define ENA_REGS_DEV_STS_FATAL_ERROR_MASK		0x20
+#define ENA_REGS_DEV_STS_QUIESCENT_STATE_IN_PROGRESS_SHIFT		6
+#define ENA_REGS_DEV_STS_QUIESCENT_STATE_IN_PROGRESS_MASK		0x40
+#define ENA_REGS_DEV_STS_QUIESCENT_STATE_ACHIEVED_SHIFT		7
+#define ENA_REGS_DEV_STS_QUIESCENT_STATE_ACHIEVED_MASK		0x80
+
+/* mmio_read register */
+#define ENA_REGS_MMIO_READ_REQ_ID_MASK		0xffff
+#define ENA_REGS_MMIO_READ_REG_OFF_SHIFT		16
+#define ENA_REGS_MMIO_READ_REG_OFF_MASK		0xffff0000
+
+#endif /*_ENA_REGS_H_ */
diff --git a/drivers/amazon/ena/ena_sysfs.c b/drivers/amazon/ena/ena_sysfs.c
new file mode 100644
index 0000000..bb4abbf
--- /dev/null
+++ b/drivers/amazon/ena/ena_sysfs.c
@@ -0,0 +1,115 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/sysfs.h>
+#include <linux/kernel.h>
+
+#include <linux/device.h>
+#include <linux/stat.h>
+#include <linux/sysfs.h>
+
+#include "ena_netdev.h"
+#include "ena_com.h"
+
+#define to_ext_attr(x) container_of(x, struct dev_ext_attribute, attr)
+
+static int ena_validate_small_copy_len(struct ena_adapter *adapter,
+				       unsigned long len)
+{
+	if (len > adapter->netdev->mtu)
+		return -EINVAL;
+
+	return 0;
+}
+
+static ssize_t ena_store_small_copy_len(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t len)
+{
+	struct ena_adapter *adapter = dev_get_drvdata(dev);
+	unsigned long small_copy_len;
+	struct ena_ring *rx_ring;
+	int err, i;
+
+	err = kstrtoul(buf, 10, &small_copy_len);
+	if (err < 0)
+		return err;
+
+	err = ena_validate_small_copy_len(adapter, small_copy_len);
+	if (err)
+		return err;
+
+	rtnl_lock();
+	adapter->small_copy_len = small_copy_len;
+
+	for (i = 0; i < adapter->num_queues; i++) {
+		rx_ring = &adapter->rx_ring[i];
+		rx_ring->rx_small_copy_len = small_copy_len;
+	}
+	rtnl_unlock();
+
+	return len;
+}
+
+static ssize_t ena_show_small_copy_len(struct device *dev,
+				       struct device_attribute *attr, char *buf)
+{
+	struct ena_adapter *adapter = dev_get_drvdata(dev);
+
+	return sprintf(buf, "%d\n", adapter->small_copy_len);
+}
+
+static struct device_attribute dev_attr_small_copy_len = {
+	.attr = {.name = "small_copy_len", .mode = (S_IRUGO | S_IWUSR)},
+	.show = ena_show_small_copy_len,
+	.store = ena_store_small_copy_len,
+};
+
+/******************************************************************************
+ *****************************************************************************/
+int ena_sysfs_init(struct device *dev)
+{
+	int status = 0;
+
+	if (device_create_file(dev, &dev_attr_small_copy_len))
+		dev_info(dev, "failed to create small_copy_len sysfs entry");
+
+	return status;
+}
+
+/******************************************************************************
+ *****************************************************************************/
+void ena_sysfs_terminate(
+	struct device *dev)
+{
+	device_remove_file(dev, &dev_attr_small_copy_len);
+}
diff --git a/drivers/amazon/ena/ena_sysfs.h b/drivers/amazon/ena/ena_sysfs.h
new file mode 100644
index 0000000..1ac83bd
--- /dev/null
+++ b/drivers/amazon/ena/ena_sysfs.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright 2015 Amazon.com, Inc. or its affiliates.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef __ENA_SYSFS_H__
+#define __ENA_SYSFS_H__
+
+#ifdef CONFIG_SYSFS
+
+int ena_sysfs_init(struct device *dev);
+
+void ena_sysfs_terminate(struct device *dev);
+
+#else /* CONFIG_SYSFS */
+
+static inline int ena_sysfs_init(struct device *dev)
+{
+	return 0;
+}
+
+static inline void ena_sysfs_terminate(struct device *dev)
+{
+}
+
+#endif /* CONFIG_SYSFS */
+
+#endif /* __ENA_SYSFS_H__ */
+
-- 
2.7.4

